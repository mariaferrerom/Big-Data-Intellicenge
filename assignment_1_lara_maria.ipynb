{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGMENT 1\n",
    "\n",
    "## Lara Monteserín Placer\n",
    "\n",
    "## María Ferrero Medina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this project is to design a machine learning model that is able to predict the energy produced by the Sotavento wind farm. For this purpose, a dataset with 555 features ans 4748 instances is available.\n",
    "\n",
    "The structure of the study will be the following:\n",
    "\n",
    "1. Exploratory Data Analysis\n",
    "2. Methodology\n",
    "3. KNN model\n",
    "4. Decission tree model\n",
    "5. Ensemble method 1 model\n",
    "6. Ensemble method 2 model\n",
    "7. Selection and performance of the final model\n",
    "\n",
    "For each of the models created, several steps have been followed to optimize them. First of all, a simple version of each model is created, with hyperparameters that seem reasonable, no feature selection and a basic imputation technique. Then, sequentially, models are improved.\n",
    "\n",
    "1. First model\n",
    "2. Feature selection\n",
    "3. Imputation techniques\n",
    "4. Hyperparameter tuning\n",
    "\n",
    "\n",
    "Things to add: - another idea (new library?, new idea...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-hPXBr4r5rS"
   },
   "source": [
    "\n",
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "Before starting to build the model, an EDA is made as a first approach to gain understanding of the dataset. In this Exploratory Data Analysis the data type of the features will be verified, the number of instances and features will be determined. Also, a brief summary of the missing values and columns with constant value will be included. \n",
    "\n",
    "### 1.1. Number of instances and features\n",
    "\n",
    "This dataset has 4748 instances and 555 features.\n",
    "\n",
    "### 1.2. Nature of the variables\n",
    "\n",
    "This dataset contains information about the meteorological conditions in several locations, the time the measures of these conditions were made and the energy produced at each moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy     float64\n",
      "year         int64\n",
      "month        int64\n",
      "day          int64\n",
      "hour         int64\n",
      "            ...   \n",
      "v100.21    float64\n",
      "v100.22    float64\n",
      "v100.23    float64\n",
      "v100.24    float64\n",
      "v100.25    float64\n",
      "Length: 555, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Read the data that is compressed as a gzip\n",
    "wind_ava = pd.read_csv('wind_available.csv.gzip', compression=\"gzip\")\n",
    "\n",
    "# Display the first rows of the dataset just to see it\n",
    "wind_ava.head()\n",
    "\n",
    "# Display the data type of each column\n",
    "column_data_types = wind_ava.dtypes\n",
    "print(column_data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having checked the data types of all the different features, it has been verified that there are:\n",
    "\n",
    "- 551 numerical variables (real numbers). From this 551, one is the energy, that is the output of the problem. And the remaining 550 are relative to the 22 different meteorological conditions measured at the 25 different locations.\n",
    "\n",
    "- 4 numerical variables (integers). These 4 variables are the year, day, month and hour of the day. These variables characterize the moment the measures were taken.\n",
    "\n",
    "### 1.3. Check for missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Column  Null Values  NaN Values\n",
      "0     energy            0           0\n",
      "1       year            0           0\n",
      "2      month            0           0\n",
      "3        day            0           0\n",
      "4       hour            0           0\n",
      "..       ...          ...         ...\n",
      "550  v100.21          261         261\n",
      "551  v100.22          387         387\n",
      "552  v100.23          569         569\n",
      "553  v100.24          579         579\n",
      "554  v100.25          436         436\n",
      "\n",
      "[555 rows x 3 columns]\n",
      "Number of columns with Null Values: 550\n",
      "Number of columns with NaN Values: 550\n"
     ]
    }
   ],
   "source": [
    "# Return the number of Null values for each column\n",
    "null_values = wind_ava.isnull().sum()\n",
    "# Return the number of NaN values for each column (just in case they are not the same)\n",
    "nan_values = wind_ava.isna().sum()\n",
    "\n",
    "# Store in missing values the amount of Null and NaN values of each column\n",
    "missing_values = pd.DataFrame({\n",
    "    'Column': null_values.index,\n",
    "    'Null Values': null_values.values,\n",
    "    'NaN Values': nan_values.values\n",
    "})\n",
    "\n",
    "# Print the amount of Null and Nan values\n",
    "print(missing_values)\n",
    "\n",
    "# Identify columns with Null or NaN values\n",
    "columns_with_null = wind_ava.columns[wind_ava.isnull().any()]\n",
    "columns_with_nan = wind_ava.columns[wind_ava.isna().any()]\n",
    "\n",
    "# Display the number of columns which have missing values\n",
    "print(\"Number of columns with Null Values:\", len(columns_with_null))\n",
    "print(\"Number of columns with NaN Values:\", len(columns_with_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All meteorological variables have missing values in different instances. The 4 categories that characterize the moment the measure was made and the target feature'energy' do not have missing values.\n",
    "\n",
    "\n",
    "### 1.4. Check for constant columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with constant values: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check for constant values in each column\n",
    "constant_columns = wind_ava.columns[wind_ava.nunique() == 1]\n",
    "\n",
    "# Print columns with constant values\n",
    "print(\"Columns with constant values:\", constant_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are no constant columns. \n",
    "\n",
    "\n",
    "### 1.5. Type of problem\n",
    "\n",
    "The objective of the model is to estimate the energy, as it is a continuous numerical value, this is a **regression problem**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "\n",
    "This section will explain the methodology that is going to be followed to evaluate the models. The evaluation tech Which are the outer evaluation and the inner evaluation. And the metrics.\n",
    "\n",
    "- On the one hand, for the **inner evaluation**, crossvalidation will be the method applied. Crossvalidation will be used to determine which is the best combination of hyperparameters. \n",
    "\n",
    "- On the other hand, for the **outer evaluation**, holdout evaluation will be used. Thie method will be used to estimate the future performance of the designed method.\n",
    "\n",
    "Later, to improve the performance of the method, once it is already computed the outer performance, the hyperparameters will be tuned again but this time using the whole dataset.\n",
    "\n",
    "The objective function that is going to be used for the validation of the method is the Mean Squared Error (MSE). This metric is more sensitive to outliers and to distant values as it squares the magnitudes. This is useful to penalize the errors that are larger, and avoid having a model that might have such large errors. \n",
    "\n",
    "**It could be longer this explanation**\n",
    "\n",
    "Note: as the variable that is going to be predicted is the 'energy', only the variables related to the meteorological characteristics will be used. It is considered that the energy produced does not depend on the moment of the day it is being produced.\n",
    "\n",
    "## 3. KNN Regressor\n",
    "\n",
    "First, the KNN algorithm is used for the predictions.\n",
    "\n",
    "### 3.1. First model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  376408.3075368221\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# First, data will be divided into train and test set \n",
    "# Considering it is a time series, it must be split in an appropiate way\n",
    "wind_ava['timestamp'] = pd.to_datetime(wind_ava[['year', 'month', 'day', 'hour']])\n",
    "wind_ava = wind_ava.sort_values(by='timestamp')\n",
    "\n",
    "train_size = 0.8  # Porcentaje de datos para entrenamiento\n",
    "split_index = int(len(wind_ava) * train_size)\n",
    "wind_ava = wind_ava.drop(columns=['timestamp'])\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "train_data = wind_ava.iloc[:split_index]\n",
    "test_data = wind_ava.iloc[split_index:]\n",
    "\n",
    "X_train = train_data.drop('energy', axis=1)\n",
    "y_train = train_data['energy']\n",
    "X_test = test_data.drop('energy', axis=1)\n",
    "y_test = test_data['energy']\n",
    "\n",
    "# Now the first model will be created\n",
    "first_knn = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',KNeighborsRegressor(n_neighbors=31))])\n",
    "first_knn.fit(X_train,y_train)\n",
    "y_predicted = first_knn.predict(X_test)\n",
    "MSE = mean_squared_error(y_test,y_predicted)\n",
    "\n",
    "print('MSE: ', MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Tree\n",
    "\n",
    "### 4.1. First model\n",
    "\n",
    "One first model using the decision tree algorithm will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  282593.2439923158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# FIRST OPTION - Univariate technique \n",
    "# Define the first tree as a Pipeline with a preprocessing stage with decision tree\n",
    "first_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',DecisionTreeRegressor())])\n",
    "first_tree.fit(X_train,y_train)\n",
    "y_predicted = first_tree.predict(X_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test,y_predicted)\n",
    "\n",
    "print('MSE: ', MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Feature selection\n",
    "\n",
    "For the feature selection, four different cases have been considered:\n",
    "- Seleting only the features related to the location of the wind farm (Sotavento). This is the features that contain the sufix 13.\n",
    "- Selecting onkly the features related to the wind characteristics. This is the features that start with u or v (the vertical and horizontal components of the wind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  303632.6816254736\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "X_1 = wind_ava.filter(regex='\\.13$', axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "y_1 = wind_ava['energy']\n",
    "\n",
    "train_size = 0.8  # Porcentaje de datos para entrenamiento\n",
    "split_index = int(len(X_1) * train_size)\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "X_train_1 = X_1.iloc[:split_index]\n",
    "X_test_1 = X_1.iloc[split_index:]\n",
    "y_train_1 = y_1.iloc[:split_index]\n",
    "y_test_1 = y_1.iloc[split_index:]\n",
    "\n",
    "# Define the first model as a Pipeline with a preprocessing stage with knn\n",
    "# first_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('feature_selection',SelectKBest(f_regression)),('regression',DecisionTreeRegressor())])\n",
    "first_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',DecisionTreeRegressor())])\n",
    "first_tree.fit(X_train_1,y_train_1)\n",
    "y_predicted_1 = first_tree.predict(X_test_1)\n",
    "MSE_1 = mean_squared_error(y_test_1,y_predicted_1)\n",
    "\n",
    "print('MSE: ', MSE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  272056.28781757894\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "X_2 = wind_ava.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "y_2 = wind_ava['energy']\n",
    "\n",
    "train_size = 0.8  # Porcentaje de datos para entrenamiento\n",
    "split_index = int(len(X_2) * train_size)\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "X_train_2 = X_2.iloc[:split_index]\n",
    "X_test_2 = X_2.iloc[split_index:]\n",
    "y_train_2 = y_2.iloc[:split_index]\n",
    "y_test_2 = y_2.iloc[split_index:]\n",
    "\n",
    "# Define the first model as a Pipeline with a preprocessing stage with knn\n",
    "second_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',DecisionTreeRegressor())])\n",
    "second_tree.fit(X_train_2,y_train_2)\n",
    "y_predicted_2 = second_tree.predict(X_test_2)\n",
    "MSE_2 = mean_squared_error(y_test_2,y_predicted_2)\n",
    "\n",
    "print('MSE: ', MSE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  302293.82269831584\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "X_3 = wind_ava.filter(regex='^(u|v).*\\.13$', axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "y_3 = wind_ava['energy']\n",
    "\n",
    "train_size = 0.8  # Porcentaje de datos para entrenamiento\n",
    "split_index = int(len(X_3) * train_size)\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "X_train_3 = X_3.iloc[:split_index]\n",
    "X_test_3 = X_3.iloc[split_index:]\n",
    "y_train_3 = y_3.iloc[:split_index]\n",
    "y_test_3 = y_3.iloc[split_index:]\n",
    "\n",
    "# Define the first model as a Pipeline with a preprocessing stage with knn\n",
    "third_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',DecisionTreeRegressor())])\n",
    "third_tree.fit(X_train_3,y_train_3)\n",
    "y_predicted_3 = third_tree.predict(X_test_3)\n",
    "MSE_3 = mean_squared_error(y_test_3,y_predicted_3)\n",
    "\n",
    "print('MSE: ', MSE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous results, it is demonstrated that the error from the model improve applying feature selection. The best results are obtained selecting only the features related to the wind. The second best result is obtained selecting only the features from Sotavento and related with the wind.\n",
    "\n",
    "Based on this results, for the KNN model, only the features related to the wind will be selected (second option). So, from now on, it will be used X_2 and y_2.\n",
    "\n",
    "### 4.3. Imputation techniques\n",
    "\n",
    "Once the feature selection has been implemented, it is time to choose the imputation technique that is going to be used. Automtic techniques will be used (instead of manual imputation). Two types of techniques will be considered:\n",
    "- Univariate imputation. That only uses the values from the feature that is going to be imputed. For this it will be used the Simple Imputer, that imputes all the missing values with the mean of the feature. We have chosen the mean instead of the median because there are not many outliers that could affect the distribution of the data. \n",
    "- Multivariate imputation. Which also uses values from other features. Two multivariate imputation techniques are used:\n",
    "- KNN Imputer. Is based on the KNN algorithm.\n",
    "- Iterative Imputer. Is based on iterative models that compute the values for the missing categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE with :  307378.97239999997\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# FIRST: one univariate technique will be used for imputation, this is done with the Simple Imputer, using the mean\n",
    "\n",
    "simple_tree = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',DecisionTreeRegressor())])\n",
    "\n",
    "simple_tree.fit(X_train_2,y_train_2)\n",
    "y_predicted_2 = simple_tree.predict(X_test_2)\n",
    "MSE_3 = mean_squared_error(y_test_2,y_predicted_2)\n",
    "\n",
    "print('MSE with : ', MSE_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE with :  281559.36892273684\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# SECOND: KNN\n",
    "\n",
    "knn_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',DecisionTreeRegressor())])\n",
    "\n",
    "knn_tree.fit(X_train_2,y_train_2)\n",
    "y_predicted_2 = knn_tree.predict(X_test_2)\n",
    "MSE_3 = mean_squared_error(y_test_2,y_predicted_2)\n",
    "\n",
    "print('MSE with : ', MSE_3)\n",
    "\n",
    "# 280817.7\n",
    "# 281559.36\n",
    "# Why does MSE change???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE with :  265675.5389817895\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "iterative_tree = Pipeline([('imputer',IterativeImputer(max_iter = 14, random_state = 100515585)),('regression',DecisionTreeRegressor())])\n",
    "\n",
    "iterative_tree.fit(X_train_2,y_train_2)\n",
    "y_predicted_2 = iterative_tree.predict(X_test_2)\n",
    "MSE_3 = mean_squared_error(y_test_2,y_predicted_2)\n",
    "\n",
    "print('MSE with : ', MSE_3)\n",
    "\n",
    "# 258502.783\n",
    "# 281559.36\n",
    "# Why does MSE change???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the version of scikit-learn if it returns an Error with IterativeImputer\n",
    "!pip install --upgrade scikit-learn --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Hyperparameter tuning\n",
    "\n",
    "Once the imputation and feature selection have been performed, it is time to improve the performance of the classifier. Hyperparameter tuning will be carried out using crossvalidation as the inner evaluation method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 49 candidates, totalling 245 fits\n",
      "MSE for Fold 1: 222627.01443445848\n",
      "MSE for Fold 2: 178322.77625902492\n",
      "MSE for Fold 3: 216531.19709254926\n",
      "MSE for Fold 4: 203556.7998632973\n",
      "MSE for Fold 5: 217695.15927992587\n",
      "Mean MSE across Folds:  207746.58938585117\n",
      "Best Hyperparameters:  {'max_depth': 4, 'min_samples_split': 2}\n",
      "Ececution time:  203.05187249183655\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "# FIRST STRATEGY: GridSearch \n",
    "# Define the grid with all possible values for the hyperparameters\n",
    "# Define the number of folds and the conditions for the crossvalidation\n",
    "param_grid = {'max_depth': list(range(2,16,2)), 'min_samples_split':list(range(2,16,2))}\n",
    "k_folds = KFold(n_splits = 5, shuffle = True, random_state =100515585)\n",
    "\n",
    "# Measure the time it takes to perform the hyperparameter tuning\n",
    "t_0 = time.time()\n",
    "# Perform the hyperparameter tuning using the previously defined k_folds for crossvalidation\n",
    "tuning = GridSearchCV(DecisionTreeRegressor(),param_grid,scoring='neg_mean_squared_error',cv=k_folds,n_jobs=1,verbose=1)\n",
    "# Fit the models (all possible combinations)\n",
    "tuning.fit(X_train_2,y_train_2)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Access the best estimator\n",
    "best_model = tuning.best_estimator_\n",
    "\n",
    "# Access the best hyperparameters\n",
    "best_params = tuning.best_params_\n",
    "\n",
    "# Access the cross-validated results\n",
    "cv_results = tuning.cv_results_\n",
    "\n",
    "# Print MSE for each fold\n",
    "for i in range(5):\n",
    "    fold_mse = -cv_results[f\"split{i}_test_score\"][tuning.best_index_]\n",
    "    print(f\"MSE for Fold {i+1}: {fold_mse}\")\n",
    "\n",
    "# Calculate and print the mean MSE across folds\n",
    "mean_mse = -cv_results[\"mean_test_score\"][tuning.best_index_]\n",
    "\n",
    "\n",
    "# Print the results for the best combination\n",
    "print(\"Mean MSE across Folds: \", mean_mse)\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "\n",
    "# ESTIMATE FUTURE PERFORMANCE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OPTUNA if required\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-28 11:35:50,593] A new study created in memory with name: no-name-50b508b3-512b-4524-8e9b-72f587920937\n",
      "[I 2023-12-28 11:35:52,361] Trial 0 finished with value: -362.2456325154089 and parameters: {'max_depth': 8, 'min_samples_split': 11}. Best is trial 0 with value: -362.2456325154089.\n",
      "[I 2023-12-28 11:35:54,114] Trial 1 finished with value: -363.6241127058186 and parameters: {'max_depth': 16, 'min_samples_split': 13}. Best is trial 0 with value: -362.2456325154089.\n",
      "[I 2023-12-28 11:35:55,898] Trial 2 finished with value: -366.1089579829439 and parameters: {'max_depth': 12, 'min_samples_split': 6}. Best is trial 0 with value: -362.2456325154089.\n",
      "[I 2023-12-28 11:35:57,115] Trial 3 finished with value: -359.9417057308623 and parameters: {'max_depth': 3, 'min_samples_split': 10}. Best is trial 3 with value: -359.9417057308623.\n",
      "[I 2023-12-28 11:35:58,905] Trial 4 finished with value: -365.1028727569707 and parameters: {'max_depth': 14, 'min_samples_split': 2}. Best is trial 3 with value: -359.9417057308623.\n",
      "[I 2023-12-28 11:36:00,643] Trial 5 finished with value: -362.3098126848 and parameters: {'max_depth': 9, 'min_samples_split': 16}. Best is trial 3 with value: -359.9417057308623.\n",
      "[I 2023-12-28 11:36:01,861] Trial 6 finished with value: -359.9417057308623 and parameters: {'max_depth': 3, 'min_samples_split': 2}. Best is trial 3 with value: -359.9417057308623.\n",
      "[I 2023-12-28 11:36:03,633] Trial 7 finished with value: -365.08320539795506 and parameters: {'max_depth': 15, 'min_samples_split': 6}. Best is trial 3 with value: -359.9417057308623.\n",
      "[I 2023-12-28 11:36:05,403] Trial 8 finished with value: -364.1521942998546 and parameters: {'max_depth': 13, 'min_samples_split': 11}. Best is trial 3 with value: -359.9417057308623.\n",
      "[I 2023-12-28 11:36:06,955] Trial 9 finished with value: -355.3939398530371 and parameters: {'max_depth': 4, 'min_samples_split': 14}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:08,654] Trial 10 finished with value: -358.7280867566746 and parameters: {'max_depth': 6, 'min_samples_split': 16}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:10,300] Trial 11 finished with value: -356.91120940069925 and parameters: {'max_depth': 5, 'min_samples_split': 16}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:11,946] Trial 12 finished with value: -356.7771004201741 and parameters: {'max_depth': 5, 'min_samples_split': 14}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:13,636] Trial 13 finished with value: -358.8141445446348 and parameters: {'max_depth': 6, 'min_samples_split': 13}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:14,467] Trial 14 finished with value: -403.39470310623767 and parameters: {'max_depth': 2, 'min_samples_split': 13}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:16,258] Trial 15 finished with value: -361.73032620569904 and parameters: {'max_depth': 11, 'min_samples_split': 7}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:17,905] Trial 16 finished with value: -356.7771004201741 and parameters: {'max_depth': 5, 'min_samples_split': 14}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:19,620] Trial 17 finished with value: -361.3273180927179 and parameters: {'max_depth': 7, 'min_samples_split': 9}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:21,384] Trial 18 finished with value: -361.82970288517856 and parameters: {'max_depth': 9, 'min_samples_split': 14}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:22,942] Trial 19 finished with value: -355.3939398530371 and parameters: {'max_depth': 4, 'min_samples_split': 11}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:23,774] Trial 20 finished with value: -403.39470310623767 and parameters: {'max_depth': 2, 'min_samples_split': 11}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:25,331] Trial 21 finished with value: -355.3939398530371 and parameters: {'max_depth': 4, 'min_samples_split': 14}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:26,890] Trial 22 finished with value: -355.3939398530371 and parameters: {'max_depth': 4, 'min_samples_split': 12}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:28,452] Trial 23 finished with value: -355.3939398530371 and parameters: {'max_depth': 4, 'min_samples_split': 8}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:30,165] Trial 24 finished with value: -360.90436184275 and parameters: {'max_depth': 7, 'min_samples_split': 15}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:31,384] Trial 25 finished with value: -359.9417057308623 and parameters: {'max_depth': 3, 'min_samples_split': 12}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:33,162] Trial 26 finished with value: -363.132252501726 and parameters: {'max_depth': 10, 'min_samples_split': 9}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:34,729] Trial 27 finished with value: -355.3939398530371 and parameters: {'max_depth': 4, 'min_samples_split': 12}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:36,462] Trial 28 finished with value: -360.90436184275 and parameters: {'max_depth': 7, 'min_samples_split': 15}. Best is trial 9 with value: -355.3939398530371.\n",
      "[I 2023-12-28 11:36:38,218] Trial 29 finished with value: -362.41406473022096 and parameters: {'max_depth': 8, 'min_samples_split': 10}. Best is trial 9 with value: -355.3939398530371.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE across Folds:  242465.54145307126\n",
      "Best Hyperparameters:  {'max_depth': 4, 'min_samples_split': 14}\n",
      "Execution time:  47.62669634819031\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# SECOND STRATEGY: Optuna\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameters that are going to be tuned\n",
    "    max_depth = trial.suggest_int('max_depth',2,16)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split',2,16)\n",
    "    \n",
    "    # Estimator with suggested hyperparameters\n",
    "    params = {'max_depth': max_depth, 'min_samples_split':min_samples_split}\n",
    "    regr_tree = DecisionTreeRegressor(random_state = 100515585, **params)\n",
    "    \n",
    "    # Define the neg means squared error as the score and the inner evaluation as corssvalidation \n",
    "    inner_score = cross_val_score(regr_tree,X_train_2,y_train_2,cv=k_folds,scoring='neg_mean_absolute_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "# Train the model and perform HPO\n",
    "# Measure the time\n",
    "t_0 = time.time()\n",
    "sampler = optuna.samplers.TPESampler(seed=100515585)\n",
    "study = optuna.create_study(direction='maximize',sampler=sampler)\n",
    "iterations = 30\n",
    "study.optimize(objective, n_trials = iterations)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "regr_opt = DecisionTreeRegressor(random_state = 100515585, **best_params)\n",
    "regr_opt.fit(X_train_2,y_train_2)\n",
    "\n",
    "# ESTIMATE FUTURE PERFORMANCE\n",
    "y_test_pred = regr_opt.predict(X_test_2)\n",
    "mse_future = mean_squared_error(y_test_pred, y_test_2)\n",
    "\n",
    "# Print results\n",
    "# Print the results for the best combination\n",
    "print(\"MSE future performance: \", mse_future)\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Execution time: \",t_1-t_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
      "     -------------------------------------- 413.4/413.4 kB 6.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from optuna) (4.64.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\admin\\anaconda3\\lib\\site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from optuna) (21.3)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from optuna) (1.4.39)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "     ------------------------------------- 233.4/233.4 kB 13.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\anaconda3\\lib\\site-packages (from optuna) (1.21.5)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.6/78.6 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from packaging>=20.0->optuna) (3.0.9)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.0 alembic-1.13.1 colorlog-6.8.0 optuna-3.5.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "t_0 = time.time()\n",
    "# Create the ensemble model pipeline\n",
    "random_forest = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42)) \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bagging Regressor with KNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# Create the ensemble model pipeline\n",
    "knn_ensemble_pipeline = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', BaggingRegressor(base_estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=10, random_state=42))  \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_ensemble_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = knn_ensemble_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
