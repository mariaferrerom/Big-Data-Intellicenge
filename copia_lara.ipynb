{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 1\n",
    "\n",
    "## Lara Monteserín Placer\n",
    "\n",
    "## María Ferrero Medina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this project is to design a machine learning model that is able to predict the energy produced by the Sotavento wind farm. For this purpose, a dataset with 555 features ans 4748 instances is available.\n",
    "\n",
    "The structure of the study will be the following:\n",
    "\n",
    "1. Exploratory Data Analysis\n",
    "2. Methodology\n",
    "3. KNN model\n",
    "4. Decission tree model\n",
    "5. Ensemble method 1 model\n",
    "6. Ensemble method 2 model\n",
    "7. Selection and performance of the final model\n",
    "\n",
    "For each of the models created, several steps have been followed to optimize them. First of all, a simple version of each model is created, with hyperparameters that seem reasonable, no feature selection and a basic imputation technique. Then, sequentially, models are improved.\n",
    "\n",
    "1. First model\n",
    "2. Feature selection\n",
    "3. Imputation techniques\n",
    "4. Hyperparameter tuning\n",
    "\n",
    "\n",
    "Things to add: - another idea (new library?, new idea...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-hPXBr4r5rS"
   },
   "source": [
    "\n",
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "Before starting to build the model, an EDA is made as a first approach to gain understanding of the dataset. In this Exploratory Data Analysis the data type of the features will be verified, the number of instances and features will be determined. Also, a brief summary of the missing values and columns with constant value will be included. \n",
    "\n",
    "### 1.1. Number of instances and features\n",
    "\n",
    "This dataset has 4748 instances and 555 features.\n",
    "\n",
    "### 1.2. Nature of the variables\n",
    "\n",
    "This dataset contains information about the meteorological conditions in several locations, the time the measures of these conditions were made and the energy produced at each moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy     float64\n",
      "year         int64\n",
      "month        int64\n",
      "day          int64\n",
      "hour         int64\n",
      "            ...   \n",
      "v100.21    float64\n",
      "v100.22    float64\n",
      "v100.23    float64\n",
      "v100.24    float64\n",
      "v100.25    float64\n",
      "Length: 555, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Read the data that is compressed as a gzip\n",
    "wind_ava = pd.read_csv('wind_available.csv.gzip', compression=\"gzip\")\n",
    "\n",
    "# Display the first rows of the dataset just to see it\n",
    "wind_ava.head()\n",
    "\n",
    "# Display the data type of each column\n",
    "column_data_types = wind_ava.dtypes\n",
    "print(column_data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having checked the data types of all the different features, it has been verified that there are:\n",
    "\n",
    "- 551 numerical variables (real numbers). From this 551, one is the energy, that is the output of the problem. And the remaining 550 are relative to the 22 different meteorological conditions measured at the 25 different locations.\n",
    "\n",
    "- 4 numerical variables (integers). These 4 variables are the year, day, month and hour of the day. These variables characterize the moment the measures were taken.\n",
    "\n",
    "### 1.3. Check for missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Column  Null Values  NaN Values\n",
      "0     energy            0           0\n",
      "1       year            0           0\n",
      "2      month            0           0\n",
      "3        day            0           0\n",
      "4       hour            0           0\n",
      "..       ...          ...         ...\n",
      "550  v100.21          261         261\n",
      "551  v100.22          387         387\n",
      "552  v100.23          569         569\n",
      "553  v100.24          579         579\n",
      "554  v100.25          436         436\n",
      "\n",
      "[555 rows x 3 columns]\n",
      "Number of columns with Null Values: 550\n",
      "Number of columns with NaN Values: 550\n"
     ]
    }
   ],
   "source": [
    "# Return the number of Null values for each column\n",
    "null_values = wind_ava.isnull().sum()\n",
    "# Return the number of NaN values for each column (just in case they are not the same)\n",
    "nan_values = wind_ava.isna().sum()\n",
    "\n",
    "# Store in missing values the amount of Null and NaN values of each column\n",
    "missing_values = pd.DataFrame({\n",
    "    'Column': null_values.index,\n",
    "    'Null Values': null_values.values,\n",
    "    'NaN Values': nan_values.values\n",
    "})\n",
    "\n",
    "# Print the amount of Null and Nan values\n",
    "print(missing_values)\n",
    "\n",
    "# Identify columns with Null or NaN values\n",
    "columns_with_null = wind_ava.columns[wind_ava.isnull().any()]\n",
    "columns_with_nan = wind_ava.columns[wind_ava.isna().any()]\n",
    "\n",
    "# Display the number of columns which have missing values\n",
    "print(\"Number of columns with Null Values:\", len(columns_with_null))\n",
    "print(\"Number of columns with NaN Values:\", len(columns_with_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All meteorological variables have missing values in different instances. The 4 categories that characterize the moment the measure was made and the target feature'energy' do not have missing values.\n",
    "\n",
    "\n",
    "### 1.4. Check for constant columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for constant values in each column\n",
    "constant_columns = wind_ava.columns[wind_ava.nunique() == 1]\n",
    "\n",
    "# Print columns with constant values\n",
    "print(\"Columns with constant values:\", constant_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are no constant columns. \n",
    "\n",
    "\n",
    "### 1.5. Type of problem\n",
    "\n",
    "The objective of the model is to estimate the energy, as it is a continuous numerical value, this is a **regression problem**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "\n",
    "This section will explain the methodology that is going to be followed to evaluate the models. The evaluation techniques that will be used for outer evaluation and inner evaluation. And also the metrics that will determine the future performance of the model.\n",
    "\n",
    "- On the one hand, for the **inner evaluation**, crossvalidation will be the method applied. Crossvalidation will be used to determine which is the best combination of hyperparameters. \n",
    "\n",
    "- On the other hand, for the **outer evaluation**, holdout evaluation will be used. Thie method will be used to estimate the future performance of the designed method.\n",
    "\n",
    "Later, to improve the performance of the method, once it is already computed the outer performance, the hyperparameters will be tuned again but this time using the whole dataset.\n",
    "\n",
    "The objective function that is going to be used for the validation of the method is the Mean Squared Error (MSE). This metric is more sensitive to outliers and to distant values as it squares the magnitudes. This is useful to penalize the errors that are larger, and avoid having a model that might have such large errors. \n",
    "\n",
    "**It could be longer this explanation**\n",
    "\n",
    "Note: as the variable that is going to be predicted is the 'energy', only the variables related to the meteorological characteristics will be used. It is considered that the energy produced does not depend on the moment of the day it is being produced.\n",
    "\n",
    "## 3. KNN Regressor\n",
    "\n",
    "First, the KNN algorithm is used for the predictions.\n",
    "\n",
    "### 3.1. First model\n",
    "\n",
    "A first model using the KNN algorithm will be implemented. It will use the default hyperparameters, the KNN Imputer as imputation strategy and no feature selection. This model will be updated and improved but it is just a simple approach to compare the adjusted models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  398902.9111835073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#Here, we define a generic random seed that will be used along the assignment\n",
    "rs = 100515585 \n",
    "\n",
    "# First, data will be divided into train and test set \n",
    "# Considering it is a time series, it must be split in an appropiate way\n",
    "wind_ava['timestamp'] = pd.to_datetime(wind_ava[['year', 'month', 'day', 'hour']])\n",
    "wind_ava = wind_ava.sort_values(by='timestamp')\n",
    "\n",
    "train_size = 0.8 \n",
    "split_index = int(len(wind_ava) * train_size)\n",
    "wind_ava = wind_ava.drop(columns=['timestamp'])\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "train_data = wind_ava.iloc[:split_index]\n",
    "test_data = wind_ava.iloc[split_index:]\n",
    "\n",
    "X_train = train_data.drop('energy', axis=1)\n",
    "y_train = train_data['energy']\n",
    "X_test = test_data.drop('energy', axis=1)\n",
    "y_test = test_data['energy']\n",
    "\n",
    "# Now the first model will be created\n",
    "first_knn = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',KNeighborsRegressor())])\n",
    "first_knn.fit(X_train,y_train)\n",
    "y_predicted = first_knn.predict(X_test)\n",
    "MSE = mean_squared_error(y_test,y_predicted)\n",
    "\n",
    "print('MSE: ', MSE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature selection\n",
    "\n",
    "Now, feature selection will be used in order to remove variables that are not affecting directly the target. This is done manually, selecting different feature combinations that seem to be more accurate for the problem. Three different cases have been considered:\n",
    "- FIRST OPTION. Selecting only the features related to the location of the wind farm (Sotavento). This is the features that contain the sufix 13.\n",
    "- SECOND OPTION. Selecting only the features related to the wind characteristics. This is the features that start with u or v (the vertical and horizontal components of the wind).\n",
    "- THIRD OPTION. Selecting only the features which are both located in Sotavento (location 13) and related to the wind characteristics. This is the features that start with u or v and end with sufix 13.\n",
    "\n",
    "This strategy will be followed for the feature selection of the rest of the models. This is sections 4.2, 5.2 and 6.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  442428.52588486316\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "X_1 = wind_ava.filter(regex='\\.13$', axis=1)\n",
    "\n",
    "# Selecting the target\n",
    "y_1 = wind_ava['energy']\n",
    "\n",
    "train_size = 0.8\n",
    "split_index = int(len(X_1) * train_size)\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "X_train_1 = X_1.iloc[:split_index]\n",
    "X_test_1 = X_1.iloc[split_index:]\n",
    "y_train_1 = y_1.iloc[:split_index]\n",
    "y_test_1 = y_1.iloc[split_index:]\n",
    "\n",
    "# Now the first model will be created\n",
    "first_knn = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',KNeighborsRegressor())])\n",
    "first_knn.fit(X_train_1,y_train_1)\n",
    "y_predicted_1 = first_knn.predict(X_test_1)\n",
    "MSE = mean_squared_error(y_test_1,y_predicted_1)\n",
    "\n",
    "print('MSE: ', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  178196.27234742316\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "X_2 = wind_ava.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "y_2 = wind_ava['energy']\n",
    "\n",
    "train_size = 0.8 \n",
    "split_index = int(len(X_2) * train_size)\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "X_train_2 = X_2.iloc[:split_index]\n",
    "X_test_2 = X_2.iloc[split_index:]\n",
    "y_train_2 = y_2.iloc[:split_index]\n",
    "y_test_2 = y_2.iloc[split_index:]\n",
    "\n",
    "# Define the first model as a Pipeline with a preprocessing stage with knn\n",
    "second_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',KNeighborsRegressor())])\n",
    "second_tree.fit(X_train_2,y_train_2)\n",
    "y_predicted_2 = second_tree.predict(X_test_2)\n",
    "MSE_2 = mean_squared_error(y_test_2,y_predicted_2)\n",
    "\n",
    "print('MSE: ', MSE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  191894.0329901347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "X_3 = wind_ava.filter(regex='^(u|v).*\\.13$', axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "y_3 = wind_ava['energy']\n",
    "\n",
    "train_size = 0.8  # Porcentaje de datos para entrenamiento\n",
    "split_index = int(len(X_3) * train_size)\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "X_train_3 = X_3.iloc[:split_index]\n",
    "X_test_3 = X_3.iloc[split_index:]\n",
    "y_train_3 = y_3.iloc[:split_index]\n",
    "y_test_3 = y_3.iloc[split_index:]\n",
    "\n",
    "# Define the first model as a Pipeline with a preprocessing stage with knn\n",
    "third_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',KNeighborsRegressor())])\n",
    "third_tree.fit(X_train_3,y_train_3)\n",
    "y_predicted_3 = third_tree.predict(X_test_3)\n",
    "MSE_3 = mean_squared_error(y_test_3,y_predicted_3)\n",
    "\n",
    "print('MSE: ', MSE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results above show that the best feature selection strategy is the second, selecting only the features related to the wind. It improves the performance of the model as it is not considering features that are not affecting the target and introduce unnecessary noise and lead to overfitting.\n",
    "\n",
    "From this moment, the KNN model will be built using only the features related to the wind (that is X_train_2, y_train_2, X_test_2, y_test_2).\n",
    "\n",
    "### 3.3. Imputation techniques\n",
    "\n",
    "Once the feature selection has been implemented, it is time to choose the imputation technique that is going to be used. \n",
    "\n",
    "Two different strategies will be considered, that belong to two different types of techniques:\n",
    "\n",
    "- Univariate imputation. That only uses the values from the feature that is going to be imputed. For this it will be used the Simple Imputer, that imputes all the missing values with the mean of the feature. We have chosen the mean instead of the median because there are not many outliers that could affect the distribution of the data.\n",
    "\n",
    "\n",
    "- Multivariate imputation. Which uses values from the feature that is going to be imputed and also from other features. Two multivariate imputation techniques are used:\n",
    "    - KNN Imputer. Is based on the KNN algorithm.\n",
    "    - Iterative Imputer. Is based on iterative models that compute the values for the missing categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# FIRST OPTION: one univariate technique will be used for imputation, this is done with the Simple Imputer, using the mean\n",
    "simple_knn = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',KNeighborsRegressor())])\n",
    "\n",
    "simple_knn.fit(X_train_2,y_train_2)\n",
    "y_predicted_2 = simple_knn.predict(X_test_2)\n",
    "MSE_3 = mean_squared_error(y_test_2,y_predicted_2)\n",
    "\n",
    "print('MSE with Simple Imputer: ', MSE_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# SECOND OPTION: multivariate technique. KNN Imputer using 3 neighbors\n",
    "\n",
    "knn_knn = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',KNeighborsRegressor())])\n",
    "\n",
    "knn_knn.fit(X_train_2,y_train_2)\n",
    "y_predicted_2 = knn_knn.predict(X_test_2)\n",
    "MSE_3 = mean_squared_error(y_test_2,y_predicted_2)\n",
    "\n",
    "print('MSE with : ', MSE_3)\n",
    "\n",
    "# Result: 178196"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the version of scikit-learn if it returns an Error with IterativeImputer\n",
    "!pip install --upgrade scikit-learn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# THIRD OPTION: multivariate technique. KNN Imputer using 3 neighbors\n",
    "\n",
    "iterative_knn = Pipeline([('imputer',IterativeImputer(max_iter = 14, random_state = rs)),('regression',KNeighborsRegressor())])\n",
    "\n",
    "iterative_knn.fit(X_train_2,y_train_2)\n",
    "y_predicted_2 = iterative_knn.predict(X_test_2)\n",
    "MSE_3 = mean_squared_error(y_test_2,y_predicted_2)\n",
    "\n",
    "print('MSE with : ', MSE_3)\n",
    "\n",
    "# Result: 174930"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, it is demonstrated that the simple imputer is the worst in terms of performance with respect to the Mean Squared Error.\n",
    "\n",
    "On the other hand, the multivariate techniques, KNN Imputer and the Iterative Imputer give similar results, but the Iterative Imputer has a lower Mean Squared Error.\n",
    "\n",
    "From this point on, the imputation technique will be the Iterative Imputation, so the pipeline will be knn_knn.\n",
    "\n",
    "### 3.4. Hyperparameter tuning\n",
    "\n",
    "Now we will perform a crucial stage, that is to optimize the model by performing hyperparameter tuning. In the case of KNN algorithm, the main hyperparameter is the number of neighbors.\n",
    "\n",
    "Several approaches for hyperparameter tuning will be implemented. The optimization time will be measured and also the best combination of hyperparameters for each method will be compared to determine which HPO technique is the most suitable. \n",
    "\n",
    "For HPO, Pipelines will not be used in order to focus on the critical hyperparameters of each step. The idea is to get the most effective optimization. First, the function to optimize will be created and so will the parameter grid. Also, the imputation will be made for the training and testing data. This will be done first to save execution time on each occasion the HPO is done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS TAKES ~40s\n",
    "# IMPUTATION\n",
    "\n",
    "# Perform the imputation for training data (outside the pipeline)\n",
    "imputer = IterativeImputer(max_iter=14, random_state=rs)\n",
    "X_train_imputed = imputer.fit_transform(X_train_2)\n",
    "\n",
    "# Imputate also for the test set to estimate future performance\n",
    "X_test_imputed = imputer.fit_transform(X_test_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL TO OPTIMIZE\n",
    "\n",
    "# Create the regressor with no hyperparamters defined\n",
    "knn_default = KNeighborsRegressor()\n",
    "\n",
    "# Create a dictionary with the values for the hyperparameters\n",
    "params = {'n_neighbors': np.arange(1, 31)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST STRATEGY - Randomized Search\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create a randomized search instance\n",
    "random_search = RandomizedSearchCV(\n",
    "    knn_default,\n",
    "    param_distributions=params,\n",
    "    n_iter=10,  # Number of iterations (can be adjusted)\n",
    "    cv=5,       # Number of cross-validation folds\n",
    "    scoring='neg_mean_squared_error',  # Mean squared error is the error metric\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Train the model, optimize the hyperparaneters and measure the time\n",
    "t_0 = time.time()\n",
    "random_search.fit(X_train_imputed, y_train_2)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Estimation of future performance\n",
    "y_test_pred = random_search.predict(X_test_imputed)\n",
    "\n",
    "print(\"KNN's MSE with HPO: \",\n",
    "     mean_squared_error(y_test_2, y_test_pred))\n",
    "\n",
    "print(\"Best Hyperparameters: \", random_search.best_params_)\n",
    "\n",
    "# print(\"MSE inner evaluation: \", mse_future)\n",
    "print(\"Execution times: \",t_1-t_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Poniendo n_neighbors = trial.suggest_int('n_neighbors', 1, 3) tarda 970 segundazos\n",
    "# TAKES LONG TO EXECUTE !-!-!-!\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# SECOND STRATEGY - Optuna\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameters that are to be tuned\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 31)\n",
    "    params = {'n_neighbors': n_neighbors}\n",
    "    \n",
    "    # Estimator with suggested hyperparameters\n",
    "    knn_default = KNeighborsRegressor()\n",
    "    \n",
    "    # Define the neg means squared error as the score and the inner evaluation as corssvalidation \n",
    "    inner_score = cross_val_score(iterative_knn, X_train_imputed, y_train_2, cv=5, scoring='neg_mean_absolute_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "# Train the model and perform HPO\n",
    "# Measure the time\n",
    "sampler = optuna.samplers.TPESampler(seed=rs)\n",
    "study = optuna.create_study(direction='maximize',sampler=sampler)\n",
    "\n",
    "iterations = 30\n",
    "t_0 = time.time()\n",
    "study.optimize(objective, n_trials = iterations)\n",
    "t_1 = time.time()\n",
    "\n",
    "\n",
    "print('5: ', time.time())\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "print('6: ', time.time())\n",
    "\n",
    "regr_opt = KNeighborsRegressor(**best_params)\n",
    "\n",
    "\n",
    "print('7: ', time.time())\n",
    "\n",
    "regr_opt.fit(X_train_imputed,y_train_2)\n",
    "\n",
    "print('8: ', time.time())\n",
    "\n",
    "# Estimate future performance\n",
    "y_test_pred = regr_opt.predict(X_test_imputed)\n",
    "mse_future = mean_squared_error(y_test_pred, y_test_2)\n",
    "\n",
    "# Print results\n",
    "# Print the results for the best combination\n",
    "print(\"MSE future performance: \", mse_future)\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Execution time: \",t_1-t_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS TAKES A LONG TIME !-!-!\n",
    "\n",
    "# THIRD STRATEGY - Successive halving\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "\n",
    "# Create\n",
    "halving = HalvingGridSearchCV(knn_default,\n",
    "                              params,\n",
    "                              scoring= 'neg_mean_squared_error',\n",
    "                              cv=5,\n",
    "                              random_state=rs,\n",
    "                              factor=2,\n",
    "                              min_resources='exhaust',\n",
    "                              max_resources='auto',\n",
    "                              n_jobs=-1, verbose=1)\n",
    "\n",
    "halving.fit(X_train_imputed, y_train_2)\n",
    "\n",
    "# Estimation of future performance\n",
    "# Measure the ex time\n",
    "t_0=time.time()\n",
    "y_test_pred = halving.predict(X_test_imputed)\n",
    "t_1=time.time()\n",
    "\n",
    "print(\"KNN's MSE with HPO: \",\n",
    "     mean_squared_error(y_test_2, y_test_pred))\n",
    "print(\"best_params\", halving.best_params_)\n",
    "print(\"Execution times: \",t_1-t_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Conclusions about KNN model\n",
    "\n",
    "- Hyperparameter tuning: the fastest method and the one that reaches the optimal solution\n",
    "- MSE\n",
    "\n",
    "Out of all these trials, we will now use the best model obtained from this study in order to compare it with the remaining models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Tree\n",
    "\n",
    "### 4.1. First model\n",
    "\n",
    "A first model using the Decision Tree algorithm will be implemented. It will use the default hyperparameters, the KNN Imputer as imputation strategy and no feature selection. This model will be updated and improved but it is just a simple approach to compare the adjusted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  284418.83268557896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# FIRST OPTION - Univariate technique \n",
    "# Define the first tree as a Pipeline with a preprocessing stage with decision tree\n",
    "first_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "first_tree.fit(X_train,y_train)\n",
    "y_predicted = first_tree.predict(X_test)\n",
    "\n",
    "MSE = mean_squared_error(y_test,y_predicted)\n",
    "\n",
    "print('MSE: ', MSE)\n",
    "\n",
    "#284418"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Feature selection\n",
    "\n",
    "For the feature selection, as in the case before, three different cases have been considered:\n",
    "- FIRST OPTION. Selecting only the features related to the location of the wind farm (Sotavento). This is the features that contain the sufix 13.\n",
    "- SECOND OPTION. Selecting only the features related to the wind characteristics. This is the features that start with u or v (the vertical and horizontal components of the wind).\n",
    "- THIRD OPTION. Selecting only the features which are both located in Sotavento (location 13) and related to the wind characteristics. This is the features that start with u or v and end with sufix 13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  320021.95956105256\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "X_1 = wind_ava.filter(regex='\\.13$', axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "y_1 = wind_ava['energy']\n",
    "\n",
    "train_size = 0.8  # Porcentaje de datos para entrenamiento\n",
    "split_index = int(len(X_1) * train_size)\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "X_train_1 = X_1.iloc[:split_index]\n",
    "X_test_1 = X_1.iloc[split_index:]\n",
    "y_train_1 = y_1.iloc[:split_index]\n",
    "y_test_1 = y_1.iloc[split_index:]\n",
    "\n",
    "# Define the first model as a Pipeline with a preprocessing stage with knn\n",
    "# first_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('feature_selection',SelectKBest(f_regression)),('regression',DecisionTreeRegressor())])\n",
    "first_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "first_tree.fit(X_train_1,y_train_1)\n",
    "y_predicted_1 = first_tree.predict(X_test_1)\n",
    "MSE_1 = mean_squared_error(y_test_1,y_predicted_1)\n",
    "\n",
    "print('MSE: ', MSE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  269120.25575199997\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "X_2 = wind_ava.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "y_2 = wind_ava['energy']\n",
    "\n",
    "train_size = 0.8\n",
    "split_index = int(len(X_2) * train_size)\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "X_train_2 = X_2.iloc[:split_index]\n",
    "X_test_2 = X_2.iloc[split_index:]\n",
    "y_train_2 = y_2.iloc[:split_index]\n",
    "y_test_2 = y_2.iloc[split_index:]\n",
    "\n",
    "# Define the first model as a Pipeline with a preprocessing stage with knn\n",
    "second_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "second_tree.fit(X_train_2,y_train_2)\n",
    "y_predicted_2 = second_tree.predict(X_test_2)\n",
    "MSE_2 = mean_squared_error(y_test_2,y_predicted_2)\n",
    "\n",
    "print('MSE: ', MSE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  298762.4396745263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "X_3 = wind_ava.filter(regex='^(u|v).*\\.13$', axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "y_3 = wind_ava['energy']\n",
    "\n",
    "train_size = 0.8  # Porcentaje de datos para entrenamiento\n",
    "split_index = int(len(X_3) * train_size)\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "X_train_3 = X_3.iloc[:split_index]\n",
    "X_test_3 = X_3.iloc[split_index:]\n",
    "y_train_3 = y_3.iloc[:split_index]\n",
    "y_test_3 = y_3.iloc[split_index:]\n",
    "\n",
    "# Define the first model as a Pipeline with a preprocessing stage with knn\n",
    "third_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "third_tree.fit(X_train_3,y_train_3)\n",
    "y_predicted_3 = third_tree.predict(X_test_3)\n",
    "MSE_3 = mean_squared_error(y_test_3,y_predicted_3)\n",
    "\n",
    "print('MSE: ', MSE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous results, we can see that the best feature selection is the second, selecting only the features related to the wind.\n",
    "\n",
    "Based on this, the Decision Tree model will be built using features related to the wind (that is X_train_2, y_train_2, X_test_2, y_test_2).\n",
    "\n",
    "The best MSE achieved until this point is: 269120.25\n",
    "\n",
    "### 4.3. Imputation techniques\n",
    "\n",
    "Now it is time to choose the imputation technique that is going to be used. Authomatic techniques will be used (instead of manual imputation).\n",
    "\n",
    "Two types of techniques will be considered:\n",
    "- Univariate imputation. That only uses the values from the feature that is going to be imputed. For this it will be used the Simple Imputer, that imputes all the missing values with the mean of the feature. We have chosen the mean instead of the median because there are not many outliers that could affect the distribution of the data.\n",
    "\n",
    "- Multivariate imputation. Which also uses values from other features. Two multivariate imputation techniques are used:\n",
    "    - KNN Imputer. Is based on the KNN algorithm.\n",
    "    - Iterative Imputer. Is based on iterative models that compute the values for the missing categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE :  294190.42834821047\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# FIRST: one univariate technique will be used for imputation, this is done with the Simple Imputer, using the mean\n",
    "\n",
    "simple_tree = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "\n",
    "simple_tree.fit(X_train_2,y_train_2)\n",
    "y_predicted = simple_tree.predict(X_test_2)\n",
    "MSE = mean_squared_error(y_test_2,y_predicted)\n",
    "\n",
    "print('MSE : ', MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE :  269120.25575199997\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# SECOND: multivariate technique. KNN Imputer using 3 neighbors\n",
    "\n",
    "knn_tree = Pipeline([('imputer',KNNImputer(n_neighbors=3)),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "\n",
    "knn_tree.fit(X_train_2,y_train_2)\n",
    "y_predicted = knn_tree.predict(X_test_2)\n",
    "MSE = mean_squared_error(y_test_2,y_predicted)\n",
    "\n",
    "print('MSE : ', MSE)\n",
    "\n",
    "# 280817.7\n",
    "# 281559.36\n",
    "# Why does MSE change???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE :  255083.116514\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "iterative_tree = Pipeline([('imputer',IterativeImputer(max_iter = 14, random_state = rs)),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "\n",
    "iterative_tree.fit(X_train_2,y_train_2)\n",
    "y_predicted = iterative_tree.predict(X_test_2)\n",
    "MSE = mean_squared_error(y_test_2,y_predicted)\n",
    "\n",
    "print('MSE : ', MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above, we can see that the simple imputer is the worst in terms of performance with respect to the Mean Squared Error.\n",
    "\n",
    "On the other hand, the multivariate techniques: KNN Imputer and the Iterative Imputer give similar results, but the Iterative Imputer has a lower Mean Squared Error. From this point on, the imputation technique will be the Iterative Imputation, so the pipeline will be knn_tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Hyperparameter tuning\n",
    "\n",
    "Once the imputation and feature selection have been performed, it is time to improve the performance of the classifier. Hyperparameter tuning will be carried out using crossvalidation as the inner evaluation method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPUTATION\n",
    "\n",
    "# Perform the imputation for training data (outside the pipeline)\n",
    "imputer = IterativeImputer(max_iter=14, random_state=rs)\n",
    "X_train_imputed = imputer.fit_transform(X_train_2)\n",
    "\n",
    "# Imputate also for the test set to estimate future performance\n",
    "X_test_imputed = imputer.fit_transform(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL TO OPTIMIZE\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create the regressor with no hyperparamters defined\n",
    "tree_default = DecisionTreeRegressor()\n",
    "\n",
    "# Create a dictionary with the values for the hyperparameters\n",
    "# Define the grid with all possible values for the hyperparameters\n",
    "# Define the number of folds and the conditions for the crossvalidation\n",
    "param_grid = {'max_depth': list(range(2,16,2)), 'min_samples_split':list(range(2,16,2))}\n",
    "k_folds = KFold(n_splits = 5, shuffle = True, random_state =rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 49 candidates, totalling 245 fits\n",
      "MSE for Fold 1: 163717.66342913502\n",
      "MSE for Fold 2: 144317.58109068763\n",
      "MSE for Fold 3: 167130.39533087678\n",
      "MSE for Fold 4: 168761.23816298658\n",
      "MSE for Fold 5: 164268.05666073156\n",
      "Mean MSE across Folds:  161638.98693488352\n",
      "Best Hyperparameters:  {'max_depth': 6, 'min_samples_split': 12}\n",
      "Execution time:  92.45113515853882\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "# FIRST STRATEGY: GridSearch \n",
    "\n",
    "# Measure the time it takes to perform the hyperparameter tuning\n",
    "t_0 = time.time()\n",
    "# Perform the hyperparameter tuning using the previously defined k_folds for crossvalidation\n",
    "tuning = GridSearchCV(\n",
    "    tree_default,\n",
    "    param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=k_folds,\n",
    "    n_jobs=1,verbose=1)\n",
    "\n",
    "# Fit the models (all possible combinations)\n",
    "tuning.fit(X_train_imputed,y_train_2)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Access the best estimator\n",
    "best_model = tuning.best_estimator_\n",
    "\n",
    "# Access the best hyperparameters\n",
    "best_params = tuning.best_params_\n",
    "\n",
    "# Access the cross-validated results\n",
    "cv_results = tuning.cv_results_\n",
    "\n",
    "# Print MSE for each fold\n",
    "for i in range(5):\n",
    "    fold_mse = -cv_results[f\"split{i}_test_score\"][tuning.best_index_]\n",
    "    print(f\"MSE for Fold {i+1}: {fold_mse}\")\n",
    "\n",
    "# Calculate and print the mean MSE across folds\n",
    "mean_mse = -cv_results[\"mean_test_score\"][tuning.best_index_]\n",
    "\n",
    "\n",
    "# Print the results for the best combination\n",
    "print(\"Mean MSE across Folds: \", mean_mse)\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "\n",
    "# ESTIMATE FUTURE PERFORMANCE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN's MSE with HPO:  189639.60576969493\n"
     ]
    }
   ],
   "source": [
    "# Estimation of future performance\n",
    "y_test_pred = tuning.predict(X_test_imputed)\n",
    "\n",
    "print(\"KNN's MSE with HPO: \",\n",
    "     mean_squared_error(y_test_2, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OPTUNA if required\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# SECOND STRATEGY: Optuna\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameters that are going to be tuned\n",
    "    max_depth = trial.suggest_int('max_depth',2,16)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split',2,16)\n",
    "    \n",
    "    # Estimator with suggested hyperparameters\n",
    "    params = {'max_depth': max_depth, 'min_samples_split':min_samples_split}\n",
    "    regr_tree = DecisionTreeRegressor(random_state = 100515585, **params)\n",
    "    \n",
    "    # Define the neg means squared error as the score and the inner evaluation as corssvalidation \n",
    "    inner_score = cross_val_score(regr_tree,X_train_2,y_train_2,cv=k_folds,scoring='neg_mean_absolute_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "# Train the model and perform HPO\n",
    "# Measure the time\n",
    "t_0 = time.time()\n",
    "sampler = optuna.samplers.TPESampler(seed=rs)\n",
    "study = optuna.create_study(direction='maximize',sampler=sampler)\n",
    "iterations = 30\n",
    "study.optimize(objective, n_trials = iterations)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "regr_opt = DecisionTreeRegressor(random_state = rs, **best_params)\n",
    "regr_opt.fit(X_train_2,y_train_2)\n",
    "\n",
    "# ESTIMATE FUTURE PERFORMANCE\n",
    "y_test_pred = regr_opt.predict(X_test_2)\n",
    "mse_future = mean_squared_error(y_test_pred, y_test_2)\n",
    "\n",
    "# Print results\n",
    "# Print the results for the best combination\n",
    "print(\"MSE future performance: \", mse_future)\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Execution time: \",t_1-t_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Successive Halving\n",
    "\n",
    "!pip install optuna scikit-optimize\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 16)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n",
    "    params = {'max_depth': max_depth, 'min_samples_split': min_samples_split}\n",
    "    regr_tree = DecisionTreeRegressor(random_state=100515585, **params)\n",
    "    inner_score = cross_val_score(regr_tree, X_train_2, y_train_2, cv=k_folds, scoring='neg_mean_absolute_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t_0 = time.time()\n",
    "\n",
    "    # Use TPESampler for Bayesian optimization\n",
    "    sampler = optuna.samplers.TPESampler(seed=100515585)\n",
    "    study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "    # Run optimization\n",
    "    n_trials = 30\n",
    "    for _ in range(n_trials):\n",
    "        trial = study.ask()\n",
    "        value = objective(trial)\n",
    "        study.tell(trial, value)\n",
    "\n",
    "    t_1 = time.time()\n",
    "\n",
    "    best_params = study.best_params\n",
    "    regr_opt = DecisionTreeRegressor(random_state=100515585, **best_params)\n",
    "    regr_opt.fit(X_train_2, y_train_2)\n",
    "\n",
    "    y_test_pred = regr_opt.predict(X_test_2)\n",
    "    mse_future = mean_squared_error(y_test_pred, y_test_2)\n",
    "\n",
    "    print(\"MSE future performance: \", mse_future)\n",
    "    print(\"Best Hyperparameters: \", best_params)\n",
    "    print(\"Execution time: \", t_1 - t_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest Regressor\n",
    "\n",
    "### 5.1 First model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "t_0 = time.time()\n",
    "# Create the ensemble model pipeline\n",
    "random_forest = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42)) \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "first_forest = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42)) \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "first_forest.fit(X_train_1, y_train_1)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predicted_1_rf = first_forest.predict(X_test_1)\n",
    "\n",
    "# Evaluate the model\n",
    "MSE_1 = mean_squared_error(y_test_1, y_predicted_1_rf)\n",
    "print(\"Mean Squared Error:\", MSE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "second_forest = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42)) \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "second_forest.fit(X_train_2,y_train_2)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predicted_2_rf = second_forest.predict(X_test_2)\n",
    "\n",
    "# Evaluate the model\n",
    "MSE_2 = mean_squared_error(y_test_2, y_predicted_2_rf)\n",
    "print(\"Mean Squared Error:\", MSE_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "third_forest = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42)) \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "third_forest.fit(X_train_3,y_train_3)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predicted_3_rf = third_forest.predict(X_test_3)\n",
    "\n",
    "# Evaluate the model\n",
    "MSE_3 = mean_squared_error(y_test_3, y_predicted_3_rf)\n",
    "print(\"Mean Squared Error:\", MSE_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error no mejora aplicando feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bagging Regressor with KNN models\n",
    "### 6.1 First model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# Create the ensemble model pipeline\n",
    "knn_ensemble_pipeline = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', BaggingRegressor(base_estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=10, random_state=42))  \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_ensemble_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = knn_ensemble_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "\n",
    "first_ensemble = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', BaggingRegressor(base_estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=10, random_state=42))  \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "first_ensemble.fit(X_train_1, y_train_1)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predicted_1_e = first_ensemble.predict(X_test_1)\n",
    "\n",
    "# Evaluate the model\n",
    "MSE_1 = mean_squared_error(y_test_1, y_predicted_1_e)\n",
    "print(\"Mean Squared Error:\", MSE_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "second_ensemble = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', BaggingRegressor(base_estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=10, random_state=42))  \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "second_ensemble.fit(X_train_2, y_train_2)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predicted_2_e = second_ensemble.predict(X_test_2)\n",
    "\n",
    "# Evaluate the model\n",
    "MSE_2 = mean_squared_error(y_test_2, y_predicted_2_e)\n",
    "print(\"Mean Squared Error:\", MSE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "third_ensemble = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', BaggingRegressor(base_estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=10, random_state=42))  \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "third_ensemble.fit(X_train_3, y_train_3)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predicted_3_e = third_ensemble.predict(X_test_3)\n",
    "\n",
    "# Evaluate the model\n",
    "MSE_3 = mean_squared_error(y_test_3, y_predicted_3_e)\n",
    "print(\"Mean Squared Error:\", MSE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous results, it is demonstrated that the error from the model improves applying feature selection. The best results are obtained selecting only the features related to the wind. The second best result is obtained selecting only the features from Sotavento and related to the wind.\n",
    "\n",
    "Based on this results, for the KNN model, only the features related to the wind will be selected (second option). So, from now on, it will be used X_2 and y_2."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
