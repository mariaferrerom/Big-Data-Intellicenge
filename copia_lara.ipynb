{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 1\n",
    "\n",
    "## Lara Monteserín Placer\n",
    "\n",
    "## María Ferrero Medina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this project is to design a machine learning model that is able to predict the energy produced by the Sotavento wind farm. For this purpose, a dataset with 555 features ans 4748 instances is available. These features are meteorological variables measured in different locations. While developing the model, different strategies for optimization will be used, so a complementary objective of this assignment is to compare these strategies. \n",
    "\n",
    "The structure of the assignment will be the following:\n",
    "\n",
    "1. **Exploratory Data Analysis.** First, an Explanatory Data Analysis will be made to gain knowledge about the dataset and understant better how to achieve the purpose of the assignment.\n",
    "\n",
    "2. **Methodology.** Secondly, an explanation on the methodology followed during the assignment (loss functions used, feature perrformance measure strategy and some strategical details).\n",
    "\n",
    "\n",
    "3. **KNN model.** Sections 3, 4, 5 and 6 are dedicated to the development of models. Four different models will be created and on each section, each model will be trained and optimized to reduce the error.\n",
    "\n",
    "4. **Decission tree model.**\n",
    "\n",
    "5. **Random Forest.** \n",
    "\n",
    "6. **Gradient Boosting with KNN.**\n",
    "\n",
    "7. **Final model.** Finally, the final model will be chosen from the four models according to the future performance\n",
    "\n",
    "8. **Conclusions.** Some ideas aboutvthe final model will be here explained and also the conclusions about the different hyperparameter tuning strategies.\n",
    "\n",
    "Each of the models will be optimized following the same strategy, that consists in four stages plus a final stage to estimate the future performance:\n",
    "1. **First model.** A first approach of the model will be created to compare with the improved model as different hyperparameters are optimized. Here, the default hyperparameters are considered and for preprocessing the KNN imputer is used and no feature selection is done.\n",
    "\n",
    "2. **Feature selection.** Three different feature selection strategies are tried and the error for each of them is measured in order to select the most suitable strategy for each model. Also, if the feature selection does not improve the performance of the model, then no feature selection is done.\n",
    "\n",
    "3. **Imputation techniques.** Three different imputation techniques are tried and the error for each of them is measured in order to determine the most adequate for each model.\n",
    "\n",
    "4. **Hyperparameter tuning.** Three different strategies for HPO are performed to choose the optimal values for hyperparameters. These three strategies are just different ways to achieve the HPO, but they are considered to compare results between them and execution times.\n",
    "\n",
    "5. **Conclusions and future performance.** The final model is built considering the results from previous stages and the future performance of the model is estimated.\n",
    "\n",
    "XXXXXXXXXXXXXXX\n",
    "\n",
    "ADD XGBOOST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-hPXBr4r5rS"
   },
   "source": [
    "\n",
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "Before starting to build the model, an EDA is made as a first approach to gain understanding of the dataset. In this Exploratory Data Analysis the data type of the features will be verified, the number of instances and features will be determined. Also, a brief summary of the missing values and columns with constant value will be included. \n",
    "\n",
    "### 1.1. Number of instances and features\n",
    "\n",
    "This dataset has 4748 instances and 555 features.\n",
    "\n",
    "### 1.2. Nature of the variables\n",
    "\n",
    "This dataset contains information about the meteorological conditions in several locations, the time the measures of these conditions were made and the energy produced at each moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy     float64\n",
      "year         int64\n",
      "month        int64\n",
      "day          int64\n",
      "hour         int64\n",
      "            ...   \n",
      "v100.21    float64\n",
      "v100.22    float64\n",
      "v100.23    float64\n",
      "v100.24    float64\n",
      "v100.25    float64\n",
      "Length: 555, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Read the data that is compressed as a gzip\n",
    "wind_ava = pd.read_csv('wind_available.csv.gzip', compression=\"gzip\")\n",
    "\n",
    "# Display the first rows of the dataset just to see it\n",
    "wind_ava.head()\n",
    "\n",
    "# Display the data type of each column\n",
    "column_data_types = wind_ava.dtypes\n",
    "print(column_data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having checked the data types of all the different features, it has been verified that there are:\n",
    "\n",
    "- 551 **numerical variables** (real numbers). From this 551:\n",
    " - One feature is the **energy**, that is the target of the problem.\n",
    " - The remaining 550 are relative to the 22 different **meteorological conditions** measured at the 25 different locations.\n",
    "\n",
    "- 4 **numerical variables** (integers). These 4 variables are the **year, day, month and hour of the day**. These variables characterize the moment the measures were taken.\n",
    "\n",
    "### 1.3. Check for missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Column  Null Values  NaN Values\n",
      "0     energy            0           0\n",
      "1       year            0           0\n",
      "2      month            0           0\n",
      "3        day            0           0\n",
      "4       hour            0           0\n",
      "..       ...          ...         ...\n",
      "550  v100.21          261         261\n",
      "551  v100.22          387         387\n",
      "552  v100.23          569         569\n",
      "553  v100.24          579         579\n",
      "554  v100.25          436         436\n",
      "\n",
      "[555 rows x 3 columns]\n",
      "Number of columns with Null Values: 550\n",
      "Number of columns with NaN Values: 550\n"
     ]
    }
   ],
   "source": [
    "# Return the number of Null values for each column\n",
    "null_values = wind_ava.isnull().sum()\n",
    "# Return the number of NaN values for each column (just in case they are not the same)\n",
    "nan_values = wind_ava.isna().sum()\n",
    "\n",
    "# Store in missing values the amount of Null and NaN values of each column\n",
    "missing_values = pd.DataFrame({\n",
    "    'Column': null_values.index,\n",
    "    'Null Values': null_values.values,\n",
    "    'NaN Values': nan_values.values\n",
    "})\n",
    "\n",
    "# Print the amount of Null and Nan values\n",
    "print(missing_values)\n",
    "\n",
    "# Identify columns with Null or NaN values\n",
    "columns_with_null = wind_ava.columns[wind_ava.isnull().any()]\n",
    "columns_with_nan = wind_ava.columns[wind_ava.isna().any()]\n",
    "\n",
    "# Display the number of columns which have missing values\n",
    "print(\"Number of columns with Null Values:\", len(columns_with_null))\n",
    "print(\"Number of columns with NaN Values:\", len(columns_with_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All meteorological variables have missing values in different instances. The 4 categories that characterize the moment the measure was made and the target feature'energy' do not have missing values.\n",
    "\n",
    "\n",
    "### 1.4. Check for constant columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with constant values: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check for constant values in each column\n",
    "constant_columns = wind_ava.columns[wind_ava.nunique() == 1]\n",
    "\n",
    "# Print columns with constant values\n",
    "print(\"Columns with constant values:\", constant_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are no constant columns in the dataset. \n",
    "\n",
    "\n",
    "### 1.5. Type of problem\n",
    "\n",
    "The objective of the model is to estimate the energy, as it is a continuous numerical value, this is a **regression problem**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "\n",
    "\n",
    "This section will explain the methodology that is going to be followed to evaluate the models. The evaluation techniques that will be used for outer evaluation and inner evaluation. And also the metrics that will determine the future performance of the model.\n",
    "\n",
    "### 2.1. Model building\n",
    "\n",
    "The structure of the assignment will be to first optimize four different models following different strategies. And then compare the optimized models to choose the one that minimizes the error.\n",
    "\n",
    "The creation and optimization of each model has been structured as follows:\n",
    "\n",
    "1. **First model.** A first approach of the model will be created to compare with the improved model as different hyperparameters are optimized. Here, the default hyperparameters are considered and for preprocessing, the KNN imputer is used to impute the missing values and no feature selection is done.\n",
    "\n",
    "2. **Feature selection.** Three different feature selection strategies are tried and the error for each of them is measured in order to select the most suitable strategy for each model. Also, if the feature selection does not improve the performance of the model, then no feature selection is done.\n",
    "\n",
    "3. **Imputation techniques.** Three different imputation techniques are tried and the error for each of them is measured in order to determine the most adequate for each model.\n",
    "\n",
    "4. **Hyperparameter tuning.** Three different strategies for HPO are performed to choose the optimal values for hyperparameters. These three strategies are just different ways to achieve the HPO, but they are considered to compare results between them and execution times.\n",
    "\n",
    "5. **Conclusions and future performance.** The final model is built considering the results from previous stages and the future performance of the model is estimated.\n",
    "\n",
    "Once the four models have been created and optimized, the future performance of each model will be computed. The best model in terms of the loss function will be chosen and the Final Regressor will be that model but it will be trained using the whole dataset. This final model will be then used with the new data that is provided in a separate dataset. \n",
    "\n",
    "### 2.2. Evaluation techniques\n",
    " \n",
    "For inner and outer evaluation, different strategies have been followed. Also, for inner evaluation, for the optimization of the preprocessing stages and the optimization of the hyperparameters of the model, different strategies have been followed:\n",
    "\n",
    "- On the one hand, for the **outer evaluation**, **holdout** evaluation will be used. This method will be used to estimate the future performance for each of the optimized models. The complete datastet will be split into training set and test set. The train set will be used for all the stages of model building (preprocessing and model optimization). The test set is reserved for an unbiased estimation of the future performance of each optimized model. This approach provides a realistic procedure of model generalization to unseen data.\n",
    "\n",
    "- On the other hand, for the **inner evaluation**, two strategies have been followed, using only the training set from the previously divided complete dataset:\n",
    " - First, for the optimization of the preprocessing stages, **holdout validation** is used. The training set is divided into inner training set and validation set. Different preprocessing strategies (feature selection and imputation techniques) are compared training the data with the training set and estimating the performance of each possibility using the validation set. This approach, chosen for its simplicity, allows for a straightforward comparison of different preprocessing methods.\n",
    " - Second, **crossvalidation** will be applied for Hyperparameter Tuning. his technique is employed to identify the optimal combination of hyperparameters by iteratively training and validating the model on different subsets of the training data. Crossvalidation ensures a more robust result as it captures the performance of the model using different subsets (folds) from the training set. This approach enhances the reliability of hyperparameter selection, contributing to improved model generalization. \n",
    "\n",
    "### 2.3. Error metric\n",
    "\n",
    "The Mean Squared Error (MSE) has been selected as the objective function for model validation and method comparison. This choice is motivated by its sensitivity to both small and large errors. MSE squares the magnitudes of errors, providing a more significant penalty for larger errors. This sensitivity allows to specially focus on minimizing the larger errors.\n",
    "\n",
    "Additionally, in certain sections, the Root Mean Squared Error (RMSE) has been employed instead of MSE. RMSE offers the same information but in a more interpretable manner by taking the square root of MSE. This adjustment ensures that the error measure has the same order of magnitude as the target feature, which, in this case, is the energy. The interpretability of RMSE facilitates a clearer understanding of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we define a generic random seed that will be used along the assignment\n",
    "rs = 100515585 \n",
    "\n",
    "# First, data will be divided into train and test set \n",
    "# Considering it is a time series, it must be split in an appropiate way\n",
    "wind_ava['timestamp'] = pd.to_datetime(wind_ava[['year', 'month', 'day', 'hour']])\n",
    "wind_ava = wind_ava.sort_values(by='timestamp')\n",
    "\n",
    "train_size = 0.8 \n",
    "split_index = int(len(wind_ava) * train_size)\n",
    "wind_ava = wind_ava.drop(columns=['timestamp'])\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "train_data = wind_ava.iloc[:split_index]\n",
    "test_data = wind_ava.iloc[split_index:]\n",
    "\n",
    "X_train = train_data.drop('energy', axis=1)\n",
    "y_train = train_data['energy']\n",
    "X_test = test_data.drop('energy', axis=1)\n",
    "y_test = test_data['energy']\n",
    "\n",
    "# For inner validation for feature selection and imputation\n",
    "# Divide the train set into inner train set (for training) and inner validation set (for validation)\n",
    "\n",
    "train_size_inner = 0.8\n",
    "split_index_inner = int(len(train_data) * train_size_inner)\n",
    "\n",
    "# Divide the data Inner Train Set and Inner Validation Set\n",
    "train_inner = train_data.iloc[:split_index_inner]\n",
    "val_inner = train_data.iloc[split_index_inner:]\n",
    "\n",
    "X_inner_train = train_inner.drop('energy', axis=1)\n",
    "y_inner_train = train_inner['energy']\n",
    "X_inner_val = val_inner.drop('energy', axis=1)\n",
    "y_inner_val = val_inner['energy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KNN Regressor\n",
    "\n",
    "The first algorithm to try is the KNN algorithm applied for Regression. In this algorithm, the output is computed as the mean of the closest neighbors, this is the training instances that are more similar to the new instance.\n",
    "\n",
    "In this section, different imputation techniques and feature selection strategies will  be used to determine which is the most adequate. Later, hyperparameter tuning will determine the optimal number of neighbors to be used. Finally, a final version of the model will be built taking all these studies into consideration. \n",
    "\n",
    "### 3.1. First model\n",
    "\n",
    "A first model using the KNN algorithm will be implemented. It will use the default hyperparameters, the KNN Imputer as imputation strategy and no feature selection. This model will be updated and improved later but a first approach is required to have a model to compare in further stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the first KNN model:  650.972879219549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Now the first KNN model will be created\n",
    "first_knn = Pipeline([('imputer',KNNImputer()),('regression',KNeighborsRegressor())])\n",
    "first_knn.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_first = first_knn.predict(X_inner_val)\n",
    "MSE = mean_squared_error(y_inner_val,y_predicted_first)\n",
    "\n",
    "print('RMSE for the first KNN model: ', np.sqrt(MSE))\n",
    "# RMSE: 650.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature selection\n",
    "\n",
    "Now, feature selection will be used in order to remove variables that are not affecting the target and thus avoid overfitting that may be lead by wrong interpretation from these variables. This has been done manually, selecting different feature combinations that seem to be accurate for the problem. Three different cases have been considered:\n",
    "- **First option**. Selecting only the features **related to the location** of the wind farm (Sotavento). This is the features that contain the sufix 13.\n",
    "- **Second option**. Selecting only the features **related to the wind** characteristics. This is the features that start with u or v (the vertical and horizontal components of the wind).\n",
    "- **Third option**. Selecting only the features which are both **located in Sotavento** (location 13) and **related to the wind** characteristics. This is the features that start with u or v and end with sufix 13.\n",
    "\n",
    "This strategy will be followed for the feature selection of the rest of the models. This is sections 4.2, 5.2 and 6.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  658.2887149048381\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "\n",
    "# Select the desired variables\n",
    "X_inner_train_1 = train_inner.filter(regex='\\.13$', axis=1)\n",
    "X_inner_val_1 = val_inner.filter(regex='\\.13$', axis=1)\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_knn.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_1 = first_knn.predict(X_inner_val_1)\n",
    "MSE_1 = mean_squared_error(y_inner_val,y_predicted_1)\n",
    "\n",
    "print('RMSE for feature selection with variables from Sotavento: ', np.sqrt(MSE_1))\n",
    "# 658.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with variables related to the wind:  428.12271404149146\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "# Select the desired variables\n",
    "X_inner_train_2 = train_inner.filter(regex='^(u|v).*$', axis=1)\n",
    "X_inner_val_2 = val_inner.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_knn.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_2 = first_knn.predict(X_inner_val_2)\n",
    "MSE_2 = mean_squared_error(y_inner_val,y_predicted_2)\n",
    "\n",
    "print('RMSE for imputation with variables related to the wind: ', np.sqrt(MSE_2))\n",
    "# 428.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with variables from Sotavento related to the wind:  433.4095713141627\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "# Select the desired variables\n",
    "X_inner_train_3 = train_inner.filter(regex='^(u|v).*\\.13$', axis=1)\n",
    "X_inner_val_3 = val_inner.filter(regex='^(u|v).*\\.13$', axis=1)\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_knn.fit(X_inner_train_3,y_inner_train)\n",
    "y_predicted_3 = first_knn.predict(X_inner_val_3)\n",
    "MSE_3 = mean_squared_error(y_inner_val,y_predicted_3)\n",
    "\n",
    "print('RMSE for imputation with variables from Sotavento related to the wind: ', np.sqrt(MSE_3))\n",
    "# 433.40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different feature selection strategies are included in the following table. These results show that the best feature selection strategy for the model is the second, selecting only the features related to the wind. Feature selection improves the performance of the model as it discards features that are not affecting the target and may lead to overfitting. \n",
    "\n",
    "|          | 0. No feature selection | 1. Sotavento features | 2. Wind features | 3. Sotavento wind features |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|:----------:|\n",
    "| RMSE | 658.28 | 663.35 | 428.12 | 433.40 |\n",
    "\n",
    "\n",
    "From this moment, the KNN model will be built using only the features related to the wind, that is the variables categorized as X_train_inner_2, X_val_inner_2.\n",
    "\n",
    "### 3.3. Imputation techniques\n",
    "\n",
    "The dataset includes many missing values, it is important to apply an adequate imputation technique. In this section, three different imputation techniques will be considered and applied. The results obtained from this study will determine which is the imputation technique that will be followed later. Three different strategies have been studied:\n",
    "- **First option**. **Simple Imputer**. It is an univariate imputation technique, this means that only uses values relative to the feature that is going to be imputed. It imputes the missing values with the mean of the feature. The mean is been chosen instead of the median because there are not many outliers that could affect the distribution of the data.\n",
    "- **Second option**. **KNN Imputer**. It is a multivariate imputation technique, this means that it uses values from the feature that is going to be imputed but also from other features. This technique in partiular is based in the KNN algorithm and it consistsn on imputing the value os the missing category as a mean from the closest neighbors.\n",
    "- **Third option**. **Iterative Imputer**. It is also a multivarite technique. Is based on iterative models that compute the values for the missing categories.\n",
    "This strategy will be followed for the imputation strategy selection of the rest of the models. This is sections 4.3, 5.3 and 6.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Simple Imputer:  457.67268509394927\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION: Simple Imputer using the mean\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "simple_knn = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',KNeighborsRegressor())])\n",
    "\n",
    "simple_knn.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_simple_imputer = simple_knn.predict(X_inner_val_2)\n",
    "MSE_simple = mean_squared_error(y_inner_val,y_predicted_simple_imputer)\n",
    "\n",
    "print('RMSE for imputation with Simple Imputer: ', np.sqrt(MSE_simple))\n",
    "# 457.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with KNN Imputer:  428.12271404149146\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION: KNN Imputer with default hyperparameters\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "knn_knn = Pipeline([('imputer',KNNImputer()),('regression',KNeighborsRegressor())])\n",
    "\n",
    "knn_knn.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_knn_imputer = knn_knn.predict(X_inner_val_2)\n",
    "MSE_knn = mean_squared_error(y_inner_val,y_predicted_knn_imputer)\n",
    "\n",
    "print('RMSE for imputation with KNN Imputer: ', np.sqrt(MSE_knn))\n",
    "# 428.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Update the version of scikit-learn if it returns an Error with IterativeImputer\n",
    "!pip install --upgrade scikit-learn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Iterative Imputer:  426.13631965182225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# THIRD OPTION: multivariate technique. Iterative imputer with 10 maximum iterations. Include random seed for reproducibility\n",
    "iterative_knn = Pipeline([('imputer',IterativeImputer(max_iter = 10, random_state = rs)),('regression',KNeighborsRegressor())])\n",
    "\n",
    "iterative_knn.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_iterative_imputer = iterative_knn.predict(X_inner_val_2)\n",
    "MSE_iterative = mean_squared_error(y_inner_val,y_predicted_iterative_imputer)\n",
    "\n",
    "print('RMSE for imputation with Iterative Imputer: ', np.sqrt(MSE_iterative))\n",
    "# 426.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          | 1. Simple Imputer | 2. KNN Imputer | 3. Iterative Imputer|\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE | 457.67 | 428.12 | 426.14 |\n",
    "\n",
    "From the results above, it is demonstrated that for this case, the Simple Imputer is the worst in terms of performance with respect to the Mean Squared Error. On the other hand, the multivariate techniques, KNN Imputer and the Iterative Imputer give similar results. The Iterative Imputer gets a lower Mean Squared Error, so it will be the chosen technique. \n",
    "\n",
    "### 3.4. Hyperparameter tuning\n",
    "\n",
    "Once the preprocessing stages have been optimized and improved, it is time to optimize the model by performing hyperparameter tuning. Crossvalidation will be the evaluation strategy for HPO.\n",
    "In the case of KNN algorithm, the main hyperparameter to tune is:\n",
    "- **n_neighbors**: the number of neighbors, that is the number of close instances to be considered to calculate the value of the outputs. \n",
    "\n",
    "Three approaches for hyperparameter tuning will be implemented:\n",
    "\n",
    "1. **First strategy**. **Random Search**. Randomly samples hyperparameter combinations from a predefined search space. It is characterized by its simplicity, but it is not the most efficient because it does not consider the data from previous iterations to guide the following iterations.\n",
    "2. **Second strategy**. **OPTUNA**. OPTUNA is a sequential optimization framework based on Bayesian optimization. It is more complex than random search, but it guides the search to more likely regions. In this sense, the exploration is more efficient. \n",
    "3. **Third strategy**. **Successive Halving**. This strategy aims to efficiently allocate resources (evaluations) to promising hyperparameter configurations. It may require more resources than the previously mentioned methods. Anyway it is also resource effective as it allocates the resources in the most promising regions and does not waste resources in unpromising regions.\n",
    "\n",
    "The optimization time will be measured and also the best combination of hyperparameters for each method will be used to estimate the error using crossvalidation. It is important to note that this crossvalidation measure of the error is optimistically biased, because it has been computed using the same data that has been used for HPO. So it is useful to compare the hyperparameter combinations but not for estimating future performance.\n",
    "\n",
    "For HPO, Pipelines will not be used, the preprocessing stages will be made previously in order to focus on the regression stage for optimization. This same procedure will be also followed for the other models for hyperparameter tuning (sections 4.4., 5.4. and 6.4.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# FILTER. Feature selection\n",
    "X_train_filtered = X_train.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# IMPUTER. Imputation of missing values\n",
    "imputer = IterativeImputer(max_iter=10, random_state=rs)\n",
    "X_train_imputed = imputer.fit_transform(X_train_filtered)\n",
    "# X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_filtered), columns=X_train_filtered.columns)\n",
    "\n",
    "\n",
    "# Preprocessing also for the test data for estimation of future performance later\n",
    "X_test_filtered = X_test.filter(regex='\\.13$', axis=1)\n",
    "X_test_imputed = imputer.fit_transform(X_test_filtered)\n",
    "# X_test_imputed_df = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL TO OPTIMIZE\n",
    "\n",
    "# Create the regressor with no hyperparamters defined\n",
    "knn_default = KNeighborsRegressor()\n",
    "\n",
    "# Create a dictionary with the values for the hyperparameters\n",
    "params_knn = {'n_neighbors': np.arange(1, 51)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params {'n_neighbors': 17}\n",
      "RANDOM SEARCH\n",
      "Best Hyperparameters for KNN: {'n_neighbors': 17}\n",
      "Execution time:  1.3825676441192627\n",
      "CV MSE Score:  391.7182984002424\n"
     ]
    }
   ],
   "source": [
    "# FIRST STRATEGY - Randomized Search\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import time\n",
    "\n",
    "# Create a randomized search instance\n",
    "random_search = RandomizedSearchCV(\n",
    "    knn_default,\n",
    "    param_distributions=params_knn,\n",
    "    n_iter=10,  # Number of iterations (can be adjusted)\n",
    "    cv=5,       # Number of cross-validation folds\n",
    "    scoring='neg_mean_squared_error',  # Mean squared error is the error metric\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "random_search.fit(X_train_imputed, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_random_knn = random_search.best_params_\n",
    "best_knn_random = KNeighborsRegressor(**best_params_random_knn) \n",
    "cv_scores = cross_val_score(best_knn_random, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_cv_rmse = np.mean(np.sqrt(-cv_scores))\n",
    "\n",
    "# Print results\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(\"Best Hyperparameters for KNN:\", best_params_random_knn)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV MSE Score: \", mean_cv_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-02 22:21:08,395] A new study created in memory with name: no-name-8a0d8bd7-2d83-4334-b775-df635c20d044\n",
      "[I 2024-01-02 22:21:39,604] Trial 0 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 21}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 22:22:11,016] Trial 1 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 33}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 22:22:42,099] Trial 2 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 50}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 22:23:13,468] Trial 3 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 40}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 22:23:44,919] Trial 4 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 37}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 22:24:16,427] Trial 5 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 15}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 22:24:47,734] Trial 6 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 5}. Best is trial 0 with value: -170261.21944310627.\n",
      "[W 2024-01-02 22:24:51,042] Trial 7 failed with parameters: {'n_neighbors': 29} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_23276\\3447748276.py\", line 18, in objective_knn\n",
      "    inner_score = cross_val_score(iterative_knn, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 562, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 214, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 309, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 423, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 377, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 957, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py\", line 765, in fit_transform\n",
      "    Xt, estimator = self._impute_one_feature(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py\", line 412, in _impute_one_feature\n",
      "    estimator.fit(X_train, y_train)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_bayes.py\", line 337, in fit\n",
      "    U, S, Vh = linalg.svd(X, full_matrices=False)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_svd.py\", line 127, in svd\n",
      "    u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n",
      "KeyboardInterrupt\n",
      "[W 2024-01-02 22:24:51,057] Trial 7 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\3447748276.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mt_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mstudy_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective_knn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mt_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         \"\"\"\n\u001b[1;32m--> 451\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfrozen_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     ):\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\3447748276.py\u001b[0m in \u001b[0;36mobjective_knn\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Define the neg means squared error as the score and the inner evaluation as corssvalidation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0minner_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterative_knn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_imputed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m     cv_results = cross_validate(\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m                     )\n\u001b[0;32m    213\u001b[0m                 ):\n\u001b[1;32m--> 214\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;31m# independent, and that it is pickle-able.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     results = parallel(\n\u001b[0m\u001b[0;32m    310\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    311\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1792\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m    422\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    375\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    378\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    958\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    763\u001b[0m                     \u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabs_corr_mat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                 )\n\u001b[1;32m--> 765\u001b[1;33m                 Xt, estimator = self._impute_one_feature(\n\u001b[0m\u001b[0;32m    766\u001b[0m                     \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                     \u001b[0mmask_missing_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py\u001b[0m in \u001b[0;36m_impute_one_feature\u001b[1;34m(self, X_filled, mask_missing_values, feat_idx, neighbor_feat_idx, estimator, fit_mode)\u001b[0m\n\u001b[0;32m    410\u001b[0m                 \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m             )\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[1;31m# if no missing values, don't predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0mXT_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0meigen_vals_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;31m# perform decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0m\u001b[0;32m    128\u001b[0m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# THIS STAGE TAKES LONG\n",
    "# SECOND STRATEGY - Optuna\n",
    "\n",
    "# Import required libraries\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective_knn(trial):\n",
    "    # Hyperparameters that are to be tuned\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 51)\n",
    "    params = {'n_neighbors': n_neighbors}\n",
    "    \n",
    "    # Estimator with suggested hyperparameters\n",
    "    knn_default = KNeighborsRegressor()\n",
    "    \n",
    "    # Define the neg means squared error as the score and the inner evaluation as corssvalidation \n",
    "    inner_score = cross_val_score(iterative_knn, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "# Train the model and perform HPO\n",
    "# Measure the time\n",
    "sampler = optuna.samplers.TPESampler(seed=rs)\n",
    "study_knn = optuna.create_study(direction='maximize',sampler=sampler)\n",
    "\n",
    "iterations = 30\n",
    "t_0 = time.time()\n",
    "study_knn.optimize(objective_knn, n_trials = iterations)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params_optuna = study_knn.best_params\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_knn_optuna = KNeighborsRegressor(**best_params_optuna) \n",
    "cv_scores_optuna = cross_val_score(best_knn_optuna, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_cv_rmse_optuna = np.mean(np.sqrt(-cv_scores_optuna))\n",
    "\n",
    "# Print results\n",
    "print(\"OPTUNA HPO\")\n",
    "print(\"Best Hyperparameters for KNN:\", best_params_optuna)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV RMSE Score: \", mean_cv_rmse_optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 6\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 118\n",
      "max_resources_: 3798\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 50\n",
      "n_resources: 118\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 25\n",
      "n_resources: 236\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 13\n",
      "n_resources: 472\n",
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 7\n",
      "n_resources: 944\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 4\n",
      "n_resources: 1888\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 2\n",
      "n_resources: 3776\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "SUCCESIVE HALVING\n",
      "Best Hyperparameters for KNN: {'n_neighbors': 11}\n",
      "Execution time:  1.9224305152893066\n",
      "CV MSE Score:  393.42255429468116\n"
     ]
    }
   ],
   "source": [
    "# THIRD STRATEGY - Successive halving\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "\n",
    "# Create\n",
    "halving = HalvingGridSearchCV(knn_default,\n",
    "                              params_knn,\n",
    "                              scoring= 'neg_mean_squared_error',\n",
    "                              cv=5,\n",
    "                              random_state=rs,\n",
    "                              factor=2,\n",
    "                              min_resources='exhaust',\n",
    "                              max_resources='auto',\n",
    "                              n_jobs=-1, verbose=1)\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "halving.fit(X_train_imputed, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_halving = halving.best_params_\n",
    "best_knn_halving = KNeighborsRegressor(**best_params_halving) \n",
    "cv_scores_halving = cross_val_score(best_knn_halving, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_cv_rmse_halving = np.mean(np.sqrt(-cv_scores_halving))\n",
    "\n",
    "# Print results\n",
    "print(\"SUCCESIVE HALVING\")\n",
    "print(\"Best Hyperparameters for KNN:\", best_params_halving)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV MSE Score: \", mean_cv_rmse_halving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          | 1. Random Search | 2. OPTUNA | 3. Successive Halving |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE (crossvalidation) | 391.71 | 391.52 | 393.42 |\n",
    "| n_neighbors | 17 | 21 | 11 |\n",
    "| Time (s) | 1.38 | 532.77 | 1.92 |\n",
    "\n",
    "The results from the HPO show that the number of neighbors does not seem to affect the performance when it changes from 17 to 21, as the crossvalidation scores are the same. But, it also shows that the successive halving in this case performs slightly worse than the other strategies.\n",
    "\n",
    "An important note to take into account is that the errors that are measured for this section do not provide real insight on the errors that will occur in future performance. This happens because these errors have been computed with the same set that was used during the training stage. Anyway, as these errors have been measured in the same way for the three strategies, it can be a measure used to determine which strategy is better. Then, the test set will be used to estimate future performance in an unbiased way.\n",
    "\n",
    "The final decission for the number of neighbors is 17, as it is one of the options that provide the best result and requires of considering a lower number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Conclusions and future performance\n",
    "\n",
    "Out of all the trials, the KNN model has demonstrated to work better with the following adjustments:\n",
    "- **Best imputation technique**: Iterative Imputer\n",
    "- **Best feature selection strategy**: selecting the variables related to wind characteristics.\n",
    "- **Best hyperparameters**:\n",
    " - Best number of neighbors: 17\n",
    " \n",
    "Now, to estimate the future performance of this model, the test set that has been previously set aside will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN model\n",
      "RMSE:  407.2477310098077\n",
      "Training time (seconds):  22.808151721954346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import time\n",
    "\n",
    "# Define optimal hyperparameters\n",
    "opt_k = 17\n",
    "\n",
    "# Custom function for filtering\n",
    "def filter_knn(data):\n",
    "    return data.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# Define the pipeline\n",
    "best_knn = Pipeline([\n",
    "    ('filter', FunctionTransformer(filter_knn, validate=False)),\n",
    "    ('imputer',IterativeImputer(max_iter = 10, random_state = rs)),\n",
    "    ('regression',KNeighborsRegressor(n_neighbors = opt_k))])\n",
    "\n",
    "# Fit training data\n",
    "t_0=time.time()\n",
    "best_knn.fit(X_train,y_train)\n",
    "t_1=time.time()\n",
    "\n",
    "# Predict test data\n",
    "y_pred_best = best_knn.predict(X_test)\n",
    "MSE_best_knn = mean_squared_error(y_test,y_pred_best)\n",
    "\n",
    "print('KNN model')\n",
    "print('RMSE: ', np.sqrt(MSE_best_knn))\n",
    "print('Training time (seconds): ', t_1-t_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Tree\n",
    "\n",
    "### 4.1. First model\n",
    "\n",
    "A first model using the Decision Tree algorithm will be implemented. It will use the default hyperparameters, the KNN Imputer as imputation strategy and no feature selection. This model will be updated and improved but it is just a simple approach to later compare the adjusted models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the first KNN model:  650.972879219549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create the first Decission Tree model\n",
    "first_tree = Pipeline([('imputer',KNNImputer()),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "first_tree.fit(X_inner_train,y_inner_train)\n",
    "y_predicted = first_tree.predict(X_inner_val)\n",
    "MSE_tree = mean_squared_error(y_inner_val,y_predicted_first)\n",
    "\n",
    "print('RMSE for the first KNN model: ', np.sqrt(MSE_tree))\n",
    "# 650.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Feature selection\n",
    "\n",
    "Same procedure as in section 3.2. but in this case using Decission Tree Regressor model. The variable selection will not be made again in this section, the filtered datasets are defined in section 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  522.4285433416003\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "\n",
    "# The first Decission Tree model that has been already created is trained now with the selected data\n",
    "first_tree.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_1 = first_tree.predict(X_inner_val_1)\n",
    "MSE_1 = mean_squared_error(y_inner_val,y_predicted_1)\n",
    "\n",
    "print('RMSE for feature selection with variables from Sotavento: ', np.sqrt(MSE_1))\n",
    "# 522.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  541.6336936084882\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "# The first Decission Tree model that has been already created is trained now with the selected data\n",
    "first_tree.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_2 = first_tree.predict(X_inner_val_2)\n",
    "MSE_2 = mean_squared_error(y_inner_val,y_predicted_2)\n",
    "\n",
    "print('RMSE for feature selection with variables related to the wind: ', np.sqrt(MSE_2))\n",
    "# 541.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with variables from Sotavento related to the wind:  543.9981445138152\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_tree.fit(X_inner_train_3,y_inner_train)\n",
    "y_predicted_3 = first_tree.predict(X_inner_val_3)\n",
    "MSE_3 = mean_squared_error(y_inner_val,y_predicted_3)\n",
    "\n",
    "print('RMSE for imputation with variables from Sotavento related to the wind: ', np.sqrt(MSE_3))\n",
    "# 544.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different feature selection strategies are included in the following table. These results show that the feature selection strategy that works better with Decission Trees is the first strategy, this is, choosing the variables related to Sotavneto only.\n",
    "\n",
    "|          | 0. No feature selection | 1. Sotavento features | 2. Wind features | 2. Wind features in Sotavento |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|:----------:|\n",
    "| RMSE |  650.97 | 522.42 | 541.63 | 543.99 |\n",
    "\n",
    "\n",
    "From this moment, the KNN model will be built using only the features that were measured in Sotavento.\n",
    "\n",
    "### 4.3. Imputation techniques\n",
    "\n",
    "Same procedure as in section 3.3. but in this case using Decission Tree Regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Simple Imputer:  546.3131100360276\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION: Simple Imputer using the mean\n",
    "simple_tree = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "\n",
    "simple_tree.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_simple_imputer = simple_tree.predict(X_inner_val_1)\n",
    "MSE_simple = mean_squared_error(y_inner_val,y_predicted_simple_imputer)\n",
    "\n",
    "print('RMSE for imputation with Simple Imputer: ', np.sqrt(MSE_simple))\n",
    "# 546.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with KNN Imputer:  522.4285433416003\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION: KNN Imputer with default hyperparameters\n",
    "knn_tree = Pipeline([('imputer',KNNImputer()),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "\n",
    "knn_tree.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_knn_imputer = knn_tree.predict(X_inner_val_1)\n",
    "MSE_knn = mean_squared_error(y_inner_val,y_predicted_knn_imputer)\n",
    "\n",
    "print('RMSE for imputation with KNN Imputer: ', np.sqrt(MSE_knn))\n",
    "# 522.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Iterative Imputer:  527.2567305971542\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION: Iterative Imputer with 10 maximum iterations. Include random seed for reproducibility\n",
    "iterative_tree = Pipeline([('imputer',IterativeImputer(max_iter = 10, random_state = rs)),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "\n",
    "iterative_tree.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_iterative_imputer = iterative_tree.predict(X_inner_val_1)\n",
    "MSE_iterative = mean_squared_error(y_inner_val,y_predicted_iterative_imputer)\n",
    "\n",
    "print('RMSE for imputation with Iterative Imputer: ', np.sqrt(MSE_iterative))\n",
    "# 527.26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different imputation techniques are included in the following table. \n",
    "\n",
    "|          | 1. Simple Imputer | 2. KNN Imputer | 3. Iterative Imputer | \n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE |  546.31 | 522.43 | 527.26 |\n",
    "\n",
    "Results from the table show that the best imputation technique for Decission Trees in this case is the KNN Imputer with default hyperparameters. From this point, for the Decission Tree model, KNN Imputer will be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Hyperparameter tuning\n",
    "\n",
    "Once the imputation and feature selection have been performed, it is time to improve the performance of the model with HPO. Hyperparameter tuning will be carried out using crossvalidation as the inner evaluation method. For the Decission Trees the hyperparameters that will be adjusted will be:\n",
    "- **max_depth**. Specifies the maximum depth of a decision tree, controlling the level of complexity.\n",
    "- **min_samples_split**. Sets the minimum number of samples required to split an internal node preventing small splits that may be lead by noise.\n",
    "The same three strategies used in section 3.4. will be repeated in this section but with Decission Tree as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# FILTER. Feature selection\n",
    "X_train_filtered_tree = X_train.filter(regex='\\.13$', axis=1)\n",
    "\n",
    "# IMPUTER. Imputation of missing values\n",
    "imputer_tree = KNNImputer()\n",
    "X_train_imputed_tree = imputer_tree.fit_transform(X_train_filtered_tree)\n",
    "\n",
    "# Preprocessing also for the test data for estimation of future performance later\n",
    "X_test_filtered_tree = X_test.filter(regex='\\.13$', axis=1)\n",
    "X_test_imputed_tree = imputer_tree.fit_transform(X_test_filtered_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL TO OPTIMIZE\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create the regressor with no hyperparamters defined\n",
    "tree_default = DecisionTreeRegressor()\n",
    "\n",
    "# Create a dictionary with the values for the hyperparameters\n",
    "# Define the grid with all possible values for the hyperparameters\n",
    "# Define the number of folds and the conditions for the crossvalidation\n",
    "params_tree = {'max_depth': list(range(2,16,2)), 'min_samples_split':list(range(2,16,2))}\n",
    "k_folds = KFold(n_splits = 5, shuffle = True, random_state =rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SEARCH\n",
      "Best Hyperparameters for Decission Tree: {'min_samples_split': 2, 'max_depth': 6}\n",
      "Execution time:  7.918375253677368\n",
      "CV mean MSE Score:  197099.39835690625\n"
     ]
    }
   ],
   "source": [
    "# FIRST STRATEGY - Randomized Search\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import time\n",
    "\n",
    "# Create a randomized search instance\n",
    "random_search_tree = RandomizedSearchCV(\n",
    "    tree_default,\n",
    "    param_distributions=params_tree,\n",
    "    n_iter=10,  # Number of iterations (can be adjusted)\n",
    "    cv=5,       # Number of cross-validation folds\n",
    "    scoring='neg_mean_squared_error',  # Mean squared error is the error metric\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "random_search_tree.fit(X_train_imputed_tree, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_random_tree = random_search_tree.best_params_\n",
    "best_tree_random = DecisionTreeRegressor(**best_params_random_tree) \n",
    "cv_scores_random_tree = cross_val_score(best_tree_random, X_train_imputed_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(\"Best Hyperparameters for Decission Tree:\", best_params_random_tree)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV mean MSE Score: \", np.mean(-cv_scores_random_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-02 22:31:06,079] A new study created in memory with name: no-name-e15e84a8-de65-4c10-8dcd-972d1de0bec5\n",
      "[I 2024-01-02 22:31:11,314] Trial 0 finished with value: -188000.55722114054 and parameters: {'max_depth': 8, 'min_samples_split': 11}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:19,269] Trial 1 finished with value: -227237.68333928063 and parameters: {'max_depth': 16, 'min_samples_split': 13}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:26,488] Trial 2 finished with value: -230969.0722119074 and parameters: {'max_depth': 12, 'min_samples_split': 6}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:28,608] Trial 3 finished with value: -196994.78752022458 and parameters: {'max_depth': 3, 'min_samples_split': 10}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:36,980] Trial 4 finished with value: -252112.08852993255 and parameters: {'max_depth': 14, 'min_samples_split': 2}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:42,849] Trial 5 finished with value: -191418.75260763027 and parameters: {'max_depth': 9, 'min_samples_split': 16}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:44,983] Trial 6 finished with value: -196994.78752022458 and parameters: {'max_depth': 3, 'min_samples_split': 2}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:53,160] Trial 7 finished with value: -248800.94816601992 and parameters: {'max_depth': 15, 'min_samples_split': 6}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:32:00,508] Trial 8 finished with value: -223291.12512823864 and parameters: {'max_depth': 13, 'min_samples_split': 11}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:32:03,286] Trial 9 finished with value: -177332.90310518988 and parameters: {'max_depth': 4, 'min_samples_split': 14}. Best is trial 9 with value: -177332.90310518988.\n",
      "[I 2024-01-02 22:32:07,325] Trial 10 finished with value: -166462.95517674088 and parameters: {'max_depth': 6, 'min_samples_split': 16}. Best is trial 10 with value: -166462.95517674088.\n",
      "[I 2024-01-02 22:32:10,776] Trial 11 finished with value: -164970.92525860431 and parameters: {'max_depth': 5, 'min_samples_split': 16}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:14,842] Trial 12 finished with value: -166462.95517674088 and parameters: {'max_depth': 6, 'min_samples_split': 16}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:18,933] Trial 13 finished with value: -167729.2418917316 and parameters: {'max_depth': 6, 'min_samples_split': 14}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:23,055] Trial 14 finished with value: -166462.95517674088 and parameters: {'max_depth': 6, 'min_samples_split': 16}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:29,940] Trial 15 finished with value: -220206.73991786418 and parameters: {'max_depth': 11, 'min_samples_split': 8}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:33,396] Trial 16 finished with value: -164970.92525860431 and parameters: {'max_depth': 5, 'min_samples_split': 13}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:34,854] Trial 17 finished with value: -249944.2448512675 and parameters: {'max_depth': 2, 'min_samples_split': 13}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:40,629] Trial 18 finished with value: -194722.70408081438 and parameters: {'max_depth': 9, 'min_samples_split': 12}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:43,423] Trial 19 finished with value: -178239.23871610392 and parameters: {'max_depth': 4, 'min_samples_split': 8}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:48,296] Trial 20 finished with value: -181329.09556918757 and parameters: {'max_depth': 7, 'min_samples_split': 14}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:51,794] Trial 21 finished with value: -164970.92525860431 and parameters: {'max_depth': 5, 'min_samples_split': 15}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:54,620] Trial 22 finished with value: -177332.90310518988 and parameters: {'max_depth': 4, 'min_samples_split': 15}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:58,188] Trial 23 finished with value: -164970.92525860431 and parameters: {'max_depth': 5, 'min_samples_split': 13}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:59,640] Trial 24 finished with value: -249944.2448512675 and parameters: {'max_depth': 2, 'min_samples_split': 15}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:33:04,909] Trial 25 finished with value: -187117.88225127748 and parameters: {'max_depth': 8, 'min_samples_split': 12}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:33:11,094] Trial 26 finished with value: -201036.8343617807 and parameters: {'max_depth': 10, 'min_samples_split': 15}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:33:14,560] Trial 27 finished with value: -164970.92525860431 and parameters: {'max_depth': 5, 'min_samples_split': 14}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:33:19,367] Trial 28 finished with value: -180837.1883359169 and parameters: {'max_depth': 7, 'min_samples_split': 12}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:33:24,644] Trial 29 finished with value: -188000.55722114054 and parameters: {'max_depth': 8, 'min_samples_split': 11}. Best is trial 11 with value: -164970.92525860431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTUNA HPO\n",
      "Best Hyperparameters for KNN: {'max_depth': 5, 'min_samples_split': 16}\n",
      "Execution time:  138.56486415863037\n",
      "CV RMSE Score:  165291.34409779933\n"
     ]
    }
   ],
   "source": [
    "# THIS STAGE TAKES LONG\n",
    "# SECOND STRATEGY - Optuna\n",
    "\n",
    "# Import required libraries\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective_tree(trial):\n",
    "    # Hyperparameters that are going to be tuned\n",
    "    max_depth = trial.suggest_int('max_depth',2,16)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split',2,16)\n",
    "    \n",
    "    # Estimator with suggested hyperparameters\n",
    "    params_tree = {'max_depth': max_depth, 'min_samples_split':min_samples_split}\n",
    "    regr_tree = DecisionTreeRegressor(random_state = rs, **params_tree)\n",
    "    \n",
    "    # Define the neg means squared error as the score and the inner evaluation as corssvalidation \n",
    "    inner_score = cross_val_score(regr_tree, X_train_imputed_tree, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "# Train the model and perform HPO\n",
    "# Measure the time\n",
    "sampler = optuna.samplers.TPESampler(seed=rs)\n",
    "study_tree = optuna.create_study(direction='maximize',sampler=sampler)\n",
    "\n",
    "iterations = 30\n",
    "t_0 = time.time()\n",
    "study_tree.optimize(objective_tree, n_trials = iterations)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params_tree_optuna = study_tree.best_params\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_tree_optuna = DecisionTreeRegressor(**best_params_tree_optuna) \n",
    "cv_scores_optuna_tree = cross_val_score(best_tree_optuna, X_train_imputed_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"OPTUNA HPO\")\n",
    "print(\"Best Hyperparameters for Decission Tree:\", best_params_tree_optuna)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV mean MSE Score: \", np.mean(-cv_scores_optuna_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 6\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 118\n",
      "max_resources_: 3798\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 49\n",
      "n_resources: 118\n",
      "Fitting 5 folds for each of 49 candidates, totalling 245 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 25\n",
      "n_resources: 236\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 13\n",
      "n_resources: 472\n",
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 7\n",
      "n_resources: 944\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 4\n",
      "n_resources: 1888\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 2\n",
      "n_resources: 3776\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "SUCCESIVE HALVING\n",
      "Best Hyperparameters for KNN: {'max_depth': 4, 'min_samples_split': 14}\n",
      "Execution time:  10.173054218292236\n",
      "CV MSE Score:  177251.27479066368\n"
     ]
    }
   ],
   "source": [
    "# THIRD STRATEGY - Successive halving\n",
    "\n",
    "# Import required libraries\n",
    "# Create\n",
    "halving_tree = HalvingGridSearchCV(tree_default,\n",
    "                              params_tree,\n",
    "                              scoring= 'neg_mean_squared_error',\n",
    "                              cv=5,\n",
    "                              random_state=rs,\n",
    "                              factor=2,\n",
    "                              min_resources='exhaust',\n",
    "                              max_resources='auto',\n",
    "                              n_jobs=-1, verbose=1)\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "halving_tree.fit(X_train_imputed_tree, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_halving_tree = halving_tree.best_params_\n",
    "best_knn_halving = DecisionTreeRegressor(**best_params_halving_tree) \n",
    "cv_scores_halving = cross_val_score(best_knn_halving, X_train_imputed_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"SUCCESIVE HALVING\")\n",
    "print(\"Best Hyperparameters for KNN:\", best_params_halving_tree)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV MSE Score: \", np.mean(-cv_scores_halving))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from this hyperparameter tuning are included in the following table:\n",
    "\n",
    "|          | 1. Random Search | 2. OPTUNA | 3. Successive Halving |\n",
    "|:-----------:|:----------------------:|:---------------------:|:----------:|\n",
    "| MSE (crossvalidation) | 197099.40 | 165291.34 | 177251.27 |\n",
    "| max_depth | 4 | 5 | 4 |\n",
    "| min_samples_split | 2 | 16 | 14 |\n",
    "| Time (s) | 7.92 | 138.56 | 10.17 |\n",
    "\n",
    "The method that minimizes the crossvalidation MSE is the OPTUNA, but it is also the method that takes longer time. The Random Search is the method that gets the worse result. Successive halving obtains a low error, close to the Optuna error, but takes about 13 times less time than OPTUNA, so if time was an important variable to consider, then Successive Halving will be the best strategy for this case.\n",
    "As all the three methods have been already implemented, and the time is already spent, the hyperparameters that the OPTUNA returns are the ones that will be used. So in conclussion, max_depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Conclusions and future performance\n",
    "\n",
    "Out of all the trials, the Decision Tree model has demonstrated to work better with the following adjustments:\n",
    "- **Best imputation technique**: KNN Imputer\n",
    "- **Best feature selection strategy**: selecting the features measured in Sotavento.\n",
    "- **Best hyperparameters**:\n",
    " - Maximum depth: 5\n",
    " - Minimum number of samples to split: 16\n",
    " \n",
    "Now, to estimate the future performance of this model, the test set that has been previously set aside will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Trees\n",
      "RMSE:  436.0183191219682\n",
      "Training time (seconds):  5.815073251724243\n"
     ]
    }
   ],
   "source": [
    "# Define optimal hyperparameters\n",
    "max_depth_opt = 5\n",
    "min_samples_split_opt = 16\n",
    "\n",
    "# Custom function for filtering\n",
    "def filter_tree(data):\n",
    "    return data.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# Define the pipeline\n",
    "best_tree = Pipeline([\n",
    "    ('filter', FunctionTransformer(filter_tree, validate=False)),\n",
    "    ('imputer',KNNImputer()),\n",
    "    ('regression',DecisionTreeRegressor(max_depth = max_depth_opt, min_samples_split = min_samples_split_opt))])\n",
    "\n",
    "# Fit training data\n",
    "t_0=time.time()\n",
    "best_tree.fit(X_train,y_train)\n",
    "t_1=time.time()\n",
    "\n",
    "# Predict test data\n",
    "y_pred_best_tree = best_tree.predict(X_test)\n",
    "MSE_best_tree = mean_squared_error(y_test,y_pred_best_tree)\n",
    "\n",
    "print('Decision Trees')\n",
    "print('RMSE: ', np.sqrt(MSE_best_tree))\n",
    "print('Training time (seconds): ', t_1-t_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest Regressor\n",
    "\n",
    "### 5.1 First model\n",
    "\n",
    "A first model using an ensemble of Randomized Decission Trees has been created: a Random Forest. This has been done using the default hyperparameters for the Random Forest, no feature selection and KNN algorithm for imputation of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the first Random Forest model:  392.6583071198578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create the first Random Forest model\n",
    "first_forest = Pipeline([('imputer',KNNImputer()),('regression',RandomForestRegressor(random_state=rs))])\n",
    "first_forest.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_forest = first_forest.predict(X_inner_val)\n",
    "MSE_forest = mean_squared_error(y_inner_val,y_predicted_forest)\n",
    "\n",
    "print('RMSE for the first Random Forest model: ', np.sqrt(MSE_forest))\n",
    "# 392.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature selection\n",
    "\n",
    "Same procedure as in section 3.2. but in this case using Decission Tree Regressor model. The variable selection will not be made again in this section, the filtered datasets are defined in section 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  394.963445594895\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "\n",
    "# The first Random Forest model that has been already created is trained now with the selected data\n",
    "first_forest.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_1_forest = first_forest.predict(X_inner_val_1)\n",
    "MSE_1_forest = mean_squared_error(y_inner_val,y_predicted_1_forest)\n",
    "\n",
    "print('RMSE for feature selection with variables from Sotavento: ', np.sqrt(MSE_1_forest))\n",
    "# 394.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  399.6697849774181\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "# The first Random Forest model that has been already created is trained now with the selected data\n",
    "first_forest.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_2_forest = first_forest.predict(X_inner_val_2)\n",
    "MSE_2_forest = mean_squared_error(y_inner_val,y_predicted_2_forest)\n",
    "\n",
    "print('RMSE for feature selection with variables related to the wind: ', np.sqrt(MSE_2_forest))\n",
    "# 399.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with variables from Sotavento related to the wind:  431.08135954944345\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_forest.fit(X_inner_train_3,y_inner_train)\n",
    "y_predicted_3_forest = first_forest.predict(X_inner_val_3)\n",
    "MSE_3_forest = mean_squared_error(y_inner_val,y_predicted_3_forest)\n",
    "\n",
    "print('RMSE for imputation with variables from Sotavento related to the wind: ', np.sqrt(MSE_3_forest))\n",
    "# 431.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from feature selection with Random Forest model are included in the following table. \n",
    "\n",
    "|          | 0. No feature selection | 1. Sotavento features | 2. Wind features | 2. Wind features in Sotavento |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|:----------:|\n",
    "| RMSE |  392.66 | 394.96 | 399.67 | 431.08 |\n",
    "\n",
    "In this case, feature selection does not reduce the error obtained with the model. So, for the model created with Random Forest, no feature selection will be done in order to obtain the best performance possible.\n",
    "\n",
    "### 5.3. Imputation techniques\n",
    "\n",
    "Same procedure as in section 3.3. but in this case using Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Simple Imputer:  389.15362367205336\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION: Simple Imputer using the mean\n",
    "simple_forest = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',RandomForestRegressor(random_state=rs))])\n",
    "\n",
    "simple_forest.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_simple_imputer_forest = simple_forest.predict(X_inner_val)\n",
    "MSE_simple_forest = mean_squared_error(y_inner_val,y_predicted_simple_imputer_forest)\n",
    "\n",
    "print('RMSE for imputation with Simple Imputer: ', np.sqrt(MSE_simple_forest))\n",
    "# 389.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with KNN Imputer:  392.6583071198578\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION: KNN Imputer with default hyperparameters\n",
    "knn_forest = Pipeline([('imputer',KNNImputer()),('regression',RandomForestRegressor(random_state=rs))])\n",
    "\n",
    "knn_forest.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_knn_imputer_forest = knn_forest.predict(X_inner_val)\n",
    "MSE_knn_forest = mean_squared_error(y_inner_val,y_predicted_knn_imputer_forest)\n",
    "\n",
    "print('RMSE for imputation with KNN Imputer: ', np.sqrt(MSE_knn_forest))\n",
    "# 392.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Iterative Imputer:  383.04874604998815\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION: Iterative Imputer with 10 maximum iterations. Include random seed for reproducibility\n",
    "iterative_forest = Pipeline([('imputer',IterativeImputer(max_iter = 10, random_state = rs)),('regression',RandomForestRegressor(random_state=rs))])\n",
    "\n",
    "iterative_forest.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_iterative_imputer_forest = iterative_forest.predict(X_inner_val)\n",
    "MSE_iterative_forest = mean_squared_error(y_inner_val,y_predicted_iterative_imputer_forest)\n",
    "\n",
    "print('RMSE for imputation with Iterative Imputer: ', np.sqrt(MSE_iterative_forest))\n",
    "# 383.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different imputation techniques are included in the following table. \n",
    "\n",
    "|          | 1. Simple Imputer | 2. KNN Imputer | 3. Iterative Imputer | \n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE |   389.15 | 392.66 | 383.05 |\n",
    "\n",
    "Results from the table show that the best imputation technique for Decission Trees in this case is the Iterative Imputer. One interesting appreciation is that, contratry to the previous cases, the Simple Imputer provides a quite good result. \n",
    "From this point, for the Random Forest model, Iterative Imputer will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Hyperparameter tuning\n",
    "\n",
    "Once the imputation and feature selection have been performed, it is time to improve the performance of the model with HPO. Hyperparameter tuning will be carried out using crossvalidation as the inner evaluation method. In the case of Random Forest, the hyperparameters that will be optimized are:\n",
    "\n",
    "- **n_estimators.** number of estimators, number of Random Decission Trees that form the ensemble.\n",
    "- **max_depth.** maximum depth of the trees that form the ensemble (same as for Decission Trees).\n",
    "- **min_samples_split.** minimum number of samples required to split an internal node (same as for Decission Trees).\n",
    "\n",
    "The same three strategies used in section 3.4. will be repeated in this section but with Decission Tree as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# NO FEATURE SELECTION. For Random Forest, feature selection does not improve the performance\n",
    "\n",
    "# IMPUTER. Imputation of missing values\n",
    "imputer_forest = IterativeImputer(max_iter = 10, random_state = rs)\n",
    "X_train_imputed_forest = imputer_forest.fit_transform(X_train)\n",
    "\n",
    "# Preprocessing also for the test data for estimation of future performance later\n",
    "X_test_imputed_forest = imputer_forest.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL TO OPTIMIZE\n",
    "\n",
    "# Create the regressor with no hyperparamters defined\n",
    "forest_default = RandomForestRegressor()\n",
    "\n",
    "# Create a dictionary with the values for the hyperparameters\n",
    "# Define the grid with all possible values for the hyperparameters\n",
    "# Define the number of folds and the conditions for the crossvalidation\n",
    "params_forest = {'max_depth': list(range(2,16,2)), 'min_samples_split':list(range(2,16,2)), 'n_estimators':list(range(20,200,20))}\n",
    "k_folds = KFold(n_splits = 5, shuffle = True, random_state =rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SEARCH\n",
      "Best Hyperparameters for Random Forest: {'n_estimators': 180, 'min_samples_split': 12, 'max_depth': 14}\n",
      "Execution time:  6212.055189609528\n",
      "CV mean MSE Score:  136218.30892870232\n"
     ]
    }
   ],
   "source": [
    "# FIRST STRATEGY - Randomized Search\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import time\n",
    "\n",
    "# Create a randomized search instance\n",
    "random_search_forest = RandomizedSearchCV(\n",
    "    forest_default,\n",
    "    param_distributions=params_forest,\n",
    "    n_iter=10,  # Number of iterations (can be adjusted)\n",
    "    cv=5,       # Number of cross-validation folds\n",
    "    scoring='neg_mean_squared_error',  # Mean squared error is the error metric\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "random_search_forest.fit(X_train_imputed_forest, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_random_forest = random_search_forest.best_params_\n",
    "best_forest_random = RandomForestRegressor(**best_params_random_forest) \n",
    "cv_scores_random_forest = cross_val_score(best_forest_random, X_train_imputed_forest, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(\"Best Hyperparameters for Random Forest:\", best_params_random_forest)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV mean MSE Score: \", np.mean(-cv_scores_random_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-03 21:52:51,128] A new study created in memory with name: no-name-70311571-0e32-4bda-88ca-27ec0bda607a\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[W 2024-01-03 21:52:59,047] Trial 0 failed with parameters: {'max_depth': 8, 'min_samples_split': 11} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_23276\\2991881163.py\", line 20, in objective_forest\n",
      "    inner_score = cross_val_score(regr_forest, X_train_imputed_forest, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 562, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 214, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 309, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 456, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 188, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1320, in fit\n",
      "    super()._fit(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 443, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "KeyboardInterrupt\n",
      "[W 2024-01-03 21:52:59,058] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\2991881163.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mt_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mstudy_forest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective_forest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mt_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         \"\"\"\n\u001b[1;32m--> 451\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfrozen_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     ):\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\2991881163.py\u001b[0m in \u001b[0;36mobjective_forest\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Define the neg means squared error as the score and the inner evaluation as corssvalidation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0minner_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregr_forest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_imputed_forest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m     cv_results = cross_validate(\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m                     )\n\u001b[0;32m    213\u001b[0m                 ):\n\u001b[1;32m--> 214\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;31m# independent, and that it is pickle-able.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     results = parallel(\n\u001b[0m\u001b[0;32m    310\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    311\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1792\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    454\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m             trees = Parallel(\n\u001b[0m\u001b[0;32m    457\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1792\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1318\u001b[0m         \"\"\"\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m         super()._fit(\n\u001b[0m\u001b[0;32m   1321\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    441\u001b[0m             )\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# THIS STAGE TAKES LONG\n",
    "# SECOND STRATEGY - Optuna\n",
    "\n",
    "# Import required libraries\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective_forest(trial):\n",
    "    # Hyperparameters that are going to be tuned\n",
    "    max_depth = trial.suggest_int('max_depth',2,16)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split',2,16)\n",
    "    n_estimators = trial.suggest_int('min_samples_split',20,200)\n",
    "    \n",
    "    # Estimator with suggested hyperparameters\n",
    "    params_forest = {'max_depth': max_depth, 'min_samples_split':min_samples_split, 'n_estimators': n_estimators}\n",
    "    regr_forest = RandomForestRegressor(random_state = rs, **params_forest)\n",
    "    \n",
    "    # Define the neg means squared error as the score and the inner evaluation as corssvalidation \n",
    "    inner_score = cross_val_score(regr_forest, X_train_imputed_forest, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "# Train the model and perform HPO\n",
    "# Measure the time\n",
    "sampler = optuna.samplers.TPESampler(seed=rs)\n",
    "study_forest = optuna.create_study(direction='maximize',sampler=sampler)\n",
    "\n",
    "iterations = 30\n",
    "t_0 = time.time()\n",
    "study_forest.optimize(objective_forest, n_trials = iterations)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params_forest_optuna = study_forest.best_params\n",
    "\n",
    "# Calculate the crossvalidation ewe3score for this model\n",
    "best_forest_optuna = RandomForestRegressor(**best_params_forest_optuna) \n",
    "cv_scores_optuna_forest = cross_val_score(best_forest_optuna, X_train_imputed_forest, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"OPTUNA HPO\")\n",
    "print(\"Best Hyperparameters for Random Forest:\", best_params_forest_optuna)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV mean MSE Score: \", np.mean(-cv_scores_optuna_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 9\n",
      "n_required_iterations: 9\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 14\n",
      "max_resources_: 3798\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 441\n",
      "n_resources: 14\n",
      "Fitting 5 folds for each of 441 candidates, totalling 2205 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 221\n",
      "n_resources: 28\n",
      "Fitting 5 folds for each of 221 candidates, totalling 1105 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 111\n",
      "n_resources: 56\n",
      "Fitting 5 folds for each of 111 candidates, totalling 555 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 56\n",
      "n_resources: 112\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 28\n",
      "n_resources: 224\n",
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 14\n",
      "n_resources: 448\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 7\n",
      "n_resources: 896\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 4\n",
      "n_resources: 1792\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "----------\n",
      "iter: 8\n",
      "n_candidates: 2\n",
      "n_resources: 3584\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "SUCCESIVE HALVING\n",
      "Best Hyperparameters for Random Forest: {'max_depth': 10, 'min_samples_split': 4, 'n_estimators': 180}\n",
      "Execution time:  2241.9711842536926\n",
      "CV MSE Score:  135585.4593359943\n"
     ]
    }
   ],
   "source": [
    "# THIRD STRATEGY - Successive halving\n",
    "\n",
    "# Import required libraries\n",
    "# Create\n",
    "halving_forest = HalvingGridSearchCV(forest_default,\n",
    "                              params_forest,\n",
    "                              scoring= 'neg_mean_squared_error',\n",
    "                              cv=5,\n",
    "                              random_state=rs,\n",
    "                              factor=2,\n",
    "                              min_resources='exhaust',\n",
    "                              max_resources='auto',\n",
    "                              n_jobs=-1, verbose=1)\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "halving_forest.fit(X_train_imputed_forest, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_halving_forest = halving_forest.best_params_\n",
    "best_forest_halving = RandomForestRegressor(**best_params_halving_forest) \n",
    "cv_scores_halving_forest = cross_val_score(best_forest_halving, X_train_imputed_forest, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"SUCCESIVE HALVING\")\n",
    "print(\"Best Hyperparameters for Random Forest:\", best_params_halving_forest)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV MSE Score: \", np.mean(-cv_scores_halving_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from this hyperparameter tuning are included in the following table:\n",
    "\n",
    "|          | 1. Random Search | 2. OPTUNA | 3. Successive Halving |\n",
    "|:-----------:|:----------------------:|:---------------------:|:----------:|\n",
    "| MSE (crossvalidation) | 136218.31 | 136029.80 | 135585.46 |\n",
    "| max_depth | 14 | 9 | 10 |\n",
    "| min_samples_split | 12 | 16 | 4 |\n",
    "| n_estimators | 180 | DEFAULT | 180 |\n",
    "| Time (s) | 6212.06 | 2096.44 | 2241.97 |\n",
    "\n",
    "The HPO strategy that performs better in this case is the Successive Halving, it is not the fastest strategy but neither the slowest. The slowest in this case, by difference is the Random Search. The hyperparameters from successive halving are the ones that will be chosen, and also it seems to be the most adequate strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Conclusions and future performance\n",
    "\n",
    "Out of all the trials, the Decision Tree model has demonstrated to work better with the following adjustments:\n",
    "- **Best imputation technique**: Iterative Imputer\n",
    "- **Best feature selection strategy**: no feature selection, using all available features.\n",
    "- **Best hyperparameters**:\n",
    " - Maximum depth: 10\n",
    " - Minimum number of samples to split: 4\n",
    " - Number of estimators: 180\n",
    "\n",
    "Now, to estimate the future performance of this model, the test set that has been previously set aside will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "RMSE:  384.3725881703589\n",
      "Training time (seconds):  878.3347749710083\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define optimal hyperparameters\n",
    "max_depth_opt_forest = 10\n",
    "min_samples_split_opt_forest = 4\n",
    "n_estimators_opt_forest = 180\n",
    "\n",
    "# Define the pipeline\n",
    "best_forest = Pipeline([\n",
    "    ('imputer',IterativeImputer(max_iter = 10, random_state = rs)),\n",
    "    ('regression',RandomForestRegressor(max_depth = max_depth_opt_forest, min_samples_split = min_samples_split_opt_forest, n_estimators = n_estimators_opt_forest))])\n",
    "\n",
    "# Fit training data\n",
    "t_0=time.time()\n",
    "best_forest.fit(X_train,y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Predict test data\n",
    "y_pred_best_forest = best_forest.predict(X_test)\n",
    "MSE_best_forest = mean_squared_error(y_test,y_pred_best_forest)\n",
    "\n",
    "print('Random Forest')\n",
    "print('RMSE: ', np.sqrt(MSE_best_forest))\n",
    "print('Training time (seconds): ', t_1-t_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient Boosting with Decision Trees\n",
    "\n",
    "The last model to be built is also an ensemble model. In this case it consists of a Boosting ensemble model composed by Decision Trees. Boosting strategy consists on adding more models to the ensemble to reduce the error of the ensemble model in each iteration. It is usually composed by weak learners, in this case, Decision Trees are the base model to be used.\n",
    "\n",
    "### 6.1 First model\n",
    "\n",
    "First of all, a model will be created with the default hyperparameters, no feature selection and the KNN algorithm for imputation. This model will be taken as reference to compare with other models buit under the same technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the first Gradient Boosting based on KNN:  389.9198065693654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create the first Boosting model\n",
    "# Define the base model for the ensemble method\n",
    "first_boosting = Pipeline([('imputer',KNNImputer()),('regression',GradientBoostingRegressor(n_estimators=100, random_state=rs))])\n",
    "first_boosting.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_boosting = first_boosting.predict(X_inner_val)\n",
    "MSE_boosting = mean_squared_error(y_inner_val,y_predicted_boosting)\n",
    "\n",
    "print('RMSE for the first Gradient Boosting based on KNN: ', np.sqrt(MSE_boosting))\n",
    "# 389.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Feature selection\n",
    "Same procedure as in section 3.2. but in this case using XXX model. The variable selection will not be made again in this section, the filtered datasets are defined in section 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  392.84563560092454\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "\n",
    "# The first Random Forest model that has been already created is trained now with the selected data\n",
    "first_boosting.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_1_boosting = first_boosting.predict(X_inner_val_1)\n",
    "MSE_1_boosting = mean_squared_error(y_inner_val,y_predicted_1_boosting)\n",
    "\n",
    "print('RMSE for feature selection with variables from Sotavento: ', np.sqrt(MSE_1_boosting))\n",
    "# 392.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables related to the wind:  397.4088186271529\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "# The first Random Forest model that has been already created is trained now with the selected data\n",
    "first_boosting.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_2_boosting = first_boosting.predict(X_inner_val_2)\n",
    "MSE_2_boosting = mean_squared_error(y_inner_val,y_predicted_2_boosting)\n",
    "\n",
    "print('RMSE for feature selection with variables related to the wind: ', np.sqrt(MSE_2_boosting))\n",
    "# 397.41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "# The first Random Forest model that has been already created is trained now with the selected data\n",
    "first_boosting.fit(X_inner_train_3,y_inner_train)\n",
    "y_predicted_3_boosting = first_boosting.predict(X_inner_val_3)\n",
    "MSE_3_boosting = mean_squared_error(y_inner_val,y_predicted_3_boosting)\n",
    "\n",
    "print('RMSE for imputation with variables from Sotavento related to the wind: ', np.sqrt(MSE_3_boosting))\n",
    "# 431.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from feature selection with Gradient Boosting based on KNN are included in the following table. \n",
    "\n",
    "|          | 0. No feature selection | 1. Sotavento features | 2. Wind features | 2. Wind features in Sotavento |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|:----------:|\n",
    "| RMSE |  389.92 | 392.85 | 397.41 | 431.08 |\n",
    "\n",
    "In this case, the same that happened with the other ensemble model, feature selection does not improve the performance of the model. So from this point, for the Gradient Boosting model, no feature selection will be used.\n",
    "\n",
    "\n",
    "### 6.3. Imputation techniques\n",
    "\n",
    "Same procedure as in section 3.3. but in this case using Gradient Boosting based on Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Simple Imputer:  383.5967928009943\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION: Simple Imputer using the mean\n",
    "simple_boosting = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',GradientBoostingRegressor(n_estimators=100, random_state=rs))])\n",
    "\n",
    "simple_boosting.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_simple_imputer_boosting = simple_boosting.predict(X_inner_val)\n",
    "MSE_simple_boosting = mean_squared_error(y_inner_val,y_predicted_simple_imputer_boosting)\n",
    "\n",
    "print('RMSE for imputation with Simple Imputer: ', np.sqrt(MSE_simple_boosting))\n",
    "# 383.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with KNN Imputer:  389.9198065693654\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION: KNN Imputer with default hyperparameters\n",
    "knn_boosting = Pipeline([('imputer',KNNImputer()),('regression',GradientBoostingRegressor(n_estimators=100, random_state=rs))])\n",
    "\n",
    "knn_boosting.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_knn_imputer_boosting = knn_boosting.predict(X_inner_val)\n",
    "MSE_knn_boosting = mean_squared_error(y_inner_val,y_predicted_knn_imputer_boosting)\n",
    "\n",
    "print('RMSE for imputation with KNN Imputer: ', np.sqrt(MSE_knn_boosting))\n",
    "# 389.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Iterative Imputer:  381.12467428107595\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION: Iterative Imputer with 10 maximum iterations. Include random seed for reproducibility\n",
    "iterative_boosting = Pipeline([('imputer',IterativeImputer(max_iter = 10, random_state = rs)),('regression',GradientBoostingRegressor(n_estimators=100, random_state=rs))])\n",
    "\n",
    "iterative_boosting.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_iterative_imputer_boosting = iterative_boosting.predict(X_inner_val)\n",
    "MSE_iterative_boosting = mean_squared_error(y_inner_val,y_predicted_iterative_imputer_boosting)\n",
    "\n",
    "print('RMSE for imputation with Iterative Imputer: ', np.sqrt(MSE_iterative_boosting))\n",
    "# 381.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different imputation techniques are included in the following table. \n",
    "\n",
    "|          | 1. Simple Imputer | 2. KNN Imputer | 3. Iterative Imputer | \n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE | 383.60 | 389.92 | 381.12 |\n",
    "\n",
    "Results from the table show that the best imputation technique for Gradient Boosting based on Decisionn Trees is Iterative Imputer. In this case there is not a big difference in the error obtained with the three strategies. It will be determined that the maximum rate   \n",
    "\n",
    "From this point, for the Gradient Boosting based on Decision Trees, Iterative Imputer will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Hyperparameter tuning\n",
    "\n",
    "Once the imputation and feature selection have been performed, it is time to improve the performance of the model with HPO. Hyperparameter tuning will be carried out using crossvalidation as the inner evaluation method. In the case of  this ensemble method, the following hyperparameters will be optimized:\n",
    "\n",
    "- **n_estimtors.** number of Decision Trees (base models) used for the final ensemble model.\n",
    "- **learning_rate.** this value determines how much the contribution of each model does on the final model.\n",
    "- **subsample.** fraction of samples used for fitting the individual trees.\n",
    "- **max_depth.** maximum depth of the Decision Trees.\n",
    "- **min_samples_split.** minimum number of samples in a node to split that node for the Decision Trees.\n",
    "\n",
    "The same three strategies used in section 3.4. will be repeated in this section but with Decission Tree as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# NO FEATURE SELECTION. For Random Forest, feature selection does not improve the performance\n",
    "\n",
    "# IMPUTER. Imputation of missing values\n",
    "imputer_boosting = IterativeImputer(max_iter = 10, random_state = rs)\n",
    "X_train_imputed_boosting = imputer_boosting.fit_transform(X_train)\n",
    "\n",
    "# Preprocessing also for the test data for estimation of future performance later\n",
    "X_test_imputed_boosting = imputer_boosting.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL TO OPTIMIZE\n",
    "\n",
    "# Create the regressor with no hyperparamters defined\n",
    "boosting_default = GradientBoostingRegressor()\n",
    "\n",
    "# Create a dictionary with the values for the hyperparameters\n",
    "# Define the grid with all possible values for the hyperparameters\n",
    "# Define the number of folds and the conditions for the crossvalidation\n",
    "params_boosting = {'max_depth': list(range(2,16,2)), 'min_samples_split':list(range(2,16,2)), 'n_estimators':list(range(20,200,20)), 'learning_rate':list(np.arange(0.2,1.1,0.2)),'subsample':list(np.arange(0.2,1,0.2)) }\n",
    "k_folds = KFold(n_splits = 5, shuffle = True, random_state =rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SEARCH\n",
      "Best Hyperparameters for Gradient Boosting: {'subsample': 0.8, 'n_estimators': 120, 'min_samples_split': 4, 'max_depth': 4, 'learning_rate': 0.2}\n",
      "Execution time:  4667.459298372269\n",
      "CV mean MSE Score:  147999.1642344899\n"
     ]
    }
   ],
   "source": [
    "# FIRST STRATEGY - Randomized Search\n",
    "\n",
    "# Create a randomized search instance\n",
    "random_search_boosting = RandomizedSearchCV(\n",
    "    boosting_default,\n",
    "    param_distributions=params_boosting,\n",
    "    n_iter=10,  # Number of iterations (can be adjusted)\n",
    "    cv=5,       # Number of cross-validation folds\n",
    "    scoring='neg_mean_squared_error',  # Mean squared error is the error metric\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "random_search_boosting.fit(X_train_imputed_boosting, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_random_boosting = random_search_boosting.best_params_\n",
    "best_boosting_random = GradientBoostingRegressor(**best_params_random_boosting) \n",
    "cv_scores_random_boosting = cross_val_score(best_boosting_random, X_train_imputed_boosting, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(\"Best Hyperparameters for Gradient Boosting:\", best_params_random_boosting)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV mean MSE Score: \", np.mean(-cv_scores_random_boosting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-03 11:48:07,279] A new study created in memory with name: no-name-e9c1bbbe-5a11-417a-94df-9498637a777c\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 11:49:28,577] Trial 0 finished with value: -287415.38626304094 and parameters: {'max_depth': 8, 'min_samples_split': 11, 'learning_rate': 0.9809071669436844, 'subsample': 0.7955293712729193}. Best is trial 0 with value: -287415.38626304094.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 11:50:12,359] Trial 1 finished with value: -166939.81709759502 and parameters: {'max_depth': 12, 'min_samples_split': 6, 'learning_rate': 0.2721222495630364, 'subsample': 0.5948751385401271}. Best is trial 1 with value: -166939.81709759502.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 11:50:41,236] Trial 2 finished with value: -205526.5755463304 and parameters: {'max_depth': 14, 'min_samples_split': 2, 'learning_rate': 0.5976059060221985, 'subsample': 0.9942609635643298}. Best is trial 1 with value: -166939.81709759502.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 11:50:43,932] Trial 3 finished with value: -189531.90506464327 and parameters: {'max_depth': 3, 'min_samples_split': 2, 'learning_rate': 0.9454883152913427, 'subsample': 0.37379550061819666}. Best is trial 1 with value: -166939.81709759502.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 11:52:45,149] Trial 4 finished with value: -165472.6106684041 and parameters: {'max_depth': 13, 'min_samples_split': 11, 'learning_rate': 0.3572929950101908, 'subsample': 0.824895480936707}. Best is trial 4 with value: -165472.6106684041.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 11:53:17,252] Trial 5 finished with value: -189508.85525662996 and parameters: {'max_depth': 6, 'min_samples_split': 14, 'learning_rate': 0.44922333937331277, 'subsample': 0.33767660413952205}. Best is trial 4 with value: -165472.6106684041.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 11:54:12,556] Trial 6 finished with value: -149488.50964312392 and parameters: {'max_depth': 4, 'min_samples_split': 13, 'learning_rate': 0.4403419582428158, 'subsample': 0.9015482553745993}. Best is trial 6 with value: -149488.50964312392.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 11:54:35,916] Trial 7 finished with value: -150313.83573041088 and parameters: {'max_depth': 2, 'min_samples_split': 15, 'learning_rate': 0.2864756944738453, 'subsample': 0.6512741634464504}. Best is trial 6 with value: -149488.50964312392.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 11:55:10,157] Trial 8 finished with value: -407299.9354515289 and parameters: {'max_depth': 10, 'min_samples_split': 11, 'learning_rate': 0.6886974625187664, 'subsample': 0.2936048694444434}. Best is trial 6 with value: -149488.50964312392.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 11:55:13,432] Trial 9 finished with value: -555519.123416608 and parameters: {'max_depth': 4, 'min_samples_split': 7, 'learning_rate': 0.9725222523003645, 'subsample': 0.10502904815215335}. Best is trial 6 with value: -149488.50964312392.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 11:59:38,983] Trial 10 finished with value: -188771.0728242363 and parameters: {'max_depth': 16, 'min_samples_split': 16, 'learning_rate': 0.5633197595121093, 'subsample': 0.9961666172484378}. Best is trial 6 with value: -149488.50964312392.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:00:03,971] Trial 11 finished with value: -152135.00013923156 and parameters: {'max_depth': 2, 'min_samples_split': 15, 'learning_rate': 0.24964942037398663, 'subsample': 0.6958616803325341}. Best is trial 6 with value: -149488.50964312392.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:00:46,956] Trial 12 finished with value: -162781.57210151045 and parameters: {'max_depth': 5, 'min_samples_split': 13, 'learning_rate': 0.4280898979903096, 'subsample': 0.5665334250780217}. Best is trial 6 with value: -149488.50964312392.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:01:13,234] Trial 13 finished with value: -163990.49995052157 and parameters: {'max_depth': 2, 'min_samples_split': 13, 'learning_rate': 0.20062374894740953, 'subsample': 0.8397366801862517}. Best is trial 6 with value: -149488.50964312392.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:02:42,526] Trial 14 finished with value: -235370.3640823417 and parameters: {'max_depth': 7, 'min_samples_split': 16, 'learning_rate': 0.7468953637024691, 'subsample': 0.6908660236086177}. Best is trial 6 with value: -149488.50964312392.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:03:06,005] Trial 15 finished with value: -170883.3170451572 and parameters: {'max_depth': 5, 'min_samples_split': 9, 'learning_rate': 0.48102844215369084, 'subsample': 0.45284590288775983}. Best is trial 6 with value: -149488.50964312392.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:04:45,008] Trial 16 finished with value: -164873.7511500256 and parameters: {'max_depth': 10, 'min_samples_split': 13, 'learning_rate': 0.37305245200482673, 'subsample': 0.6980084454333327}. Best is trial 6 with value: -149488.50964312392.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:05:22,956] Trial 17 finished with value: -142567.27036550493 and parameters: {'max_depth': 4, 'min_samples_split': 9, 'learning_rate': 0.33570723047319045, 'subsample': 0.8976146232689017}. Best is trial 17 with value: -142567.27036550493.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:06:29,680] Trial 18 finished with value: -169325.02301017212 and parameters: {'max_depth': 8, 'min_samples_split': 8, 'learning_rate': 0.5073875311417381, 'subsample': 0.9125929706821444}. Best is trial 17 with value: -142567.27036550493.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:06:46,416] Trial 19 finished with value: -155503.44240519035 and parameters: {'max_depth': 4, 'min_samples_split': 4, 'learning_rate': 0.7104901119358342, 'subsample': 0.883704252909868}. Best is trial 17 with value: -142567.27036550493.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:07:35,993] Trial 20 finished with value: -149839.72888220646 and parameters: {'max_depth': 6, 'min_samples_split': 9, 'learning_rate': 0.3600644955141947, 'subsample': 0.7650590649688664}. Best is trial 17 with value: -142567.27036550493.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:08:23,257] Trial 21 finished with value: -150753.81825853884 and parameters: {'max_depth': 6, 'min_samples_split': 9, 'learning_rate': 0.35501967665884027, 'subsample': 0.7607727061353378}. Best is trial 17 with value: -142567.27036550493.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:09:06,952] Trial 22 finished with value: -145068.17844119412 and parameters: {'max_depth': 4, 'min_samples_split': 10, 'learning_rate': 0.4001755165443283, 'subsample': 0.9276136503456286}. Best is trial 17 with value: -142567.27036550493.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:09:54,962] Trial 23 finished with value: -155788.94600724435 and parameters: {'max_depth': 4, 'min_samples_split': 11, 'learning_rate': 0.5035018420223816, 'subsample': 0.9253692637478247}. Best is trial 17 with value: -142567.27036550493.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:10:27,862] Trial 24 finished with value: -147627.12383403588 and parameters: {'max_depth': 3, 'min_samples_split': 10, 'learning_rate': 0.4106334903498502, 'subsample': 0.9210913908387206}. Best is trial 17 with value: -142567.27036550493.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:10:49,354] Trial 25 finished with value: -159142.21792283206 and parameters: {'max_depth': 3, 'min_samples_split': 6, 'learning_rate': 0.3083380758852753, 'subsample': 0.9969051437854988}. Best is trial 17 with value: -142567.27036550493.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:11:20,142] Trial 26 finished with value: -149981.3175751877 and parameters: {'max_depth': 3, 'min_samples_split': 10, 'learning_rate': 0.5460723220507003, 'subsample': 0.8606856484228588}. Best is trial 17 with value: -142567.27036550493.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:11:46,740] Trial 27 finished with value: -250797.49472112634 and parameters: {'max_depth': 7, 'min_samples_split': 7, 'learning_rate': 0.6322933087267866, 'subsample': 0.47829795275473913}. Best is trial 17 with value: -142567.27036550493.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:12:43,002] Trial 28 finished with value: -180540.63018787565 and parameters: {'max_depth': 5, 'min_samples_split': 10, 'learning_rate': 0.8210174878126455, 'subsample': 0.9579888270054124}. Best is trial 17 with value: -142567.27036550493.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 12:14:17,670] Trial 29 finished with value: -146873.67977760217 and parameters: {'max_depth': 9, 'min_samples_split': 12, 'learning_rate': 0.2194231235278018, 'subsample': 0.8001315867980712}. Best is trial 17 with value: -142567.27036550493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTUNA HPO\n",
      "Best Hyperparameters for Gradient Boosting with Decision Trees: {'max_depth': 4, 'min_samples_split': 9, 'learning_rate': 0.33570723047319045, 'subsample': 0.8976146232689017}\n",
      "Execution time:  1570.3911883831024\n",
      "CV mean MSE Score:  163447.9580011363\n"
     ]
    }
   ],
   "source": [
    "# SECOND STRATEGY - Optuna\n",
    "\n",
    "# Define the objective function\n",
    "def objective_boosting(trial):\n",
    "    # Hyperparameters that are going to be tuned\n",
    "    max_depth = trial.suggest_int('max_depth',2,16)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split',2,16)\n",
    "    n_estimators = trial.suggest_int('min_samples_split',20,200)\n",
    "    learning_rate = trial.suggest_float('learning_rate',0.2,1)\n",
    "    subsample = trial.suggest_float('subsample',0.1,1)\n",
    "    \n",
    "    # Estimator with suggested hyperparameters\n",
    "    params_boosting = {'max_depth': max_depth, 'min_samples_split':min_samples_split, 'n_estimators': n_estimators,'learning_rate':learning_rate,'subsample':subsample}\n",
    "    regr_boosting = GradientBoostingRegressor(random_state = rs, **params_boosting)\n",
    "    \n",
    "    # Define the neg means squared error as the score and the inner evaluation as corssvalidation \n",
    "    inner_score = cross_val_score(regr_boosting, X_train_imputed_boosting, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "# Train the model and perform HPO\n",
    "# Measure the time\n",
    "sampler = optuna.samplers.TPESampler(seed=rs)\n",
    "study_boosting = optuna.create_study(direction='maximize',sampler=sampler)\n",
    "\n",
    "iterations = 30\n",
    "t_0 = time.time()\n",
    "study_boosting.optimize(objective_boosting, n_trials = iterations)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params_boosting_optuna = study_boosting.best_params\n",
    "\n",
    "# Calculate the crossvalidation ewe3score for this model\n",
    "best_boosting_optuna = GradientBoostingRegressor(**best_params_boosting_optuna) \n",
    "cv_scores_optuna_boosting = cross_val_score(best_boosting_optuna, X_train_imputed_boosting, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"OPTUNA HPO\")\n",
    "print(\"Best Hyperparameters for Gradient Boosting with Decision Trees:\", best_params_boosting_optuna)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV mean MSE Score: \", np.mean(-cv_scores_optuna_boosting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 9\n",
      "n_required_iterations: 14\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 10\n",
      "max_resources_: 3798\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 8820\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 8820 candidates, totalling 44100 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 4410\n",
      "n_resources: 20\n",
      "Fitting 5 folds for each of 4410 candidates, totalling 22050 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2205\n",
      "n_resources: 40\n",
      "Fitting 5 folds for each of 2205 candidates, totalling 11025 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 1103\n",
      "n_resources: 80\n",
      "Fitting 5 folds for each of 1103 candidates, totalling 5515 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 552\n",
      "n_resources: 160\n",
      "Fitting 5 folds for each of 552 candidates, totalling 2760 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 276\n",
      "n_resources: 320\n",
      "Fitting 5 folds for each of 276 candidates, totalling 1380 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 138\n",
      "n_resources: 640\n",
      "Fitting 5 folds for each of 138 candidates, totalling 690 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 69\n",
      "n_resources: 1280\n",
      "Fitting 5 folds for each of 69 candidates, totalling 345 fits\n",
      "----------\n",
      "iter: 8\n",
      "n_candidates: 35\n",
      "n_resources: 2560\n",
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\2419125975.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Calculate the crossvalidation score for this model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mbest_params_halving_boosting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhalving_boosting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mbest_boosting_halving\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mbest_params_halving_boosting\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mcv_scores_halving_boosting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_boosting_halving\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_imputed_boosting\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'learning_rate'"
     ]
    }
   ],
   "source": [
    "# THIRD STRATEGY - Successive halving\n",
    "\n",
    "# Import required libraries\n",
    "# Create\n",
    "halving_boosting = HalvingGridSearchCV(boosting_default,\n",
    "                              params_boosting,\n",
    "                              scoring= 'neg_mean_squared_error',\n",
    "                              cv=5,\n",
    "                              random_state=rs,\n",
    "                              factor=2,\n",
    "                              min_resources='exhaust',\n",
    "                              max_resources='auto',\n",
    "                              n_jobs=-1, verbose=1)\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "halving_boosting.fit(X_train_imputed_boosting, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_halving_boosting = halving_boosting.best_params_\n",
    "best_boosting_halving = GradientBoostingRegressor(**best_params_halving_boosting) \n",
    "cv_scores_halving_boosting = cross_val_score(best_boosting_halving, X_train_imputed_boosting, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"SUCCESIVE HALVING\")\n",
    "print(\"Best Hyperparameters for Gradient Boosting with Decision Trees:\", best_params_halving_boosting)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV MSE Score: \", np.mean(-cv_scores_halving_boosting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from this hyperparameter tuning are included in the following table:\n",
    "\n",
    "|          | 1. Random Search | 2. OPTUNA | 3. Successive Halving |\n",
    "|:-----------:|:----------------------:|:---------------------:|:----------:|\n",
    "| MSE (crossvalidation) | 147999.16 | 163447.95 | 141089.75 |\n",
    "| max_depth | 4 | 4 | 4 |\n",
    "| min_samples_split | 4 | 9 | 8 |\n",
    "| learning_rate | 0.2 | 0.3357 | 0.2 |\n",
    "| subsample | 0.8 | 0 0.8976 | 0.8 |\n",
    "| n_estimators | 120 | DEFAULT | 40 |\n",
    "| Time (s) | 4667.46 | 1570.39 | 14752.05 |\n",
    "\n",
    "The strategy that obtains the lowest error in this case is the Successive Halving, but it is also the one that takes the longest time. It takes 10 times more than the OPTUNA, and almost 4 times more than the Random Search. As it has already been computed, it will be the strategy to be used. But if the strategy to choose was influenced by the execution time, then Random Search or OPTUNA will be better alternatives.\n",
    "As it has happened in the Random Forest HPO, the OPTUNa does not optimize the value for the number of estimators and leaves it to the default value, adjusting the other hyperparameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Conclusions and future performance\n",
    "\n",
    "Out of all the trials, the Gradient Boosting based on KNN model has demonstrated to work better with the following adjustments:\n",
    "- **Best imputation technique**: Iterative Imputer.\n",
    "- **Best feature selection strategy**: no feature selection. Considering all available variables. \n",
    "- **Best hyperparameters**:\n",
    " - max_depth: 4\n",
    " - min_samples_split: 8\n",
    " - learning_rate: 0.2\n",
    " - subsample: 0.8\n",
    " - n_estimators: 40\n",
    "\n",
    "Now, to estimate the future performance of this model, the test set that has been previously set aside will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting with Decision Trees\n",
      "RMSE:  390.6632315895801\n",
      "Training time (seconds):  619.030433177948\n"
     ]
    }
   ],
   "source": [
    "# Define optimal hyperparameters\n",
    "max_depth_opt_boosting = 4\n",
    "min_samples_split_opt_boosting = 8\n",
    "n_estimators_opt_boosting = 40\n",
    "learning_rate_opt_boosting = 0.2\n",
    "subsample_opt_boosting = 0.8\n",
    "\n",
    "# Define the pipeline\n",
    "best_boosting = Pipeline([\n",
    "    ('imputer',IterativeImputer(max_iter = 10, random_state = rs)),\n",
    "    ('regression',GradientBoostingRegressor(max_depth = max_depth_opt_boosting, min_samples_split = min_samples_split_opt_boosting, n_estimators = n_estimators_opt_boosting, learning_rate = learning_rate_opt_boosting, subsample = subsample_opt_boosting))])\n",
    "\n",
    "# Fit training data\n",
    "t_0 = time.time()\n",
    "best_boosting.fit(X_train,y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Predict test data\n",
    "y_pred_best_boosting = best_boosting.predict(X_test)\n",
    "MSE_best_boosting = mean_squared_error(y_test,y_pred_best_boosting)\n",
    "\n",
    "print('Gradient Boosting with Decision Trees')\n",
    "print('RMSE: ', np.sqrt(MSE_best_boosting))\n",
    "print('Training time (seconds): ', t_1-t_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final model\n",
    "\n",
    "In this stage the four methods have already been created and trained with the complete training set. The future performance has been estimated using the Root Mean Squared Error. The results from the RMSE and the training time are stored in the following table.\n",
    "\n",
    "|          | KNN | Decision Tree | Random Forest | Gradient Boosting |\n",
    "|:--------:|:--------:|:--------:|:--------:|:--------:|\n",
    "| RMSE (future performance) | 407.98 | 436.02 | 384.37 | 390.66 |\n",
    "| Training time (seconds) | 22.81 | 5.82 |  878.33 | 619.03 |\n",
    "\n",
    "The training time is included in the table just to demonstrate that the more complex models (Random Forest and Gradient Boosting with Decision Trees) take the longest training time. But focusing also on the error of the data, it is also true that these are the methods that provide better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusions\n",
    "\n",
    "This project has fullfilled the aim of building a regression model able to predict the energy produced from the meteorological variables. But it also has served to analyze different hyperparameter tuning strategies and compare them.\n",
    "Some conclusions that can be drawn from the optimization of the models are the following:\n",
    "- Feature selection improves the performance of the simple models (KNN and Decision Tree) but in the case of the snesemble models, they work better with no feeature selection.\n",
    "- The best imputation strategy depends on the model that is going to be tuned, but the Iterative Imputer is the one that gives better results in general. However, for the Decision Tree Regressor, the KNN Imputer is the imputation method that provides better result.\n",
    "- The model that, optimized, provides the lowest Mean Squared Error is the Random Forest. The final model will use the hyperparamters already tuned.\n",
    "- The final model, the Random Forest, is also the model that has the longer training time. \n",
    "- The models with lower training time are the simple models, KNN and Decision Tree. These are also the models that provide the lower performances, but if time was a determining factor when choosing the model, those will be the options to consider.\n",
    " \n",
    "### 8.1. Hyperparameter tuning strategies\n",
    "\n",
    "Three different hyperparameter tuning strategies have been followed, in order to perform the HPO of each model. The execution times for each model and each performance are shown in the following graph and in the following table.\n",
    "These results show that clearly, as a general tendency, the more number of hyperparameters that are going to be tuned, the longer the execution time takes. OPTUNA shows that optimizes the execution time when there are several cases to compute (cases 3 and 4, the ensemble models). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['KNN', 'Decision Tree', 'Random Forest', 'Gradient Boosting']\n",
    "strategies = ['Random Search', 'OPTUNA', 'Succesive Halving']\n",
    "execution_times = np.array([[1.38, 7.92, 6212.06, 4667.46],\n",
    "                            [532.77, 138.56, 2096.44, 1570.39],\n",
    "                            [1.92, 10.17, 2241.97, 14752.05]])\n",
    "\n",
    "# Bar width\n",
    "bar_width = 0.2\n",
    "\n",
    "# Set up positions for bars\n",
    "model_positions = np.arange(len(models))\n",
    "\n",
    "# Create grouped bar plot\n",
    "for i, strategy in enumerate(strategies):\n",
    "    plt.bar(model_positions + i * bar_width, execution_times[i], width=bar_width, label=strategy)\n",
    "\n",
    "# Replace numeric x-axis ticks with model names\n",
    "plt.xticks(model_positions + (len(strategies) - 1) * bar_width / 2, models)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Execution Time (seconds)')\n",
    "plt.title('Execution Times for Different Models and Strategies')\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results show that the execution time for Successive Halving is highly affected by the number of hyperparameter combinations that is going to be considered. For example, for KNN and Decision Trees, the nummer of posible combinations is lower and execution time is lower, but for Gradient Boosting, there is a larger number of cimbinations and the execution time is higher.\n",
    "\n",
    "On the other hand, OPTUNA spends a lot of time for the simple models (KNN and Decision Tree) but does not increase that much the optimization time for more complex models (Random Forest and Gradient Boosting). This makes OPTUNA an adequate alternative when dealing with very complex models that have many hyperparameters to be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Training the final model\n",
    "\n",
    "To conclude this study, the final model will be trained using the complete dataset. This training model could be used with the new dataset.\n",
    "The model and the predictions will be stored in two files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;imputer&#x27;, IterativeImputer(random_state=100515585)),\n",
       "                (&#x27;regression&#x27;,\n",
       "                 RandomForestRegressor(max_depth=10, min_samples_split=4,\n",
       "                                       n_estimators=180))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;imputer&#x27;, IterativeImputer(random_state=100515585)),\n",
       "                (&#x27;regression&#x27;,\n",
       "                 RandomForestRegressor(max_depth=10, min_samples_split=4,\n",
       "                                       n_estimators=180))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IterativeImputer</label><div class=\"sk-toggleable__content\"><pre>IterativeImputer(random_state=100515585)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=10, min_samples_split=4, n_estimators=180)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('imputer', IterativeImputer(random_state=100515585)),\n",
       "                ('regression',\n",
       "                 RandomForestRegressor(max_depth=10, min_samples_split=4,\n",
       "                                       n_estimators=180))])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the final model (it is going to be the Random Forest)\n",
    "final_model = best_forest\n",
    "\n",
    "# Fit the whole data\n",
    "X_data = wind_ava.drop('energy', axis=1)\n",
    "y_data = wind_ava['energy']\n",
    "final_model.fit(X_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_model.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save model as a joblib file\n",
    "joblib.dump(final_model, \"final_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Predictions on the competition data\n",
    "\n",
    "Once the model is already prepared for making predictions, the competition dataset is used to compute the predictions of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Read the data that is compressed as a gzip\n",
    "wind_com = pd.read_csv('wind_competition.csv.gzip', compression=\"gzip\")\n",
    "\n",
    "# Display the first rows of the dataset just to see it\n",
    "wind_com.head()\n",
    "\n",
    "# Predict using the competition dataset\n",
    "wind_com['timestamp'] = pd.to_datetime(wind_com[['year', 'month', 'day', 'hour']])\n",
    "wind_com = wind_com.drop(columns=['timestamp'])\n",
    "\n",
    "# Predict the energy production for the competition dataset\n",
    "predictions = final_model.predict(wind_com)\n",
    "\n",
    "np.savetxt('predictions.txt', predictions, delimiter='\\n', fmt='%0.6f')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4. Confidence Intertvalas\n",
    "\n",
    "It is interesting to have confidence intervals for the predictions, apart from the puntual predictions. This will be done using a similar model, but the loss function in the regressor will be in this case the quantile, and it will compute the lower quantile and the higher quantile. The best performance model is Random Forest, that in this case does not provide the possibility to compute the quantiles. So, the best performance Gradient Boosting method will be considered for this purpose, using its predictions only to compute the predictions of the model. \n",
    "\n",
    "For the sake of this study, a 90% confidence interval is considered, so the lower quantile will be the 5% quantile and the higher will be the 95% quantile. It is required to train the model again with these new models.\n",
    "\n",
    "The result for these three predictions will be stored in three different files:\n",
    "- punctual_pred.txt: for the predictions of the model\n",
    "- 5quantile_pred.txt: for the lower bound interval predictions\n",
    "- 95quantile_pred.txt: for the higher bound interval predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11628\\2446902889.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[1;34m'imputer'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIterativeImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     ('regression',GradientBoostingRegressor(loss='quantile', alpha=lower_quantile,random_state=rs,max_depth = max_depth_opt_boosting, min_samples_split = min_samples_split_opt_boosting, n_estimators = n_estimators_opt_boosting, learning_rate = learning_rate_opt_boosting, subsample = subsample_opt_boosting))])\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mlower_boosting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Train a model for the upper quantile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m    422\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    375\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    378\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    958\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    763\u001b[0m                     \u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabs_corr_mat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                 )\n\u001b[1;32m--> 765\u001b[1;33m                 Xt, estimator = self._impute_one_feature(\n\u001b[0m\u001b[0;32m    766\u001b[0m                     \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                     \u001b[0mmask_missing_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py\u001b[0m in \u001b[0;36m_impute_one_feature\u001b[1;34m(self, X_filled, mask_missing_values, feat_idx, neighbor_feat_idx, estimator, fit_mode)\u001b[0m\n\u001b[0;32m    410\u001b[0m                 \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m             )\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[1;31m# if no missing values, don't predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0mXT_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0meigen_vals_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;31m# perform decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0m\u001b[0;32m    128\u001b[0m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define optimal hyperparameters\n",
    "max_depth_opt_boosting = 4\n",
    "min_samples_split_opt_boosting = 8\n",
    "n_estimators_opt_boosting = 40\n",
    "learning_rate_opt_boosting = 0.2\n",
    "subsample_opt_boosting = 0.8\n",
    "\n",
    "\n",
    "# Specify the quantiles\n",
    "lower_quantile = 0.5\n",
    "upper_quantile = 0.95\n",
    "\n",
    "# Train a model for the lower quantile\n",
    "lower_boosting = Pipeline([\n",
    "    ('imputer',IterativeImputer(max_iter = 10, random_state = rs)),\n",
    "    ('regression',GradientBoostingRegressor(loss='quantile', alpha=lower_quantile,random_state=rs,max_depth = max_depth_opt_boosting, min_samples_split = min_samples_split_opt_boosting, n_estimators = n_estimators_opt_boosting, learning_rate = learning_rate_opt_boosting, subsample = subsample_opt_boosting))])\n",
    "lower_boosting.fit(X_data, y_data)\n",
    "\n",
    "# Train a model for the upper quantile\n",
    "upper_boosting = Pipeline([\n",
    "    ('imputer',IterativeImputer(max_iter = 10, random_state = rs)),\n",
    "    ('regression',GradientBoostingRegressor(loss='quantile', alpha=upper_quantile,random_state=rs,max_depth = max_depth_opt_boosting, min_samples_split = min_samples_split_opt_boosting, n_estimators = n_estimators_opt_boosting, learning_rate = learning_rate_opt_boosting, subsample = subsample_opt_boosting))])\n",
    "upper_boosting.fit(X_data, y_data)\n",
    "\n",
    "# Make predictions for the competition dataset\n",
    "lower_prediction = lower_boosting.predict(wind_com)\n",
    "upper_prediction = upper_boosting.predict(wind_com)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
