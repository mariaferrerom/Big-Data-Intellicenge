{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 1\n",
    "\n",
    "## Lara Monteserín Placer\n",
    "\n",
    "## María Ferrero Medina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this project is to design a machine learning model that is able to predict the energy produced by the Sotavento wind farm. For this purpose, a dataset with 555 features ans 4748 instances is available.\n",
    "\n",
    "The structure of the study will be the following:\n",
    "\n",
    "1. Exploratory Data Analysis\n",
    "2. Methodology\n",
    "3. KNN model\n",
    "4. Decission tree model\n",
    "5. Ensemble method 1 model\n",
    "6. Ensemble method 2 model\n",
    "7. Selection and performance of the final model\n",
    "\n",
    "For each of the models created, several steps have been followed to optimize them. First of all, a simple version of each model is created, with hyperparameters that seem reasonable, no feature selection and a basic imputation technique. Then, sequentially, models are improved.\n",
    "\n",
    "1. First model\n",
    "2. Feature selection\n",
    "3. Imputation techniques\n",
    "4. Hyperparameter tuning\n",
    "\n",
    "\n",
    "Things to add: - another idea (new library?, new idea...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-hPXBr4r5rS"
   },
   "source": [
    "\n",
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "Before starting to build the model, an EDA is made as a first approach to gain understanding of the dataset. In this Exploratory Data Analysis the data type of the features will be verified, the number of instances and features will be determined. Also, a brief summary of the missing values and columns with constant value will be included. \n",
    "\n",
    "### 1.1. Number of instances and features\n",
    "\n",
    "This dataset has 4748 instances and 555 features.\n",
    "\n",
    "### 1.2. Nature of the variables\n",
    "\n",
    "This dataset contains information about the meteorological conditions in several locations, the time the measures of these conditions were made and the energy produced at each moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy     float64\n",
      "year         int64\n",
      "month        int64\n",
      "day          int64\n",
      "hour         int64\n",
      "            ...   \n",
      "v100.21    float64\n",
      "v100.22    float64\n",
      "v100.23    float64\n",
      "v100.24    float64\n",
      "v100.25    float64\n",
      "Length: 555, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Read the data that is compressed as a gzip\n",
    "wind_ava = pd.read_csv('wind_available.csv.gzip', compression=\"gzip\")\n",
    "\n",
    "# Display the first rows of the dataset just to see it\n",
    "wind_ava.head()\n",
    "\n",
    "# Display the data type of each column\n",
    "column_data_types = wind_ava.dtypes\n",
    "print(column_data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having checked the data types of all the different features, it has been verified that there are:\n",
    "\n",
    "- 551 numerical variables (real numbers). From this 551, one is the energy, that is the output of the problem. And the remaining 550 are relative to the 22 different meteorological conditions measured at the 25 different locations.\n",
    "\n",
    "- 4 numerical variables (integers). These 4 variables are the year, day, month and hour of the day. These variables characterize the moment the measures were taken.\n",
    "\n",
    "### 1.3. Check for missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Column  Null Values  NaN Values\n",
      "0     energy            0           0\n",
      "1       year            0           0\n",
      "2      month            0           0\n",
      "3        day            0           0\n",
      "4       hour            0           0\n",
      "..       ...          ...         ...\n",
      "550  v100.21          261         261\n",
      "551  v100.22          387         387\n",
      "552  v100.23          569         569\n",
      "553  v100.24          579         579\n",
      "554  v100.25          436         436\n",
      "\n",
      "[555 rows x 3 columns]\n",
      "Number of columns with Null Values: 550\n",
      "Number of columns with NaN Values: 550\n"
     ]
    }
   ],
   "source": [
    "# Return the number of Null values for each column\n",
    "null_values = wind_ava.isnull().sum()\n",
    "# Return the number of NaN values for each column (just in case they are not the same)\n",
    "nan_values = wind_ava.isna().sum()\n",
    "\n",
    "# Store in missing values the amount of Null and NaN values of each column\n",
    "missing_values = pd.DataFrame({\n",
    "    'Column': null_values.index,\n",
    "    'Null Values': null_values.values,\n",
    "    'NaN Values': nan_values.values\n",
    "})\n",
    "\n",
    "# Print the amount of Null and Nan values\n",
    "print(missing_values)\n",
    "\n",
    "# Identify columns with Null or NaN values\n",
    "columns_with_null = wind_ava.columns[wind_ava.isnull().any()]\n",
    "columns_with_nan = wind_ava.columns[wind_ava.isna().any()]\n",
    "\n",
    "# Display the number of columns which have missing values\n",
    "print(\"Number of columns with Null Values:\", len(columns_with_null))\n",
    "print(\"Number of columns with NaN Values:\", len(columns_with_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All meteorological variables have missing values in different instances. The 4 categories that characterize the moment the measure was made and the target feature'energy' do not have missing values.\n",
    "\n",
    "\n",
    "### 1.4. Check for constant columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with constant values: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check for constant values in each column\n",
    "constant_columns = wind_ava.columns[wind_ava.nunique() == 1]\n",
    "\n",
    "# Print columns with constant values\n",
    "print(\"Columns with constant values:\", constant_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are no constant columns. \n",
    "\n",
    "\n",
    "### 1.5. Type of problem\n",
    "\n",
    "The objective of the model is to estimate the energy, as it is a continuous numerical value, this is a **regression problem**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "\n",
    "This section will explain the methodology that is going to be followed to evaluate the models. The evaluation techniques that will be used for outer evaluation and inner evaluation. And also the metrics that will determine the future performance of the model.\n",
    "The structure of the assignment will be to first optimize four different models following different strategies. And then compare the best model obtained for each of these four cases and choose the best performance model that has been reached.\n",
    "The creation and optimmization of each model has been structured as follows: \n",
    "- First, a simple approach of the models with KNN as imputation technique, no feature selection and default hyperparameters is created. This aims to obtain a first approach mod\n",
    "- Second, different feature selection strategies will be made, by hand. Three cases are considered and the best alterntive in terms of the minimum error, is chosen. The goal when performing feature selection first, is to reduce the dimensionality of the data in the following stages.\n",
    "- Third, different imputation techniques will be compared and then, again, the best strategy in terms of the error will be chosen. Here, the idea is to have the imputation technique that works better for the model before starting the optimization of the hyperparameters.\n",
    "- Finaly, to obtain the final version of each model, hyperparameter tuning is done. Three different strategies are used for hyperparameter tuning, this is in order to compare the execution times of each of them. But the error will be the strategy to determine later which combination of hyperparameters is the best one.\n",
    "All these stages will be carried out using only the training set, then, to estimate the future performance, \n",
    "\n",
    "- On the one hand, for the **outer evaluation**, holdout evaluation will be used. Thie method will be used to estimate the future performance of the designed method. So, a train set will be used for all the stages of model building and then a test set will be kept to estimate the future performance in an unbiased manner. \n",
    "\n",
    "- On the other hand, for the **inner evaluation**, crossvalidation will be the method applied. Crossvalidation will be used to determine which is the best combination of hyperparameters. Also, to determine the best strategy for feature selection and for imputation techniques, crossvalidation will be used, using only the available data in the training set. \n",
    "\n",
    "Later, to improve the performance of the method, once it is already computed the outer performance, the hyperparameters will be tuned again but this time using the whole training set. And finaly, the model will be trained using the whole training dataset. \n",
    "\n",
    "The objective function that is going to be used for the validation of the method, and for comparing the methods between them is the Root Mean Squared Error (RMSE). This metric is more sensitive to outliers and to distant values than the MAE as it squares the magnitudes. This is useful to penalize the errors that are larger, and avoid having a model that might have such large errors. RMSE has been used instead of MSE as it is easier to interpret considering it has the same order of magnitude than the target feature (the energy in this case).\n",
    "\n",
    "Once this study has been done, the models will be trained using the complete dataset to develop the final model. This final model will be then used with the new data that is provided in a separate dataset. \n",
    "\n",
    "**It could be longer this explanation**\n",
    "\n",
    "Note: as the variable that is going to be predicted is the 'energy', only the variables related to the meteorological characteristics will be used. It is considered that the energy produced does not depend on the moment of the day it is being produced.\n",
    "\n",
    "\n",
    "The train and test split that is done for holdout validation is made considering that the data comes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we define a generic random seed that will be used along the assignment\n",
    "rs = 100515585 \n",
    "\n",
    "# First, data will be divided into train and test set \n",
    "# Considering it is a time series, it must be split in an appropiate way\n",
    "wind_ava['timestamp'] = pd.to_datetime(wind_ava[['year', 'month', 'day', 'hour']])\n",
    "wind_ava = wind_ava.sort_values(by='timestamp')\n",
    "\n",
    "train_size = 0.8 \n",
    "split_index = int(len(wind_ava) * train_size)\n",
    "wind_ava = wind_ava.drop(columns=['timestamp'])\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "train_data = wind_ava.iloc[:split_index]\n",
    "test_data = wind_ava.iloc[split_index:]\n",
    "\n",
    "X_train = train_data.drop('energy', axis=1)\n",
    "y_train = train_data['energy']\n",
    "X_test = test_data.drop('energy', axis=1)\n",
    "y_test = test_data['energy']\n",
    "\n",
    "# For inner validation for feature selection and imputation\n",
    "# Divide the train set into inner train set (for training) and inner validation set (for validation)\n",
    "\n",
    "train_size_inner = 0.8\n",
    "split_index_inner = int(len(train_data) * train_size_inner)\n",
    "\n",
    "# Divide the data Inner Train Set and Inner Validation Set\n",
    "train_inner = train_data.iloc[:split_index_inner]\n",
    "val_inner = train_data.iloc[split_index_inner:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KNN Regressor\n",
    "\n",
    "The algorithm to try is the KNN algorithm applied for Regression. In this section, different imputation techniques and feature selection strategies will  be used to determine which is the most adequate. Later, hyperparameter tuning will determine the optimal number of neighbors to be used. Finally, a final version of the model will be built taking all these studies into consideration. \n",
    "\n",
    "### 3.1. First model\n",
    "\n",
    "A first model using the KNN algorithm will be implemented. It will use the default hyperparameters, the KNN Imputer as imputation strategy and no feature selection. This model will be updated and improved later but a first approach is required to have a model to compare in further stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the first KNN model:  650.972879219549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Now the first KNN model will be created\n",
    "first_knn = Pipeline([('imputer',KNNImputer()),('regression',KNeighborsRegressor())])\n",
    "first_knn.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_first = first_knn.predict(X_inner_val)\n",
    "MSE = mean_squared_error(y_inner_val,y_predicted_first)\n",
    "\n",
    "print('RMSE for the first KNN model: ', np.sqrt(MSE))\n",
    "# RMSE: 650.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature selection\n",
    "\n",
    "Now, feature selection will be used in order to remove variables that are not affecting the target and thus avoid overfitting that may be lead by wrong interpretation from these variables. This has been done manually, selecting different feature combinations that seem to be accurate for the problem. Three different cases have been considered:\n",
    "- FIRST OPTION. Selecting only the features related to the location of the wind farm (Sotavento). This is the features that contain the sufix 13.\n",
    "- SECOND OPTION. Selecting only the features related to the wind characteristics. This is the features that start with u or v (the vertical and horizontal components of the wind).\n",
    "- THIRD OPTION. Selecting only the features which are both located in Sotavento (location 13) and related to the wind characteristics. This is the features that start with u or v and end with sufix 13.\n",
    "\n",
    "This strategy will be followed for the feature selection of the rest of the models. This is sections 4.2, 5.2 and 6.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  658.2887149048381\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "\n",
    "# Select the desired variables\n",
    "X_inner_train_1 = train_inner.filter(regex='\\.13$', axis=1)\n",
    "X_inner_val_1 = val_inner.filter(regex='\\.13$', axis=1)\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_knn.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_1 = first_knn.predict(X_inner_val_1)\n",
    "MSE_1 = mean_squared_error(y_inner_val,y_predicted_1)\n",
    "\n",
    "print('RMSE for feature selection with variables from Sotavento: ', np.sqrt(MSE_1))\n",
    "# 658.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with variables related to the wind:  428.12271404149146\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "# Select the desired variables\n",
    "X_inner_train_2 = train_inner.filter(regex='^(u|v).*$', axis=1)\n",
    "X_inner_val_2 = val_inner.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_knn.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_2 = first_knn.predict(X_inner_val_2)\n",
    "MSE_2 = mean_squared_error(y_inner_val,y_predicted_2)\n",
    "\n",
    "print('RMSE for imputation with variables related to the wind: ', np.sqrt(MSE_2))\n",
    "# 428.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with variables from Sotavento related to the wind:  433.4095713141627\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "# Select the desired variables\n",
    "X_inner_train_3 = train_inner.filter(regex='^(u|v).*\\.13$', axis=1)\n",
    "X_inner_val_3 = val_inner.filter(regex='^(u|v).*\\.13$', axis=1)\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_knn.fit(X_inner_train_3,y_inner_train)\n",
    "y_predicted_3 = first_knn.predict(X_inner_val_3)\n",
    "MSE_3 = mean_squared_error(y_inner_val,y_predicted_3)\n",
    "\n",
    "print('RMSE for imputation with variables from Sotavento related to the wind: ', np.sqrt(MSE_3))\n",
    "# 433.40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different feature selection strategies are included in the following table. These results show that the best feature selection strategy for the model is the second, selecting only the features related to the wind. Feature selection improves the performance of the model as it discards features that are not affecting the target and may lead to overfitting. \n",
    "\n",
    "|          | 0. No feature selection | 1. Sotavento features | 2. Wind features | 3. Sotavento wind features |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|:----------:|\n",
    "| RMSE | 658.28 | 663.35 | 428.12 | 433.40 |\n",
    "\n",
    "\n",
    "From this moment, the KNN model will be built using only the features related to the wind, that is the variables categorized as X_train_inner_2, X_val_inner_2.\n",
    "\n",
    "### 3.3. Imputation techniques\n",
    "\n",
    "The dataset includes many missing values, it is important to apply an adequate imputation technique. In this section, three different imputation techniques will be considered and applied. The results obtained from this study will determine which is the imputation technique that will be followed later. Three different strategies have been studied:\n",
    "- FIRST OPTION: Simple Imputer. It is an univariate imputation technique, this means that only uses values relative to the feature that is going to be imputed. It imputes the missing values with the mean of the feature. The mean is been chosen instead of the median because there are not many outliers that could affect the distribution of the data.\n",
    "- SECOND OPTION: KNN Imputer. It is a multivariate imputation technique, this means that it uses values from the feature that is going to be imputed but also from other features. This technique in partiular is based in the KNN algorithm and it consistsn on imputing the value os the missing category as a mean from the closest neighbors.\n",
    "- THIRD OPTION: Iterative Imputer. It is also a muktivarite technique. Is based on iterative models that compute the values for the missing categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Simple Imputer:  457.67268509394927\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION: Simple Imputer using the mean\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "simple_knn = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',KNeighborsRegressor())])\n",
    "\n",
    "simple_knn.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_simple_imputer = simple_knn.predict(X_inner_val_2)\n",
    "MSE_simple = mean_squared_error(y_inner_val,y_predicted_simple_imputer)\n",
    "\n",
    "print('RMSE for imputation with Simple Imputer: ', np.sqrt(MSE_simple))\n",
    "# 457.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with KNN Imputer:  428.12271404149146\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION: KNN Imputer with default hyperparameters\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "knn_knn = Pipeline([('imputer',KNNImputer()),('regression',KNeighborsRegressor())])\n",
    "\n",
    "knn_knn.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_knn_imputer = knn_knn.predict(X_inner_val_2)\n",
    "MSE_knn = mean_squared_error(y_inner_val,y_predicted_knn_imputer)\n",
    "\n",
    "print('RMSE for imputation with KNN Imputer: ', np.sqrt(MSE_knn))\n",
    "# 428.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the version of scikit-learn if it returns an Error with IterativeImputer\n",
    "!pip install --upgrade scikit-learn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Iterative Imputer:  426.13631965182225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# THIRD OPTION: multivariate technique. Iterative imputer with 10 maximum iterations. Include random seed for reproducibility\n",
    "iterative_knn = Pipeline([('imputer',IterativeImputer(max_iter = 10, random_state = rs)),('regression',KNeighborsRegressor())])\n",
    "\n",
    "iterative_knn.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_iterative_imputer = iterative_knn.predict(X_inner_val_2)\n",
    "MSE_iterative = mean_squared_error(y_inner_val,y_predicted_iterative_imputer)\n",
    "\n",
    "print('RMSE for imputation with Iterative Imputer: ', np.sqrt(MSE_iterative))\n",
    "# 426.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          | 1. Simple Imputer | 2. KNN Imputer | 3. Iterative Imputer|\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE | 457.67 | 428.12 | 426.14 |\n",
    "\n",
    "From the results above, it is demonstrated that the Simple Imputer is the worst in terms of performance with respect to the Mean Squared Error.\n",
    "\n",
    "On the other hand, the multivariate techniques, KNN Imputer and the Iterative Imputer give similar results. The Iterative Imputer gets a lower Mean Squared Error, so it will be the chosen technique. \n",
    "\n",
    "### 3.4. Hyperparameter tuning\n",
    "\n",
    "Once the preprocessing stages have been optimized and improved, it is time to optimize the model by performing hyperparameter tuning. Crossvalidation will be the stratefy for HPO. In the case of KNN algorithm, the main hyperparameter is the number of neighbors, this number of neighbors. \n",
    "\n",
    "Several approaches for hyperparameter tuning will be implemented. The optimization time will be measured and also the best combination of hyperparameters for each method will be compared. This additional study will provide insight in which HPO strategy is the most adequate in terms of execution time and also in terms of performance.\n",
    "\n",
    "For HPO, Pipelines will not be used, the preprocessing stages will be made previously in order to make the process faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# FILTER. Feature selection\n",
    "X_train_filtered = X_train.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# IMPUTER. Imputation of missing values\n",
    "imputer = IterativeImputer(max_iter=10, random_state=rs)\n",
    "X_train_imputed = imputer.fit_transform(X_train_filtered)\n",
    "# X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_filtered), columns=X_train_filtered.columns)\n",
    "\n",
    "\n",
    "# Preprocessing also for the test data for estimation of future performance later\n",
    "X_test_filtered = X_test.filter(regex='\\.13$', axis=1)\n",
    "X_test_imputed = imputer.fit_transform(X_test_filtered)\n",
    "# X_test_imputed_df = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL TO OPTIMIZE\n",
    "\n",
    "# Create the regressor with no hyperparamters defined\n",
    "knn_default = KNeighborsRegressor()\n",
    "\n",
    "# Create a dictionary with the values for the hyperparameters\n",
    "params_knn = {'n_neighbors': np.arange(1, 51)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params {'n_neighbors': 17}\n",
      "RANDOM SEARCH\n",
      "Best Hyperparameters for KNN: {'n_neighbors': 17}\n",
      "Execution time:  1.3825676441192627\n",
      "CV MSE Score:  391.7182984002424\n"
     ]
    }
   ],
   "source": [
    "# FIRST STRATEGY - Randomized Search\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import time\n",
    "\n",
    "# Create a randomized search instance\n",
    "random_search = RandomizedSearchCV(\n",
    "    knn_default,\n",
    "    param_distributions=params_knn,\n",
    "    n_iter=10,  # Number of iterations (can be adjusted)\n",
    "    cv=5,       # Number of cross-validation folds\n",
    "    scoring='neg_mean_squared_error',  # Mean squared error is the error metric\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "random_search.fit(X_train_imputed, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_random = random_search.best_params_\n",
    "best_knn_random = KNeighborsRegressor(**best_params_random) \n",
    "cv_scores = cross_val_score(best_knn_random, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_cv_rmse = np.mean(np.sqrt(-cv_scores))\n",
    "\n",
    "# Print results\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(\"Best Hyperparameters for KNN:\", best_params_random)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV MSE Score: \", mean_cv_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-02 20:25:24,937] A new study created in memory with name: no-name-af39b7fe-e670-473c-adfb-839e51849685\n",
      "[I 2024-01-02 20:25:42,513] Trial 0 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 21}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:26:00,560] Trial 1 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 33}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:26:18,122] Trial 2 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 50}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:26:35,977] Trial 3 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 40}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:26:54,141] Trial 4 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 37}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:27:11,860] Trial 5 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 15}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:27:29,976] Trial 6 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 5}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:27:47,920] Trial 7 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 29}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:28:05,333] Trial 8 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 43}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:28:23,486] Trial 9 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 2}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:28:41,494] Trial 10 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 19}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:28:59,205] Trial 11 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 28}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:29:17,130] Trial 12 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 19}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:29:34,643] Trial 13 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 35}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:29:52,162] Trial 14 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 10}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:30:09,881] Trial 15 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 24}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:30:27,378] Trial 16 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 32}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:30:44,994] Trial 17 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 22}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:31:02,356] Trial 18 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 45}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:31:20,238] Trial 19 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 10}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:31:37,853] Trial 20 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 32}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:31:55,417] Trial 21 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 49}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:32:13,060] Trial 22 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 50}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:32:30,768] Trial 23 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 44}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:32:48,872] Trial 24 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 15}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:33:06,929] Trial 25 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 38}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:33:24,653] Trial 26 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 25}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:33:42,263] Trial 27 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 32}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:33:59,975] Trial 28 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 20}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 20:34:17,713] Trial 29 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 51}. Best is trial 0 with value: -170261.21944310627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTUNA HPO\n",
      "Best Hyperparameters for KNN: {'n_neighbors': 21}\n",
      "Execution time:  532.7751214504242\n",
      "CV RMSE Score:  391.52835078116124\n"
     ]
    }
   ],
   "source": [
    "# THIS STAGE TAKES LONG\n",
    "# SECOND STRATEGY - Optuna\n",
    "\n",
    "# Import required libraries\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameters that are to be tuned\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 51)\n",
    "    params = {'n_neighbors': n_neighbors}\n",
    "    \n",
    "    # Estimator with suggested hyperparameters\n",
    "    knn_default = KNeighborsRegressor()\n",
    "    \n",
    "    # Define the neg means squared error as the score and the inner evaluation as corssvalidation \n",
    "    inner_score = cross_val_score(iterative_knn, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "# Train the model and perform HPO\n",
    "# Measure the time\n",
    "sampler = optuna.samplers.TPESampler(seed=rs)\n",
    "study_knn = optuna.create_study(direction='maximize',sampler=sampler)\n",
    "\n",
    "iterations = 30\n",
    "t_0 = time.time()\n",
    "study_knn.optimize(objective, n_trials = iterations)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params_optuna = study_knn.best_params\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_knn_optuna = KNeighborsRegressor(**best_params_optuna) \n",
    "cv_scores_optuna = cross_val_score(best_knn_optuna, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_cv_rmse_optuna = np.mean(np.sqrt(-cv_scores_optuna))\n",
    "\n",
    "# Print results\n",
    "print(\"OPTUNA HPO\")\n",
    "print(\"Best Hyperparameters for KNN:\", best_params_optuna)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV RMSE Score: \", mean_cv_rmse_optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 6\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 118\n",
      "max_resources_: 3798\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 50\n",
      "n_resources: 118\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 25\n",
      "n_resources: 236\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 13\n",
      "n_resources: 472\n",
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 7\n",
      "n_resources: 944\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 4\n",
      "n_resources: 1888\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 2\n",
      "n_resources: 3776\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "SUCCESIVE HALVING\n",
      "Best Hyperparameters for KNN: {'n_neighbors': 11}\n",
      "Execution time:  1.9224305152893066\n",
      "CV MSE Score:  393.42255429468116\n"
     ]
    }
   ],
   "source": [
    "# THIRD STRATEGY - Successive halving\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "\n",
    "# Create\n",
    "halving = HalvingGridSearchCV(knn_default,\n",
    "                              params_knn,\n",
    "                              scoring= 'neg_mean_squared_error',\n",
    "                              cv=5,\n",
    "                              random_state=rs,\n",
    "                              factor=2,\n",
    "                              min_resources='exhaust',\n",
    "                              max_resources='auto',\n",
    "                              n_jobs=-1, verbose=1)\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "halving.fit(X_train_imputed, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_halving = halving.best_params_\n",
    "best_knn_halving = KNeighborsRegressor(**best_params_halving) \n",
    "cv_scores_halving = cross_val_score(best_knn_halving, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_cv_rmse_halving = np.mean(np.sqrt(-cv_scores_halving))\n",
    "\n",
    "# Print results\n",
    "print(\"SUCCESIVE HALVING\")\n",
    "print(\"Best Hyperparameters for KNN:\", best_params_halving)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV MSE Score: \", mean_cv_rmse_halving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          | 1. Random Search | 2. OPTUNA | 3. Successive Halving |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE (crossvalidation) | 391.71 | 391.52 | 393.42 |\n",
    "| n_neighbors | 17 | 21 | 11 |\n",
    "| Time (s) | 1.38 | 532.77 | 1.92 |\n",
    "\n",
    "The results from the HPO show that the number of neighbors does not seem to affect the performance when it changes from 17 to 21, as the crossvalidation scores are the same. But, it also shows that the  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Conclusions about KNN model\n",
    "\n",
    "Out of all the trials, the KNN model has demonstrated to work better with the following adjustments:\n",
    "- Best number of neighbors:\n",
    "- Best imputation technique: Iterative Imputer\n",
    "- Best feature selection strategy: selecting the variables related to wind characteristics.\n",
    "Now, to estimate the future performance of this model, the test set that has been previously set aside will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the optimized KNN model:  407.98965946652226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Define optimal hyperparameters\n",
    "opt_k = 17\n",
    "\n",
    "# Custom function for filtering\n",
    "def filter_knn(data):\n",
    "    return data.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# Define the pipeline\n",
    "best_knn = Pipeline([\n",
    "    ('filter', FunctionTransformer(filter_knn, validate=False)),\n",
    "    ('imputer',IterativeImputer(max_iter = 10, random_state = rs)),\n",
    "    ('regression',KNeighborsRegressor(n_neighbors = opt_k))])\n",
    "\n",
    "# Fit training data\n",
    "best_knn.fit(X_train,y_train)\n",
    "\n",
    "# Predict test data\n",
    "y_pred_best = best_knn.predict(X_test)\n",
    "MSE_best_knn = mean_squared_error(y_test,y_pred_best)\n",
    "\n",
    "print('RMSE for the optimized KNN model: ', np.sqrt(MSE_best_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Training the KNN model with the whole training set\n",
    "\n",
    "Now that feature selection, imputation and hyperparameters have been optimized, the final model is trained using the whole wind_ava dataset. A pipeline with all the optimized stages will be done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Tree\n",
    "\n",
    "### 4.1. First model\n",
    "\n",
    "A first model using the Decision Tree algorithm will be implemented. It will use the default hyperparameters, the KNN Imputer as imputation strategy and no feature selection. This model will be updated and improved but it is just a simple approach to later compare the adjusted models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the first KNN model:  650.972879219549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create the first Decission Tree model\n",
    "first_tree = Pipeline([('imputer',KNNImputer()),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "first_tree.fit(X_inner_train,y_inner_train)\n",
    "y_predicted = first_tree.predict(X_inner_val)\n",
    "MSE_tree = mean_squared_error(y_inner_val,y_predicted_first)\n",
    "\n",
    "print('RMSE for the first KNN model: ', np.sqrt(MSE_tree))\n",
    "# 650.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Feature selection\n",
    "\n",
    "Same procedure as in section 3.2. but in this case using Decission Tree Regressor model. The variable selection will not be made again in this section, the filtered datasets are defined in section 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  522.4285433416003\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "\n",
    "# The first Decission Tree model that has been already created is trained now with the selected data\n",
    "first_tree.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_1 = first_tree.predict(X_inner_val_1)\n",
    "MSE_1 = mean_squared_error(y_inner_val,y_predicted_1)\n",
    "\n",
    "print('RMSE for feature selection with variables from Sotavento: ', np.sqrt(MSE_1))\n",
    "# 522.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  541.6336936084882\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "# The first Decission Tree model that has been already created is trained now with the selected data\n",
    "first_tree.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_2 = first_tree.predict(X_inner_val_2)\n",
    "MSE_2 = mean_squared_error(y_inner_val,y_predicted_2)\n",
    "\n",
    "print('RMSE for feature selection with variables related to the wind: ', np.sqrt(MSE_2))\n",
    "# 541.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with variables from Sotavento related to the wind:  543.9981445138152\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_tree.fit(X_inner_train_3,y_inner_train)\n",
    "y_predicted_3 = first_tree.predict(X_inner_val_3)\n",
    "MSE_3 = mean_squared_error(y_inner_val,y_predicted_3)\n",
    "\n",
    "print('RMSE for imputation with variables from Sotavento related to the wind: ', np.sqrt(MSE_3))\n",
    "# 544.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different feature selection strategies are included in the following table. These results show that the feature selection strategy that works better with Decission Trees is the first strategy, this is, choosing the variables related to Sotavneto only.\n",
    "\n",
    "|          | 0. No feature selection | 1. Sotavento features | 2. Wind features | 2. Wind features in Sotavento |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|:----------:|\n",
    "| RMSE |  650.97 | 522.42 | 541.63 | 543.99 |\n",
    "\n",
    "\n",
    "From this moment, the KNN model will be built using only the features that were measured in Sotavento.\n",
    "\n",
    "The best RMSE achieved until this point is: 522.42\n",
    "\n",
    "### 4.3. Imputation techniques\n",
    "\n",
    "Same procedure as in section 3.3. but in this case using Decission Tree Regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Simple Imputer:  546.3131100360276\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION: Simple Imputer using the mean\n",
    "simple_tree = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "\n",
    "simple_tree.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_simple_imputer = simple_tree.predict(X_inner_val_1)\n",
    "MSE_simple = mean_squared_error(y_inner_val,y_predicted_simple_imputer)\n",
    "\n",
    "print('RMSE for imputation with Simple Imputer: ', np.sqrt(MSE_simple))\n",
    "# 546.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with KNN Imputer:  522.4285433416003\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION: KNN Imputer with default hyperparameters\n",
    "knn_tree = Pipeline([('imputer',KNNImputer()),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "\n",
    "knn_tree.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_knn_imputer = knn_tree.predict(X_inner_val_1)\n",
    "MSE_knn = mean_squared_error(y_inner_val,y_predicted_knn_imputer)\n",
    "\n",
    "print('RMSE for imputation with KNN Imputer: ', np.sqrt(MSE_knn))\n",
    "# 522.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Iterative Imputer:  527.2567305971542\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION: Iterative Imputer with 10 maximum iterations. Include random seed for reproducibility\n",
    "iterative_tree = Pipeline([('imputer',IterativeImputer(max_iter = 10, random_state = rs)),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "\n",
    "iterative_tree.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_iterative_imputer = iterative_tree.predict(X_inner_val_1)\n",
    "MSE_iterative = mean_squared_error(y_inner_val,y_predicted_iterative_imputer)\n",
    "\n",
    "print('RMSE for imputation with Iterative Imputer: ', np.sqrt(MSE_iterative))\n",
    "# 527.26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different imputation techniques are included in the following table. \n",
    "\n",
    "|          | 1. Simple Imputer | 2. KNN Imputer | 3. Iterative Imputer | \n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE |  546.31 | 522.43 | 527.26 |\n",
    "\n",
    "Results from the table show that the best imputation technique for Decission Trees in this case is the KNN Imputer with default hyperparameters. From this point, for the Decission Tree model, KNN Imputer will be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Hyperparameter tuning\n",
    "\n",
    "Once the imputation and feature selection have been performed, it is time to improve the performance of the model with HPO. Hyperparameter tuning will be carried out using crossvalidation as the inner evaluation method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# FILTER. Feature selection\n",
    "X_train_filtered_tree = X_train.filter(regex='\\.13$', axis=1)\n",
    "\n",
    "# IMPUTER. Imputation of missing values\n",
    "imputer_tree = KNNImputer()\n",
    "X_train_imputed_tree = imputer_tree.fit_transform(X_train_filtered_tree)\n",
    "\n",
    "# Preprocessing also for the test data for estimation of future performance later\n",
    "X_test_filtered_tree = X_test.filter(regex='\\.13$', axis=1)\n",
    "X_test_imputed_tree = imputer_tree.fit_transform(X_test_filtered_tree)\n",
    "# X_test_imputed_df = pd.DataFrame(X_test_imputed, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL TO OPTIMIZE\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create the regressor with no hyperparamters defined\n",
    "tree_default = DecisionTreeRegressor()\n",
    "\n",
    "# Create a dictionary with the values for the hyperparameters\n",
    "# Define the grid with all possible values for the hyperparameters\n",
    "# Define the number of folds and the conditions for the crossvalidation\n",
    "params_tree = {'max_depth': list(range(2,16,2)), 'min_samples_split':list(range(2,16,2))}\n",
    "k_folds = KFold(n_splits = 5, shuffle = True, random_state =rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SEARCH\n",
      "Best Hyperparameters for Decission Tree: {'min_samples_split': 2, 'max_depth': 6}\n",
      "Execution time:  3.1810808181762695\n",
      "CV mean MSE Score:  196838.08065346064\n"
     ]
    }
   ],
   "source": [
    "# FIRST STRATEGY - Randomized Search\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import time\n",
    "\n",
    "# Create a randomized search instance\n",
    "random_search_tree = RandomizedSearchCV(\n",
    "    tree_default,\n",
    "    param_distributions=params_tree,\n",
    "    n_iter=10,  # Number of iterations (can be adjusted)\n",
    "    cv=5,       # Number of cross-validation folds\n",
    "    scoring='neg_mean_squared_error',  # Mean squared error is the error metric\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "random_search_tree.fit(X_train_imputed_tree, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_random_tree = random_search_tree.best_params_\n",
    "best_tree_random = DecisionTreeRegressor(**best_params_random_tree) \n",
    "cv_scores_random_tree = cross_val_score(best_tree_random, X_train_imputed_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(\"Best Hyperparameters for Decission Tree:\", best_params_random_tree)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV mean MSE Score: \", np.mean(-cv_scores_random_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\3985550352.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Fit the models (all possible combinations)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtuning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_imputed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mt_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train_2' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "# FIRST STRATEGY: GridSearch \n",
    "\n",
    "# Measure the time it takes to perform the hyperparameter tuning\n",
    "t_0 = time.time()\n",
    "# Perform the hyperparameter tuning using the previously defined k_folds for crossvalidation\n",
    "tuning = GridSearchCV(\n",
    "    tree_default,\n",
    "    param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=k_folds,\n",
    "    n_jobs=1,verbose=1)\n",
    "\n",
    "# Fit the models (all possible combinations)\n",
    "tuning.fit(X_train_imputed,y_train_2)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Access the best estimator\n",
    "best_model = tuning.best_estimator_\n",
    "\n",
    "# Access the best hyperparameters\n",
    "best_params = tuning.best_params_\n",
    "\n",
    "# Access the cross-validated results\n",
    "cv_results = tuning.cv_results_\n",
    "\n",
    "# Print MSE for each fold\n",
    "for i in range(5):\n",
    "    fold_mse = -cv_results[f\"split{i}_test_score\"][tuning.best_index_]\n",
    "    print(f\"MSE for Fold {i+1}: {fold_mse}\")\n",
    "\n",
    "# Calculate and print the mean MSE across folds\n",
    "mean_mse = -cv_results[\"mean_test_score\"][tuning.best_index_]\n",
    "\n",
    "\n",
    "# Print the results for the best combination\n",
    "print(\"Mean MSE across Folds: \", mean_mse)\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "\n",
    "# ESTIMATE FUTURE PERFORMANCE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN's MSE with HPO:  189639.60576969493\n"
     ]
    }
   ],
   "source": [
    "# Estimation of future performance\n",
    "y_test_pred = tuning.predict(X_test_imputed)\n",
    "\n",
    "print(\"KNN's MSE with HPO: \",\n",
    "     mean_squared_error(y_test_2, y_test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OPTUNA if required\n",
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# SECOND STRATEGY: Optuna\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameters that are going to be tuned\n",
    "    max_depth = trial.suggest_int('max_depth',2,16)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split',2,16)\n",
    "    \n",
    "    # Estimator with suggested hyperparameters\n",
    "    params = {'max_depth': max_depth, 'min_samples_split':min_samples_split}\n",
    "    regr_tree = DecisionTreeRegressor(random_state = 100515585, **params)\n",
    "    \n",
    "    # Define the neg means squared error as the score and the inner evaluation as corssvalidation \n",
    "    inner_score = cross_val_score(regr_tree,X_train_2,y_train_2,cv=k_folds,scoring='neg_mean_absolute_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "# Train the model and perform HPO\n",
    "# Measure the time\n",
    "t_0 = time.time()\n",
    "sampler = optuna.samplers.TPESampler(seed=rs)\n",
    "study = optuna.create_study(direction='maximize',sampler=sampler)\n",
    "iterations = 30\n",
    "study.optimize(objective, n_trials = iterations)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "regr_opt = DecisionTreeRegressor(random_state = rs, **best_params)\n",
    "regr_opt.fit(X_train_2,y_train_2)\n",
    "\n",
    "# ESTIMATE FUTURE PERFORMANCE\n",
    "y_test_pred = regr_opt.predict(X_test_2)\n",
    "mse_future = mean_squared_error(y_test_pred, y_test_2)\n",
    "\n",
    "# Print results\n",
    "# Print the results for the best combination\n",
    "print(\"MSE future performance: \", mse_future)\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "print(\"Execution time: \",t_1-t_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install optuna if required \n",
    "!pip install optuna scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Successive Halving\n",
    "\n",
    "\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 16)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 16)\n",
    "    params = {'max_depth': max_depth, 'min_samples_split': min_samples_split}\n",
    "    regr_tree = DecisionTreeRegressor(random_state=100515585, **params)\n",
    "    inner_score = cross_val_score(regr_tree, X_train_2, y_train_2, cv=k_folds, scoring='neg_mean_absolute_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    t_0 = time.time()\n",
    "\n",
    "    # Use TPESampler for Bayesian optimization\n",
    "    sampler = optuna.samplers.TPESampler(seed=100515585)\n",
    "    study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "\n",
    "    # Run optimization\n",
    "    n_trials = 30\n",
    "    for _ in range(n_trials):\n",
    "        trial = study.ask()\n",
    "        value = objective(trial)\n",
    "        study.tell(trial, value)\n",
    "\n",
    "    t_1 = time.time()\n",
    "\n",
    "    best_params = study.best_params\n",
    "    regr_opt = DecisionTreeRegressor(random_state=100515585, **best_params)\n",
    "    regr_opt.fit(X_train_2, y_train_2)\n",
    "\n",
    "    y_test_pred = regr_opt.predict(X_test_2)\n",
    "    mse_future = mean_squared_error(y_test_pred, y_test_2)\n",
    "\n",
    "    print(\"MSE future performance: \", mse_future)\n",
    "    print(\"Best Hyperparameters: \", best_params)\n",
    "    print(\"Execution time: \", t_1 - t_0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest Regressor\n",
    "\n",
    "### 5.1 First model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "t_0 = time.time()\n",
    "# Create the ensemble model pipeline\n",
    "random_forest = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42)) \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "first_forest = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42)) \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "first_forest.fit(X_train_1, y_train_1)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predicted_1_rf = first_forest.predict(X_test_1)\n",
    "\n",
    "# Evaluate the model\n",
    "MSE_1 = mean_squared_error(y_test_1, y_predicted_1_rf)\n",
    "print(\"Mean Squared Error:\", MSE_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "second_forest = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42)) \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "second_forest.fit(X_train_2,y_train_2)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predicted_2_rf = second_forest.predict(X_test_2)\n",
    "\n",
    "# Evaluate the model\n",
    "MSE_2 = mean_squared_error(y_test_2, y_predicted_2_rf)\n",
    "print(\"Mean Squared Error:\", MSE_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "third_forest = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42)) \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "third_forest.fit(X_train_3,y_train_3)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predicted_3_rf = third_forest.predict(X_test_3)\n",
    "\n",
    "# Evaluate the model\n",
    "MSE_3 = mean_squared_error(y_test_3, y_predicted_3_rf)\n",
    "print(\"Mean Squared Error:\", MSE_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El error no mejora aplicando feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bagging Regressor with KNN models\n",
    "\n",
    "The last model to be built is also an ensemble model. In this case it consists of an aggregation of KNN models. \n",
    "### 6.1 First model\n",
    "\n",
    "First of all, a model will be created with the default hyperparameters, no feature selection and the KNN algorithm for imputation. This model will be taken as reference to compare with other models buit under the same technique. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# Create the ensemble model pipeline\n",
    "knn_ensemble_pipeline = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', BaggingRegressor(base_estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=10, random_state=42))  \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "knn_ensemble_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = knn_ensemble_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "\n",
    "first_ensemble = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', BaggingRegressor(base_estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=10, random_state=42))  \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "first_ensemble.fit(X_train_1, y_train_1)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predicted_1_e = first_ensemble.predict(X_test_1)\n",
    "\n",
    "# Evaluate the model\n",
    "MSE_1 = mean_squared_error(y_test_1, y_predicted_1_e)\n",
    "print(\"Mean Squared Error:\", MSE_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "second_ensemble = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', BaggingRegressor(base_estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=10, random_state=42))  \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "second_ensemble.fit(X_train_2, y_train_2)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predicted_2_e = second_ensemble.predict(X_test_2)\n",
    "\n",
    "# Evaluate the model\n",
    "MSE_2 = mean_squared_error(y_test_2, y_predicted_2_e)\n",
    "print(\"Mean Squared Error:\", MSE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "third_ensemble = Pipeline([\n",
    "    ('imputer',KNNImputer(n_neighbors=3)),\n",
    "    ('regressor', BaggingRegressor(base_estimator=KNeighborsRegressor(n_neighbors=5), n_estimators=10, random_state=42))  \n",
    "])\n",
    "\n",
    "# Fit the model on the training data\n",
    "third_ensemble.fit(X_train_3, y_train_3)\n",
    "\n",
    "# Predict on the test data\n",
    "y_predicted_3_e = third_ensemble.predict(X_test_3)\n",
    "\n",
    "# Evaluate the model\n",
    "MSE_3 = mean_squared_error(y_test_3, y_predicted_3_e)\n",
    "print(\"Mean Squared Error:\", MSE_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous results, it is demonstrated that the error from the model improves applying feature selection. The best results are obtained selecting only the features related to the wind. The second best result is obtained selecting only the features from Sotavento and related to the wind.\n",
    "\n",
    "Based on this results, for the KNN model, only the features related to the wind will be selected (second option). So, from now on, it will be used X_2 and y_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
