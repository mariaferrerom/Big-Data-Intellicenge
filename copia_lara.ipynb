{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASSIGNMENT 1\n",
    "\n",
    "## Lara Monteserín Placer\n",
    "\n",
    "## María Ferrero Medina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this project is to design a machine learning model that is able to predict the energy produced by the Sotavento wind farm. For this purpose, a dataset with 555 features ans 4748 instances is available.\n",
    "\n",
    "The structure of the study will be the following:\n",
    "\n",
    "1. Exploratory Data Analysis\n",
    "2. Methodology\n",
    "3. KNN model\n",
    "4. Decission tree model\n",
    "5. Ensemble method 1 model\n",
    "6. Ensemble method 2 model\n",
    "7. Selection and performance of the final model\n",
    "\n",
    "For each of the models created, several steps have been followed to optimize them. First of all, a simple version of each model is created, with hyperparameters that seem reasonable, no feature selection and a basic imputation technique. Then, sequentially, models are improved.\n",
    "\n",
    "1. First model\n",
    "2. Feature selection\n",
    "3. Imputation techniques\n",
    "4. Hyperparameter tuning\n",
    "\n",
    "\n",
    "Things to add: - another idea (new library?, new idea...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-hPXBr4r5rS"
   },
   "source": [
    "\n",
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "Before starting to build the model, an EDA is made as a first approach to gain understanding of the dataset. In this Exploratory Data Analysis the data type of the features will be verified, the number of instances and features will be determined. Also, a brief summary of the missing values and columns with constant value will be included. \n",
    "\n",
    "### 1.1. Number of instances and features\n",
    "\n",
    "This dataset has 4748 instances and 555 features.\n",
    "\n",
    "### 1.2. Nature of the variables\n",
    "\n",
    "This dataset contains information about the meteorological conditions in several locations, the time the measures of these conditions were made and the energy produced at each moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy     float64\n",
      "year         int64\n",
      "month        int64\n",
      "day          int64\n",
      "hour         int64\n",
      "            ...   \n",
      "v100.21    float64\n",
      "v100.22    float64\n",
      "v100.23    float64\n",
      "v100.24    float64\n",
      "v100.25    float64\n",
      "Length: 555, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Read the data that is compressed as a gzip\n",
    "wind_ava = pd.read_csv('wind_available.csv.gzip', compression=\"gzip\")\n",
    "\n",
    "# Display the first rows of the dataset just to see it\n",
    "wind_ava.head()\n",
    "\n",
    "# Display the data type of each column\n",
    "column_data_types = wind_ava.dtypes\n",
    "print(column_data_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having checked the data types of all the different features, it has been verified that there are:\n",
    "\n",
    "- 551 numerical variables (real numbers). From this 551, one is the energy, that is the output of the problem. And the remaining 550 are relative to the 22 different meteorological conditions measured at the 25 different locations.\n",
    "\n",
    "- 4 numerical variables (integers). These 4 variables are the year, day, month and hour of the day. These variables characterize the moment the measures were taken.\n",
    "\n",
    "### 1.3. Check for missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Column  Null Values  NaN Values\n",
      "0     energy            0           0\n",
      "1       year            0           0\n",
      "2      month            0           0\n",
      "3        day            0           0\n",
      "4       hour            0           0\n",
      "..       ...          ...         ...\n",
      "550  v100.21          261         261\n",
      "551  v100.22          387         387\n",
      "552  v100.23          569         569\n",
      "553  v100.24          579         579\n",
      "554  v100.25          436         436\n",
      "\n",
      "[555 rows x 3 columns]\n",
      "Number of columns with Null Values: 550\n",
      "Number of columns with NaN Values: 550\n"
     ]
    }
   ],
   "source": [
    "# Return the number of Null values for each column\n",
    "null_values = wind_ava.isnull().sum()\n",
    "# Return the number of NaN values for each column (just in case they are not the same)\n",
    "nan_values = wind_ava.isna().sum()\n",
    "\n",
    "# Store in missing values the amount of Null and NaN values of each column\n",
    "missing_values = pd.DataFrame({\n",
    "    'Column': null_values.index,\n",
    "    'Null Values': null_values.values,\n",
    "    'NaN Values': nan_values.values\n",
    "})\n",
    "\n",
    "# Print the amount of Null and Nan values\n",
    "print(missing_values)\n",
    "\n",
    "# Identify columns with Null or NaN values\n",
    "columns_with_null = wind_ava.columns[wind_ava.isnull().any()]\n",
    "columns_with_nan = wind_ava.columns[wind_ava.isna().any()]\n",
    "\n",
    "# Display the number of columns which have missing values\n",
    "print(\"Number of columns with Null Values:\", len(columns_with_null))\n",
    "print(\"Number of columns with NaN Values:\", len(columns_with_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All meteorological variables have missing values in different instances. The 4 categories that characterize the moment the measure was made and the target feature'energy' do not have missing values.\n",
    "\n",
    "\n",
    "### 1.4. Check for constant columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with constant values: Index([], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check for constant values in each column\n",
    "constant_columns = wind_ava.columns[wind_ava.nunique() == 1]\n",
    "\n",
    "# Print columns with constant values\n",
    "print(\"Columns with constant values:\", constant_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are no constant columns. \n",
    "\n",
    "\n",
    "### 1.5. Type of problem\n",
    "\n",
    "The objective of the model is to estimate the energy, as it is a continuous numerical value, this is a **regression problem**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "\n",
    "This section will explain the methodology that is going to be followed to evaluate the models. The evaluation techniques that will be used for outer evaluation and inner evaluation. And also the metrics that will determine the future performance of the model.\n",
    "The structure of the assignment will be to first optimize four different models following different strategies. And then compare the best model obtained for each of these four cases and choose the best performance model that has been reached.\n",
    "The creation and optimmization of each model has been structured as follows: \n",
    "- First, a simple approach of the models with KNN as imputation technique, no feature selection and default hyperparameters is created. This aims to obtain a first approach mod\n",
    "- Second, different feature selection strategies will be made, by hand. Three cases are considered and the best alterntive in terms of the minimum error, is chosen. The goal when performing feature selection first, is to reduce the dimensionality of the data in the following stages.\n",
    "- Third, different imputation techniques will be compared and then, again, the best strategy in terms of the error will be chosen. Here, the idea is to have the imputation technique that works better for the model before starting the optimization of the hyperparameters.\n",
    "- Finaly, to obtain the final version of each model, hyperparameter tuning is done. Three different strategies are used for hyperparameter tuning, this is in order to compare the execution times of each of them. But the error will be the strategy to determine later which combination of hyperparameters is the best one.\n",
    "All these stages will be carried out using only the training set, then, to estimate the future performance, \n",
    "\n",
    "- On the one hand, for the **outer evaluation**, holdout evaluation will be used. Thie method will be used to estimate the future performance of the designed method. So, a train set will be used for all the stages of model building and then a test set will be kept to estimate the future performance in an unbiased manner. \n",
    "\n",
    "- On the other hand, for the **inner evaluation**, crossvalidation will be the method applied. Crossvalidation will be used to determine which is the best combination of hyperparameters. Also, to determine the best strategy for feature selection and for imputation techniques, crossvalidation will be used, using only the available data in the training set. \n",
    "\n",
    "Later, to improve the performance of the method, once it is already computed the outer performance, the hyperparameters will be tuned again but this time using the whole training set. And finaly, the model will be trained using the whole training dataset. \n",
    "\n",
    "The objective function that is going to be used for the validation of the method, and for comparing the methods between them is the Root Mean Squared Error (RMSE). This metric is more sensitive to outliers and to distant values than the MAE as it squares the magnitudes. This is useful to penalize the errors that are larger, and avoid having a model that might have such large errors. RMSE has been used instead of MSE as it is easier to interpret considering it has the same order of magnitude than the target feature (the energy in this case).\n",
    "\n",
    "Once this study has been done, the models will be trained using the complete dataset to develop the final model. This final model will be then used with the new data that is provided in a separate dataset. \n",
    "\n",
    "**It could be longer this explanation**\n",
    "\n",
    "Note: as the variable that is going to be predicted is the 'energy', only the variables related to the meteorological characteristics will be used. It is considered that the energy produced does not depend on the moment of the day it is being produced.\n",
    "\n",
    "\n",
    "The train and test split that is done for holdout validation is made considering that the data comes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we define a generic random seed that will be used along the assignment\n",
    "rs = 100515585 \n",
    "\n",
    "# First, data will be divided into train and test set \n",
    "# Considering it is a time series, it must be split in an appropiate way\n",
    "wind_ava['timestamp'] = pd.to_datetime(wind_ava[['year', 'month', 'day', 'hour']])\n",
    "wind_ava = wind_ava.sort_values(by='timestamp')\n",
    "\n",
    "train_size = 0.8 \n",
    "split_index = int(len(wind_ava) * train_size)\n",
    "wind_ava = wind_ava.drop(columns=['timestamp'])\n",
    "\n",
    "# Divide the data into X_train, y_train, X_test, y_test\n",
    "train_data = wind_ava.iloc[:split_index]\n",
    "test_data = wind_ava.iloc[split_index:]\n",
    "\n",
    "X_train = train_data.drop('energy', axis=1)\n",
    "y_train = train_data['energy']\n",
    "X_test = test_data.drop('energy', axis=1)\n",
    "y_test = test_data['energy']\n",
    "\n",
    "# For inner validation for feature selection and imputation\n",
    "# Divide the train set into inner train set (for training) and inner validation set (for validation)\n",
    "\n",
    "train_size_inner = 0.8\n",
    "split_index_inner = int(len(train_data) * train_size_inner)\n",
    "\n",
    "# Divide the data Inner Train Set and Inner Validation Set\n",
    "train_inner = train_data.iloc[:split_index_inner]\n",
    "val_inner = train_data.iloc[split_index_inner:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KNN Regressor\n",
    "\n",
    "The algorithm to try is the KNN algorithm applied for Regression. In this section, different imputation techniques and feature selection strategies will  be used to determine which is the most adequate. Later, hyperparameter tuning will determine the optimal number of neighbors to be used. Finally, a final version of the model will be built taking all these studies into consideration. \n",
    "\n",
    "### 3.1. First model\n",
    "\n",
    "A first model using the KNN algorithm will be implemented. It will use the default hyperparameters, the KNN Imputer as imputation strategy and no feature selection. This model will be updated and improved later but a first approach is required to have a model to compare in further stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the first KNN model:  650.972879219549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Now the first KNN model will be created\n",
    "first_knn = Pipeline([('imputer',KNNImputer()),('regression',KNeighborsRegressor())])\n",
    "first_knn.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_first = first_knn.predict(X_inner_val)\n",
    "MSE = mean_squared_error(y_inner_val,y_predicted_first)\n",
    "\n",
    "print('RMSE for the first KNN model: ', np.sqrt(MSE))\n",
    "# RMSE: 650.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature selection\n",
    "\n",
    "Now, feature selection will be used in order to remove variables that are not affecting the target and thus avoid overfitting that may be lead by wrong interpretation from these variables. This has been done manually, selecting different feature combinations that seem to be accurate for the problem. Three different cases have been considered:\n",
    "- FIRST OPTION. Selecting only the features related to the location of the wind farm (Sotavento). This is the features that contain the sufix 13.\n",
    "- SECOND OPTION. Selecting only the features related to the wind characteristics. This is the features that start with u or v (the vertical and horizontal components of the wind).\n",
    "- THIRD OPTION. Selecting only the features which are both located in Sotavento (location 13) and related to the wind characteristics. This is the features that start with u or v and end with sufix 13.\n",
    "\n",
    "This strategy will be followed for the feature selection of the rest of the models. This is sections 4.2, 5.2 and 6.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  658.2887149048381\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "\n",
    "# Select the desired variables\n",
    "X_inner_train_1 = train_inner.filter(regex='\\.13$', axis=1)\n",
    "X_inner_val_1 = val_inner.filter(regex='\\.13$', axis=1)\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_knn.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_1 = first_knn.predict(X_inner_val_1)\n",
    "MSE_1 = mean_squared_error(y_inner_val,y_predicted_1)\n",
    "\n",
    "print('RMSE for feature selection with variables from Sotavento: ', np.sqrt(MSE_1))\n",
    "# 658.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with variables related to the wind:  428.12271404149146\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "# Select the desired variables\n",
    "X_inner_train_2 = train_inner.filter(regex='^(u|v).*$', axis=1)\n",
    "X_inner_val_2 = val_inner.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_knn.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_2 = first_knn.predict(X_inner_val_2)\n",
    "MSE_2 = mean_squared_error(y_inner_val,y_predicted_2)\n",
    "\n",
    "print('RMSE for imputation with variables related to the wind: ', np.sqrt(MSE_2))\n",
    "# 428.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with variables from Sotavento related to the wind:  433.4095713141627\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "# Select the desired variables\n",
    "X_inner_train_3 = train_inner.filter(regex='^(u|v).*\\.13$', axis=1)\n",
    "X_inner_val_3 = val_inner.filter(regex='^(u|v).*\\.13$', axis=1)\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_knn.fit(X_inner_train_3,y_inner_train)\n",
    "y_predicted_3 = first_knn.predict(X_inner_val_3)\n",
    "MSE_3 = mean_squared_error(y_inner_val,y_predicted_3)\n",
    "\n",
    "print('RMSE for imputation with variables from Sotavento related to the wind: ', np.sqrt(MSE_3))\n",
    "# 433.40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different feature selection strategies are included in the following table. These results show that the best feature selection strategy for the model is the second, selecting only the features related to the wind. Feature selection improves the performance of the model as it discards features that are not affecting the target and may lead to overfitting. \n",
    "\n",
    "|          | 0. No feature selection | 1. Sotavento features | 2. Wind features | 3. Sotavento wind features |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|:----------:|\n",
    "| RMSE | 658.28 | 663.35 | 428.12 | 433.40 |\n",
    "\n",
    "\n",
    "From this moment, the KNN model will be built using only the features related to the wind, that is the variables categorized as X_train_inner_2, X_val_inner_2.\n",
    "\n",
    "### 3.3. Imputation techniques\n",
    "\n",
    "The dataset includes many missing values, it is important to apply an adequate imputation technique. In this section, three different imputation techniques will be considered and applied. The results obtained from this study will determine which is the imputation technique that will be followed later. Three different strategies have been studied:\n",
    "- FIRST OPTION: Simple Imputer. It is an univariate imputation technique, this means that only uses values relative to the feature that is going to be imputed. It imputes the missing values with the mean of the feature. The mean is been chosen instead of the median because there are not many outliers that could affect the distribution of the data.\n",
    "- SECOND OPTION: KNN Imputer. It is a multivariate imputation technique, this means that it uses values from the feature that is going to be imputed but also from other features. This technique in partiular is based in the KNN algorithm and it consistsn on imputing the value os the missing category as a mean from the closest neighbors.\n",
    "- THIRD OPTION: Iterative Imputer. It is also a muktivarite technique. Is based on iterative models that compute the values for the missing categories.\n",
    "This strategy will be followed for the imputation strategy selection of the rest of the models. This is sections 4.3, 5.3 and 6.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Simple Imputer:  457.67268509394927\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION: Simple Imputer using the mean\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "simple_knn = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',KNeighborsRegressor())])\n",
    "\n",
    "simple_knn.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_simple_imputer = simple_knn.predict(X_inner_val_2)\n",
    "MSE_simple = mean_squared_error(y_inner_val,y_predicted_simple_imputer)\n",
    "\n",
    "print('RMSE for imputation with Simple Imputer: ', np.sqrt(MSE_simple))\n",
    "# 457.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with KNN Imputer:  428.12271404149146\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION: KNN Imputer with default hyperparameters\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "knn_knn = Pipeline([('imputer',KNNImputer()),('regression',KNeighborsRegressor())])\n",
    "\n",
    "knn_knn.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_knn_imputer = knn_knn.predict(X_inner_val_2)\n",
    "MSE_knn = mean_squared_error(y_inner_val,y_predicted_knn_imputer)\n",
    "\n",
    "print('RMSE for imputation with KNN Imputer: ', np.sqrt(MSE_knn))\n",
    "# 428.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the version of scikit-learn if it returns an Error with IterativeImputer\n",
    "!pip install --upgrade scikit-learn --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Iterative Imputer:  426.13631965182225\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# THIRD OPTION: multivariate technique. Iterative imputer with 10 maximum iterations. Include random seed for reproducibility\n",
    "iterative_knn = Pipeline([('imputer',IterativeImputer(max_iter = 10, random_state = rs)),('regression',KNeighborsRegressor())])\n",
    "\n",
    "iterative_knn.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_iterative_imputer = iterative_knn.predict(X_inner_val_2)\n",
    "MSE_iterative = mean_squared_error(y_inner_val,y_predicted_iterative_imputer)\n",
    "\n",
    "print('RMSE for imputation with Iterative Imputer: ', np.sqrt(MSE_iterative))\n",
    "# 426.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          | 1. Simple Imputer | 2. KNN Imputer | 3. Iterative Imputer|\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE | 457.67 | 428.12 | 426.14 |\n",
    "\n",
    "From the results above, it is demonstrated that the Simple Imputer is the worst in terms of performance with respect to the Mean Squared Error.\n",
    "\n",
    "On the other hand, the multivariate techniques, KNN Imputer and the Iterative Imputer give similar results. The Iterative Imputer gets a lower Mean Squared Error, so it will be the chosen technique. \n",
    "\n",
    "### 3.4. Hyperparameter tuning\n",
    "\n",
    "Once the preprocessing stages have been optimized and improved, it is time to optimize the model by performing hyperparameter tuning. Crossvalidation will be the stratefy for HPO. In the case of KNN algorithm, the main hyperparameter is the number of neighbors, this number of neighbors. \n",
    "\n",
    "Several approaches for hyperparameter tuning will be implemented. The optimization time will be measured and also the best combination of hyperparameters for each method will be compared. This additional study will provide insight in which HPO strategy is the most adequate in terms of execution time and also in terms of performance.\n",
    "\n",
    "For HPO, Pipelines will not be used, the preprocessing stages will be made previously in order to make the process faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# FILTER. Feature selection\n",
    "X_train_filtered = X_train.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# IMPUTER. Imputation of missing values\n",
    "imputer = IterativeImputer(max_iter=10, random_state=rs)\n",
    "X_train_imputed = imputer.fit_transform(X_train_filtered)\n",
    "# X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_filtered), columns=X_train_filtered.columns)\n",
    "\n",
    "\n",
    "# Preprocessing also for the test data for estimation of future performance later\n",
    "X_test_filtered = X_test.filter(regex='\\.13$', axis=1)\n",
    "X_test_imputed = imputer.fit_transform(X_test_filtered)\n",
    "# X_test_imputed_df = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL TO OPTIMIZE\n",
    "\n",
    "# Create the regressor with no hyperparamters defined\n",
    "knn_default = KNeighborsRegressor()\n",
    "\n",
    "# Create a dictionary with the values for the hyperparameters\n",
    "params_knn = {'n_neighbors': np.arange(1, 51)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params {'n_neighbors': 17}\n",
      "RANDOM SEARCH\n",
      "Best Hyperparameters for KNN: {'n_neighbors': 17}\n",
      "Execution time:  1.3825676441192627\n",
      "CV MSE Score:  391.7182984002424\n"
     ]
    }
   ],
   "source": [
    "# FIRST STRATEGY - Randomized Search\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import time\n",
    "\n",
    "# Create a randomized search instance\n",
    "random_search = RandomizedSearchCV(\n",
    "    knn_default,\n",
    "    param_distributions=params_knn,\n",
    "    n_iter=10,  # Number of iterations (can be adjusted)\n",
    "    cv=5,       # Number of cross-validation folds\n",
    "    scoring='neg_mean_squared_error',  # Mean squared error is the error metric\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "random_search.fit(X_train_imputed, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_random = random_search.best_params_\n",
    "best_knn_random = KNeighborsRegressor(**best_params_random) \n",
    "cv_scores = cross_val_score(best_knn_random, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_cv_rmse = np.mean(np.sqrt(-cv_scores))\n",
    "\n",
    "# Print results\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(\"Best Hyperparameters for KNN:\", best_params_random)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV MSE Score: \", mean_cv_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-02 22:21:08,395] A new study created in memory with name: no-name-8a0d8bd7-2d83-4334-b775-df635c20d044\n",
      "[I 2024-01-02 22:21:39,604] Trial 0 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 21}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 22:22:11,016] Trial 1 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 33}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 22:22:42,099] Trial 2 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 50}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 22:23:13,468] Trial 3 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 40}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 22:23:44,919] Trial 4 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 37}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 22:24:16,427] Trial 5 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 15}. Best is trial 0 with value: -170261.21944310627.\n",
      "[I 2024-01-02 22:24:47,734] Trial 6 finished with value: -170261.21944310627 and parameters: {'n_neighbors': 5}. Best is trial 0 with value: -170261.21944310627.\n",
      "[W 2024-01-02 22:24:51,042] Trial 7 failed with parameters: {'n_neighbors': 29} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_23276\\3447748276.py\", line 18, in objective_knn\n",
      "    inner_score = cross_val_score(iterative_knn, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 562, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 214, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 309, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 65, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 127, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 423, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 377, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 957, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 157, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py\", line 765, in fit_transform\n",
      "    Xt, estimator = self._impute_one_feature(\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py\", line 412, in _impute_one_feature\n",
      "    estimator.fit(X_train, y_train)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_bayes.py\", line 337, in fit\n",
      "    U, S, Vh = linalg.svd(X, full_matrices=False)\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_svd.py\", line 127, in svd\n",
      "    u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n",
      "KeyboardInterrupt\n",
      "[W 2024-01-02 22:24:51,057] Trial 7 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\3447748276.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mt_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mstudy_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective_knn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_trials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mt_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\study.py\u001b[0m in \u001b[0;36moptimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    449\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         \"\"\"\n\u001b[1;32m--> 451\u001b[1;33m         _optimize(\n\u001b[0m\u001b[0;32m    452\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             _optimize_sequential(\n\u001b[0m\u001b[0;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfrozen_trial\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[1;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     ):\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m             \u001b[0mvalue_or_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\3447748276.py\u001b[0m in \u001b[0;36mobjective_knn\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Define the neg means squared error as the score and the inner evaluation as corssvalidation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0minner_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterative_knn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_imputed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m     cv_results = cross_validate(\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    212\u001b[0m                     )\n\u001b[0;32m    213\u001b[0m                 ):\n\u001b[1;32m--> 214\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                 \u001b[1;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[1;31m# independent, and that it is pickle-able.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m     \u001b[0mparallel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m     results = parallel(\n\u001b[0m\u001b[0;32m    310\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    311\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1792\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m    422\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Pipeline\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"passthrough\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    375\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    378\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 957\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    958\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m         \u001b[0mdata_to_wrap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    763\u001b[0m                     \u001b[0mn_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mabs_corr_mat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m                 )\n\u001b[1;32m--> 765\u001b[1;33m                 Xt, estimator = self._impute_one_feature(\n\u001b[0m\u001b[0;32m    766\u001b[0m                     \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                     \u001b[0mmask_missing_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\impute\\_iterative.py\u001b[0m in \u001b[0;36m_impute_one_feature\u001b[1;34m(self, X_filled, mask_missing_values, feat_idx, neighbor_feat_idx, estimator, fit_mode)\u001b[0m\n\u001b[0;32m    410\u001b[0m                 \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m             )\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[1;31m# if no missing values, don't predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0mXT_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0meigen_vals_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;31m# perform decomposition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0m\u001b[0;32m    128\u001b[0m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# THIS STAGE TAKES LONG\n",
    "# SECOND STRATEGY - Optuna\n",
    "\n",
    "# Import required libraries\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective_knn(trial):\n",
    "    # Hyperparameters that are to be tuned\n",
    "    n_neighbors = trial.suggest_int('n_neighbors', 1, 51)\n",
    "    params = {'n_neighbors': n_neighbors}\n",
    "    \n",
    "    # Estimator with suggested hyperparameters\n",
    "    knn_default = KNeighborsRegressor()\n",
    "    \n",
    "    # Define the neg means squared error as the score and the inner evaluation as corssvalidation \n",
    "    inner_score = cross_val_score(iterative_knn, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "# Train the model and perform HPO\n",
    "# Measure the time\n",
    "sampler = optuna.samplers.TPESampler(seed=rs)\n",
    "study_knn = optuna.create_study(direction='maximize',sampler=sampler)\n",
    "\n",
    "iterations = 30\n",
    "t_0 = time.time()\n",
    "study_knn.optimize(objective_knn, n_trials = iterations)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params_optuna = study_knn.best_params\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_knn_optuna = KNeighborsRegressor(**best_params_optuna) \n",
    "cv_scores_optuna = cross_val_score(best_knn_optuna, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_cv_rmse_optuna = np.mean(np.sqrt(-cv_scores_optuna))\n",
    "\n",
    "# Print results\n",
    "print(\"OPTUNA HPO\")\n",
    "print(\"Best Hyperparameters for KNN:\", best_params_optuna)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV RMSE Score: \", mean_cv_rmse_optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 6\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 118\n",
      "max_resources_: 3798\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 50\n",
      "n_resources: 118\n",
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 25\n",
      "n_resources: 236\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 13\n",
      "n_resources: 472\n",
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 7\n",
      "n_resources: 944\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 4\n",
      "n_resources: 1888\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 2\n",
      "n_resources: 3776\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "SUCCESIVE HALVING\n",
      "Best Hyperparameters for KNN: {'n_neighbors': 11}\n",
      "Execution time:  1.9224305152893066\n",
      "CV MSE Score:  393.42255429468116\n"
     ]
    }
   ],
   "source": [
    "# THIRD STRATEGY - Successive halving\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "\n",
    "# Create\n",
    "halving = HalvingGridSearchCV(knn_default,\n",
    "                              params_knn,\n",
    "                              scoring= 'neg_mean_squared_error',\n",
    "                              cv=5,\n",
    "                              random_state=rs,\n",
    "                              factor=2,\n",
    "                              min_resources='exhaust',\n",
    "                              max_resources='auto',\n",
    "                              n_jobs=-1, verbose=1)\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "halving.fit(X_train_imputed, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_halving = halving.best_params_\n",
    "best_knn_halving = KNeighborsRegressor(**best_params_halving) \n",
    "cv_scores_halving = cross_val_score(best_knn_halving, X_train_imputed, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "mean_cv_rmse_halving = np.mean(np.sqrt(-cv_scores_halving))\n",
    "\n",
    "# Print results\n",
    "print(\"SUCCESIVE HALVING\")\n",
    "print(\"Best Hyperparameters for KNN:\", best_params_halving)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV MSE Score: \", mean_cv_rmse_halving)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|          | 1. Random Search | 2. OPTUNA | 3. Successive Halving |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE (crossvalidation) | 391.71 | 391.52 | 393.42 |\n",
    "| n_neighbors | 17 | 21 | 11 |\n",
    "| Time (s) | 1.38 | 532.77 | 1.92 |\n",
    "\n",
    "The results from the HPO show that the number of neighbors does not seem to affect the performance when it changes from 17 to 21, as the crossvalidation scores are the same. But, it also shows that the successive halving in this case performs slightly worse than the other strategies.\n",
    "\n",
    "An important note to take into account is that the errors that are measured for this section do not provide real insight on the errors that will occur in future performance. This happens because these errors have been computed with the same set that was used during the training stage. Anyway, as these errors have been measured in the same way for the three strategies, it can be a measure used to determine which strategy is better. Then, the test set will be used to estimate future performance in an unbiased way.\n",
    "\n",
    "The final decission for the number of neighbors is 17, as it is one of the options that provide the best result and requires of considering a lower number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Conclusions and future performance\n",
    "\n",
    "Out of all the trials, the KNN model has demonstrated to work better with the following adjustments:\n",
    "- **Best imputation technique**: Iterative Imputer\n",
    "- **Best feature selection strategy**: selecting the variables related to wind characteristics.\n",
    "- **Best hyperparameters**:\n",
    " - Best number of neighbors: 21\n",
    " \n",
    "Now, to estimate the future performance of this model, the test set that has been previously set aside will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the optimized KNN model:  407.98965946652226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Define optimal hyperparameters\n",
    "opt_k = 17\n",
    "\n",
    "# Custom function for filtering\n",
    "def filter_knn(data):\n",
    "    return data.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# Define the pipeline\n",
    "best_knn = Pipeline([\n",
    "    ('filter', FunctionTransformer(filter_knn, validate=False)),\n",
    "    ('imputer',IterativeImputer(max_iter = 10, random_state = rs)),\n",
    "    ('regression',KNeighborsRegressor(n_neighbors = opt_k))])\n",
    "\n",
    "# Fit training data\n",
    "best_knn.fit(X_train,y_train)\n",
    "\n",
    "# Predict test data\n",
    "y_pred_best = best_knn.predict(X_test)\n",
    "MSE_best_knn = mean_squared_error(y_test,y_pred_best)\n",
    "\n",
    "print('RMSE for the optimized KNN model: ', np.sqrt(MSE_best_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Training the KNN model with the whole training set\n",
    "\n",
    "Now that feature selection, imputation and hyperparameters have been optimized, the final model is trained using the whole wind_ava dataset. A pipeline with all the optimized stages will be done.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Tree\n",
    "\n",
    "### 4.1. First model\n",
    "\n",
    "A first model using the Decision Tree algorithm will be implemented. It will use the default hyperparameters, the KNN Imputer as imputation strategy and no feature selection. This model will be updated and improved but it is just a simple approach to later compare the adjusted models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the first KNN model:  650.972879219549\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create the first Decission Tree model\n",
    "first_tree = Pipeline([('imputer',KNNImputer()),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "first_tree.fit(X_inner_train,y_inner_train)\n",
    "y_predicted = first_tree.predict(X_inner_val)\n",
    "MSE_tree = mean_squared_error(y_inner_val,y_predicted_first)\n",
    "\n",
    "print('RMSE for the first KNN model: ', np.sqrt(MSE_tree))\n",
    "# 650.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Feature selection\n",
    "\n",
    "Same procedure as in section 3.2. but in this case using Decission Tree Regressor model. The variable selection will not be made again in this section, the filtered datasets are defined in section 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  522.4285433416003\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "\n",
    "# The first Decission Tree model that has been already created is trained now with the selected data\n",
    "first_tree.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_1 = first_tree.predict(X_inner_val_1)\n",
    "MSE_1 = mean_squared_error(y_inner_val,y_predicted_1)\n",
    "\n",
    "print('RMSE for feature selection with variables from Sotavento: ', np.sqrt(MSE_1))\n",
    "# 522.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  541.6336936084882\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "# The first Decission Tree model that has been already created is trained now with the selected data\n",
    "first_tree.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_2 = first_tree.predict(X_inner_val_2)\n",
    "MSE_2 = mean_squared_error(y_inner_val,y_predicted_2)\n",
    "\n",
    "print('RMSE for feature selection with variables related to the wind: ', np.sqrt(MSE_2))\n",
    "# 541.63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with variables from Sotavento related to the wind:  543.9981445138152\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_tree.fit(X_inner_train_3,y_inner_train)\n",
    "y_predicted_3 = first_tree.predict(X_inner_val_3)\n",
    "MSE_3 = mean_squared_error(y_inner_val,y_predicted_3)\n",
    "\n",
    "print('RMSE for imputation with variables from Sotavento related to the wind: ', np.sqrt(MSE_3))\n",
    "# 544.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different feature selection strategies are included in the following table. These results show that the feature selection strategy that works better with Decission Trees is the first strategy, this is, choosing the variables related to Sotavneto only.\n",
    "\n",
    "|          | 0. No feature selection | 1. Sotavento features | 2. Wind features | 2. Wind features in Sotavento |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|:----------:|\n",
    "| RMSE |  650.97 | 522.42 | 541.63 | 543.99 |\n",
    "\n",
    "\n",
    "From this moment, the KNN model will be built using only the features that were measured in Sotavento.\n",
    "\n",
    "The best RMSE achieved until this point is: 522.42\n",
    "\n",
    "### 4.3. Imputation techniques\n",
    "\n",
    "Same procedure as in section 3.3. but in this case using Decission Tree Regressor model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Simple Imputer:  546.3131100360276\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION: Simple Imputer using the mean\n",
    "simple_tree = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "\n",
    "simple_tree.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_simple_imputer = simple_tree.predict(X_inner_val_1)\n",
    "MSE_simple = mean_squared_error(y_inner_val,y_predicted_simple_imputer)\n",
    "\n",
    "print('RMSE for imputation with Simple Imputer: ', np.sqrt(MSE_simple))\n",
    "# 546.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with KNN Imputer:  522.4285433416003\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION: KNN Imputer with default hyperparameters\n",
    "knn_tree = Pipeline([('imputer',KNNImputer()),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "\n",
    "knn_tree.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_knn_imputer = knn_tree.predict(X_inner_val_1)\n",
    "MSE_knn = mean_squared_error(y_inner_val,y_predicted_knn_imputer)\n",
    "\n",
    "print('RMSE for imputation with KNN Imputer: ', np.sqrt(MSE_knn))\n",
    "# 522.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Iterative Imputer:  527.2567305971542\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION: Iterative Imputer with 10 maximum iterations. Include random seed for reproducibility\n",
    "iterative_tree = Pipeline([('imputer',IterativeImputer(max_iter = 10, random_state = rs)),('regression',DecisionTreeRegressor(random_state=rs))])\n",
    "\n",
    "iterative_tree.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_iterative_imputer = iterative_tree.predict(X_inner_val_1)\n",
    "MSE_iterative = mean_squared_error(y_inner_val,y_predicted_iterative_imputer)\n",
    "\n",
    "print('RMSE for imputation with Iterative Imputer: ', np.sqrt(MSE_iterative))\n",
    "# 527.26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different imputation techniques are included in the following table. \n",
    "\n",
    "|          | 1. Simple Imputer | 2. KNN Imputer | 3. Iterative Imputer | \n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE |  546.31 | 522.43 | 527.26 |\n",
    "\n",
    "Results from the table show that the best imputation technique for Decission Trees in this case is the KNN Imputer with default hyperparameters. From this point, for the Decission Tree model, KNN Imputer will be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Hyperparameter tuning\n",
    "\n",
    "Once the imputation and feature selection have been performed, it is time to improve the performance of the model with HPO. Hyperparameter tuning will be carried out using crossvalidation as the inner evaluation method. For the Decission Trees the hyperparameters that will be adjusted will be:\n",
    "- this\n",
    "- this other one\n",
    "The same three strategies used in section 3.4. will be repeated in this section but with Decission Tree as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# FILTER. Feature selection\n",
    "X_train_filtered_tree = X_train.filter(regex='\\.13$', axis=1)\n",
    "\n",
    "# IMPUTER. Imputation of missing values\n",
    "imputer_tree = KNNImputer()\n",
    "X_train_imputed_tree = imputer_tree.fit_transform(X_train_filtered_tree)\n",
    "\n",
    "# Preprocessing also for the test data for estimation of future performance later\n",
    "X_test_filtered_tree = X_test.filter(regex='\\.13$', axis=1)\n",
    "X_test_imputed_tree = imputer_tree.fit_transform(X_test_filtered_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL TO OPTIMIZE\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Create the regressor with no hyperparamters defined\n",
    "tree_default = DecisionTreeRegressor()\n",
    "\n",
    "# Create a dictionary with the values for the hyperparameters\n",
    "# Define the grid with all possible values for the hyperparameters\n",
    "# Define the number of folds and the conditions for the crossvalidation\n",
    "params_tree = {'max_depth': list(range(2,16,2)), 'min_samples_split':list(range(2,16,2))}\n",
    "k_folds = KFold(n_splits = 5, shuffle = True, random_state =rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM SEARCH\n",
      "Best Hyperparameters for Decission Tree: {'min_samples_split': 2, 'max_depth': 6}\n",
      "Execution time:  7.918375253677368\n",
      "CV mean MSE Score:  197099.39835690625\n"
     ]
    }
   ],
   "source": [
    "# FIRST STRATEGY - Randomized Search\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import time\n",
    "\n",
    "# Create a randomized search instance\n",
    "random_search_tree = RandomizedSearchCV(\n",
    "    tree_default,\n",
    "    param_distributions=params_tree,\n",
    "    n_iter=10,  # Number of iterations (can be adjusted)\n",
    "    cv=5,       # Number of cross-validation folds\n",
    "    scoring='neg_mean_squared_error',  # Mean squared error is the error metric\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "random_search_tree.fit(X_train_imputed_tree, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_random_tree = random_search_tree.best_params_\n",
    "best_tree_random = DecisionTreeRegressor(**best_params_random_tree) \n",
    "cv_scores_random_tree = cross_val_score(best_tree_random, X_train_imputed_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(\"Best Hyperparameters for Decission Tree:\", best_params_random_tree)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV mean MSE Score: \", np.mean(-cv_scores_random_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-02 22:31:06,079] A new study created in memory with name: no-name-e15e84a8-de65-4c10-8dcd-972d1de0bec5\n",
      "[I 2024-01-02 22:31:11,314] Trial 0 finished with value: -188000.55722114054 and parameters: {'max_depth': 8, 'min_samples_split': 11}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:19,269] Trial 1 finished with value: -227237.68333928063 and parameters: {'max_depth': 16, 'min_samples_split': 13}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:26,488] Trial 2 finished with value: -230969.0722119074 and parameters: {'max_depth': 12, 'min_samples_split': 6}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:28,608] Trial 3 finished with value: -196994.78752022458 and parameters: {'max_depth': 3, 'min_samples_split': 10}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:36,980] Trial 4 finished with value: -252112.08852993255 and parameters: {'max_depth': 14, 'min_samples_split': 2}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:42,849] Trial 5 finished with value: -191418.75260763027 and parameters: {'max_depth': 9, 'min_samples_split': 16}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:44,983] Trial 6 finished with value: -196994.78752022458 and parameters: {'max_depth': 3, 'min_samples_split': 2}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:31:53,160] Trial 7 finished with value: -248800.94816601992 and parameters: {'max_depth': 15, 'min_samples_split': 6}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:32:00,508] Trial 8 finished with value: -223291.12512823864 and parameters: {'max_depth': 13, 'min_samples_split': 11}. Best is trial 0 with value: -188000.55722114054.\n",
      "[I 2024-01-02 22:32:03,286] Trial 9 finished with value: -177332.90310518988 and parameters: {'max_depth': 4, 'min_samples_split': 14}. Best is trial 9 with value: -177332.90310518988.\n",
      "[I 2024-01-02 22:32:07,325] Trial 10 finished with value: -166462.95517674088 and parameters: {'max_depth': 6, 'min_samples_split': 16}. Best is trial 10 with value: -166462.95517674088.\n",
      "[I 2024-01-02 22:32:10,776] Trial 11 finished with value: -164970.92525860431 and parameters: {'max_depth': 5, 'min_samples_split': 16}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:14,842] Trial 12 finished with value: -166462.95517674088 and parameters: {'max_depth': 6, 'min_samples_split': 16}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:18,933] Trial 13 finished with value: -167729.2418917316 and parameters: {'max_depth': 6, 'min_samples_split': 14}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:23,055] Trial 14 finished with value: -166462.95517674088 and parameters: {'max_depth': 6, 'min_samples_split': 16}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:29,940] Trial 15 finished with value: -220206.73991786418 and parameters: {'max_depth': 11, 'min_samples_split': 8}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:33,396] Trial 16 finished with value: -164970.92525860431 and parameters: {'max_depth': 5, 'min_samples_split': 13}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:34,854] Trial 17 finished with value: -249944.2448512675 and parameters: {'max_depth': 2, 'min_samples_split': 13}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:40,629] Trial 18 finished with value: -194722.70408081438 and parameters: {'max_depth': 9, 'min_samples_split': 12}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:43,423] Trial 19 finished with value: -178239.23871610392 and parameters: {'max_depth': 4, 'min_samples_split': 8}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:48,296] Trial 20 finished with value: -181329.09556918757 and parameters: {'max_depth': 7, 'min_samples_split': 14}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:51,794] Trial 21 finished with value: -164970.92525860431 and parameters: {'max_depth': 5, 'min_samples_split': 15}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:54,620] Trial 22 finished with value: -177332.90310518988 and parameters: {'max_depth': 4, 'min_samples_split': 15}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:58,188] Trial 23 finished with value: -164970.92525860431 and parameters: {'max_depth': 5, 'min_samples_split': 13}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:32:59,640] Trial 24 finished with value: -249944.2448512675 and parameters: {'max_depth': 2, 'min_samples_split': 15}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:33:04,909] Trial 25 finished with value: -187117.88225127748 and parameters: {'max_depth': 8, 'min_samples_split': 12}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:33:11,094] Trial 26 finished with value: -201036.8343617807 and parameters: {'max_depth': 10, 'min_samples_split': 15}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:33:14,560] Trial 27 finished with value: -164970.92525860431 and parameters: {'max_depth': 5, 'min_samples_split': 14}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:33:19,367] Trial 28 finished with value: -180837.1883359169 and parameters: {'max_depth': 7, 'min_samples_split': 12}. Best is trial 11 with value: -164970.92525860431.\n",
      "[I 2024-01-02 22:33:24,644] Trial 29 finished with value: -188000.55722114054 and parameters: {'max_depth': 8, 'min_samples_split': 11}. Best is trial 11 with value: -164970.92525860431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTUNA HPO\n",
      "Best Hyperparameters for KNN: {'max_depth': 5, 'min_samples_split': 16}\n",
      "Execution time:  138.56486415863037\n",
      "CV RMSE Score:  165291.34409779933\n"
     ]
    }
   ],
   "source": [
    "# THIS STAGE TAKES LONG\n",
    "# SECOND STRATEGY - Optuna\n",
    "\n",
    "# Import required libraries\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective_tree(trial):\n",
    "    # Hyperparameters that are going to be tuned\n",
    "    max_depth = trial.suggest_int('max_depth',2,16)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split',2,16)\n",
    "    \n",
    "    # Estimator with suggested hyperparameters\n",
    "    params_tree = {'max_depth': max_depth, 'min_samples_split':min_samples_split}\n",
    "    regr_tree = DecisionTreeRegressor(random_state = rs, **params_tree)\n",
    "    \n",
    "    # Define the neg means squared error as the score and the inner evaluation as corssvalidation \n",
    "    inner_score = cross_val_score(regr_tree, X_train_imputed_tree, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "# Train the model and perform HPO\n",
    "# Measure the time\n",
    "sampler = optuna.samplers.TPESampler(seed=rs)\n",
    "study_tree = optuna.create_study(direction='maximize',sampler=sampler)\n",
    "\n",
    "iterations = 30\n",
    "t_0 = time.time()\n",
    "study_tree.optimize(objective_tree, n_trials = iterations)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params_tree_optuna = study_tree.best_params\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_tree_optuna = DecisionTreeRegressor(**best_params_tree_optuna) \n",
    "cv_scores_optuna_tree = cross_val_score(best_tree_optuna, X_train_imputed_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"OPTUNA HPO\")\n",
    "print(\"Best Hyperparameters for Decission Tree:\", best_params_tree_optuna)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV mean MSE Score: \", np.mean(-cv_scores_optuna_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 6\n",
      "n_required_iterations: 6\n",
      "n_possible_iterations: 6\n",
      "min_resources_: 118\n",
      "max_resources_: 3798\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 49\n",
      "n_resources: 118\n",
      "Fitting 5 folds for each of 49 candidates, totalling 245 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 25\n",
      "n_resources: 236\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 13\n",
      "n_resources: 472\n",
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 7\n",
      "n_resources: 944\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 4\n",
      "n_resources: 1888\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 2\n",
      "n_resources: 3776\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "SUCCESIVE HALVING\n",
      "Best Hyperparameters for KNN: {'max_depth': 4, 'min_samples_split': 14}\n",
      "Execution time:  10.173054218292236\n",
      "CV MSE Score:  177251.27479066368\n"
     ]
    }
   ],
   "source": [
    "# THIRD STRATEGY - Successive halving\n",
    "\n",
    "# Import required libraries\n",
    "# Create\n",
    "halving_tree = HalvingGridSearchCV(tree_default,\n",
    "                              params_tree,\n",
    "                              scoring= 'neg_mean_squared_error',\n",
    "                              cv=5,\n",
    "                              random_state=rs,\n",
    "                              factor=2,\n",
    "                              min_resources='exhaust',\n",
    "                              max_resources='auto',\n",
    "                              n_jobs=-1, verbose=1)\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "halving_tree.fit(X_train_imputed_tree, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_halving_tree = halving_tree.best_params_\n",
    "best_knn_halving = DecisionTreeRegressor(**best_params_halving_tree) \n",
    "cv_scores_halving = cross_val_score(best_knn_halving, X_train_imputed_tree, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"SUCCESIVE HALVING\")\n",
    "print(\"Best Hyperparameters for KNN:\", best_params_halving_tree)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV MSE Score: \", np.mean(-cv_scores_halving))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from this hyperparameter tuning are included in the following table:\n",
    "\n",
    "|          | 1. Random Search | 2. OPTUNA | 3. Successive Halving |\n",
    "|:-----------:|:----------------------:|:---------------------:|:----------:|\n",
    "| MSE (crossvalidation) | 197099.40 | 165291.34 | 177251.27 |\n",
    "| max_depth | 4 | 5 | 4 |\n",
    "| min_samples_split | 2 | 16 | 14 |\n",
    "| Time (s) | 7.92 | 138.56 | 10.17 |\n",
    "\n",
    "The method that minimizes the crossvalidation MSE is the OPTUNA, but it is also the method that takes longer time. The Random Search is the method that gets the worse result. Successive halving obtains a low error, close to the Optuna error, but takes about 13 times less time than OPTUNA, so if time was an important variable to consider, then Successive Halving will be the best strategy for this case.\n",
    "As all the three methods have been already implemented, and the time is already spent, the hyperparameters that the OPTUNA returns are the ones that will be used. So in conclussion, max_depth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Conclusions and future performance\n",
    "\n",
    "Out of all the trials, the Decision Tree model has demonstrated to work better with the following adjustments:\n",
    "- **Best imputation technique**: KNN Imputer\n",
    "- **Best feature selection strategy**: selecting the features measured in Sotavento.\n",
    "- **Best hyperparameters**:\n",
    " - Maximum depth: 5\n",
    " - Minimum number of samples to split: 16\n",
    " \n",
    "Now, to estimate the future performance of this model, the test set that has been previously set aside will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the optimized Decision Tree model:  437.8210577221916\n"
     ]
    }
   ],
   "source": [
    "# Define optimal hyperparameters\n",
    "max_depth_opt = 5\n",
    "min_samples_split_opt = 16\n",
    "\n",
    "# Custom function for filtering\n",
    "def filter_tree(data):\n",
    "    return data.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# Define the pipeline\n",
    "best_tree = Pipeline([\n",
    "    ('filter', FunctionTransformer(filter_tree, validate=False)),\n",
    "    ('imputer',KNNImputer()),\n",
    "    ('regression',DecisionTreeRegressor(max_depth = max_depth_opt, min_samples_split = min_samples_split_opt))])\n",
    "\n",
    "# Fit training data\n",
    "best_tree.fit(X_train,y_train)\n",
    "\n",
    "# Predict test data\n",
    "y_pred_best_tree = best_tree.predict(X_test)\n",
    "MSE_best_tree = mean_squared_error(y_test,y_pred_best_tree)\n",
    "\n",
    "print('RMSE for the optimized Decision Tree model: ', np.sqrt(MSE_best_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest Regressor\n",
    "\n",
    "### 5.1 First model\n",
    "\n",
    "A first model using an ensemble of Randomized Decission Trees has been created: a Random Forest. This has been done using the default hyperparameters for the Random Forest, no feature selection and KNN algorithm for imputation of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for the first Random Forest model:  392.6583071198578\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create the first Random Forest model\n",
    "first_forest = Pipeline([('imputer',KNNImputer()),('regression',RandomForestRegressor(random_state=rs))])\n",
    "first_forest.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_forest = first_forest.predict(X_inner_val)\n",
    "MSE_forest = mean_squared_error(y_inner_val,y_predicted_forest)\n",
    "\n",
    "print('RMSE for the first Random Forest model: ', np.sqrt(MSE_forest))\n",
    "# 392.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature selection\n",
    "\n",
    "Same procedure as in section 3.2. but in this case using Decission Tree Regressor model. The variable selection will not be made again in this section, the filtered datasets are defined in section 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  394.963445594895\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "\n",
    "# The first Random Forest model that has been already created is trained now with the selected data\n",
    "first_forest.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_1_forest = first_forest.predict(X_inner_val_1)\n",
    "MSE_1_forest = mean_squared_error(y_inner_val,y_predicted_1_forest)\n",
    "\n",
    "print('RMSE for feature selection with variables from Sotavento: ', np.sqrt(MSE_1_forest))\n",
    "# 394.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for feature selection with variables from Sotavento:  399.6697849774181\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "# The first Random Forest model that has been already created is trained now with the selected data\n",
    "first_forest.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_2_forest = first_forest.predict(X_inner_val_2)\n",
    "MSE_2_forest = mean_squared_error(y_inner_val,y_predicted_2_forest)\n",
    "\n",
    "print('RMSE for feature selection with variables related to the wind: ', np.sqrt(MSE_2_forest))\n",
    "# 399.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with variables from Sotavento related to the wind:  431.08135954944345\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "# The first model that has been already created is trained now with the selected data\n",
    "first_forest.fit(X_inner_train_3,y_inner_train)\n",
    "y_predicted_3_forest = first_forest.predict(X_inner_val_3)\n",
    "MSE_3_forest = mean_squared_error(y_inner_val,y_predicted_3_forest)\n",
    "\n",
    "print('RMSE for imputation with variables from Sotavento related to the wind: ', np.sqrt(MSE_3_forest))\n",
    "# 431.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from feature selection with Random Forest model are included in the following table. \n",
    "\n",
    "|          | 0. No feature selection | 1. Sotavento features | 2. Wind features | 2. Wind features in Sotavento |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|:----------:|\n",
    "| RMSE |  392.66 | 394.96 | 399.67 | 431.08 |\n",
    "\n",
    "In this case, feature selection does not reduce the error obtained with the model. So, for the model created with Random Forest, no feature selection will be done in order to obtain the best performance possible.\n",
    "\n",
    "### 5.3. Imputation techniques\n",
    "\n",
    "Same procedure as in section 3.3. but in this case using Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Simple Imputer:  389.15362367205336\n"
     ]
    }
   ],
   "source": [
    "# FIRST OPTION: Simple Imputer using the mean\n",
    "simple_forest = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',RandomForestRegressor(random_state=rs))])\n",
    "\n",
    "simple_forest.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_simple_imputer_forest = simple_forest.predict(X_inner_val)\n",
    "MSE_simple_forest = mean_squared_error(y_inner_val,y_predicted_simple_imputer_forest)\n",
    "\n",
    "print('RMSE for imputation with Simple Imputer: ', np.sqrt(MSE_simple_forest))\n",
    "# 389.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with KNN Imputer:  392.6583071198578\n"
     ]
    }
   ],
   "source": [
    "# SECOND OPTION: KNN Imputer with default hyperparameters\n",
    "knn_forest = Pipeline([('imputer',KNNImputer()),('regression',RandomForestRegressor(random_state=rs))])\n",
    "\n",
    "knn_forest.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_knn_imputer_forest = knn_forest.predict(X_inner_val)\n",
    "MSE_knn_forest = mean_squared_error(y_inner_val,y_predicted_knn_imputer_forest)\n",
    "\n",
    "print('RMSE for imputation with KNN Imputer: ', np.sqrt(MSE_knn_forest))\n",
    "# 392.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for imputation with Iterative Imputer:  383.04874604998815\n"
     ]
    }
   ],
   "source": [
    "# THIRD OPTION: Iterative Imputer with 10 maximum iterations. Include random seed for reproducibility\n",
    "iterative_forest = Pipeline([('imputer',IterativeImputer(max_iter = 10, random_state = rs)),('regression',RandomForestRegressor(random_state=rs))])\n",
    "\n",
    "iterative_forest.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_iterative_imputer_forest = iterative_forest.predict(X_inner_val)\n",
    "MSE_iterative_forest = mean_squared_error(y_inner_val,y_predicted_iterative_imputer_forest)\n",
    "\n",
    "print('RMSE for imputation with Iterative Imputer: ', np.sqrt(MSE_iterative_forest))\n",
    "# 383.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different imputation techniques are included in the following table. \n",
    "\n",
    "|          | 1. Simple Imputer | 2. KNN Imputer | 3. Iterative Imputer | \n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE |   389.15 | 392.66 | 383.05 |\n",
    "\n",
    "Results from the table show that the best imputation technique for Decission Trees in this case is the Iterative Imputer. One interesting appreciation is that, contratry to the previous cases, the Simple Imputer provides a quite good result, \n",
    "From this point, for the Random Forest model, Iterative Imputer will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Hyperparameter tuning\n",
    "\n",
    "Once the imputation and feature selection have been performed, it is time to improve the performance of the model with HPO. Hyperparameter tuning will be carried out using crossvalidation as the inner evaluation method. In the case of Random Forest, the hyperparameters that will be optimized are:\n",
    "\n",
    "- n_estimators: number of estimators, number of Random Decission Trees that form the ensemble.\n",
    "- max_depth: maximum depth of the trees that form the ensemble (same as for Decission Trees).\n",
    "- min_samples_split: minimum number of samples required to split an internal node (same as for Decission Trees).\n",
    "\n",
    "The same three strategies used in section 3.4. will be repeated in this section but with Decission Tree as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "# NO FEATURE SELECTION. For Random Forest, feature selection does not improve the performance\n",
    "\n",
    "# IMPUTER. Imputation of missing values\n",
    "imputer_forest = IterativeImputer(max_iter = 10, random_state = rs)\n",
    "X_train_imputed_forest = imputer_forest.fit_transform(X_train)\n",
    "\n",
    "# Preprocessing also for the test data for estimation of future performance later\n",
    "X_test_imputed_forest = imputer_forest.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL TO OPTIMIZE\n",
    "\n",
    "# Create the regressor with no hyperparamters defined\n",
    "forest_default = RandomForestRegressor()\n",
    "\n",
    "# Create a dictionary with the values for the hyperparameters\n",
    "# Define the grid with all possible values for the hyperparameters\n",
    "# Define the number of folds and the conditions for the crossvalidation\n",
    "params_forest = {'max_depth': list(range(2,16,2)), 'min_samples_split':list(range(2,16,2)), 'n_estimators':list(range(20,200,20))}\n",
    "k_folds = KFold(n_splits = 5, shuffle = True, random_state =rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23276\\2008301110.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Train the model, optimize the hyperparameters and measure the time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mt_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mrandom_search_forest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_imputed_forest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mt_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    896\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1807\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1808\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1809\u001b[1;33m         evaluate_candidates(\n\u001b[0m\u001b[0;32m   1810\u001b[0m             ParameterSampler(\n\u001b[0;32m   1811\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    843\u001b[0m                     )\n\u001b[0;32m    844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 845\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    846\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1792\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    454\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m             trees = Parallel(\n\u001b[0m\u001b[0;32m    457\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         )\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1863\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1865\u001b[0m         \u001b[1;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1792\u001b[1;33m                 \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1150\u001b[0m                 )\n\u001b[0;32m   1151\u001b[0m             ):\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1318\u001b[0m         \"\"\"\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1320\u001b[1;33m         super()._fit(\n\u001b[0m\u001b[0;32m   1321\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    441\u001b[0m             )\n\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FIRST STRATEGY - Randomized Search\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "import time\n",
    "\n",
    "# Create a randomized search instance\n",
    "random_search_forest = RandomizedSearchCV(\n",
    "    forest_default,\n",
    "    param_distributions=params_forest,\n",
    "    n_iter=10,  # Number of iterations (can be adjusted)\n",
    "    cv=5,       # Number of cross-validation folds\n",
    "    scoring='neg_mean_squared_error',  # Mean squared error is the error metric\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "random_search_forest.fit(X_train_imputed_forest, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_random_forest = random_search_forest.best_params_\n",
    "best_forest_random = RandomForestRegressor(**best_params_random_forest) \n",
    "cv_scores_random_forest = cross_val_score(best_forest_random, X_train_imputed_forest, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"RANDOM SEARCH\")\n",
    "print(\"Best Hyperparameters for Random Forest:\", best_params_random_forest)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV mean MSE Score: \", np.mean(-cv_scores_random_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-03 02:08:44,425] A new study created in memory with name: no-name-1f840c63-370e-46ff-a190-4a15cbc76b46\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:09:45,903] Trial 0 finished with value: -142234.49571508775 and parameters: {'max_depth': 8, 'min_samples_split': 11}. Best is trial 0 with value: -142234.49571508775.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:11:34,316] Trial 1 finished with value: -143610.2693869997 and parameters: {'max_depth': 16, 'min_samples_split': 13}. Best is trial 0 with value: -142234.49571508775.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:12:20,448] Trial 2 finished with value: -151918.5612170666 and parameters: {'max_depth': 12, 'min_samples_split': 6}. Best is trial 0 with value: -142234.49571508775.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:12:42,784] Trial 3 finished with value: -182035.25796472823 and parameters: {'max_depth': 3, 'min_samples_split': 10}. Best is trial 0 with value: -142234.49571508775.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:13:00,757] Trial 4 finished with value: -199745.793094099 and parameters: {'max_depth': 14, 'min_samples_split': 2}. Best is trial 0 with value: -142234.49571508775.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:14:36,947] Trial 5 finished with value: -140598.0502941825 and parameters: {'max_depth': 9, 'min_samples_split': 16}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:14:41,444] Trial 6 finished with value: -189767.28098617715 and parameters: {'max_depth': 3, 'min_samples_split': 2}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:15:33,696] Trial 7 finished with value: -153735.27291460495 and parameters: {'max_depth': 15, 'min_samples_split': 6}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:16:58,677] Trial 8 finished with value: -144164.07483098967 and parameters: {'max_depth': 13, 'min_samples_split': 11}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:17:39,800] Trial 9 finished with value: -158431.14113917662 and parameters: {'max_depth': 4, 'min_samples_split': 14}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:18:58,530] Trial 10 finished with value: -141473.26348595327 and parameters: {'max_depth': 7, 'min_samples_split': 16}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:20:26,443] Trial 11 finished with value: -140996.57372325135 and parameters: {'max_depth': 8, 'min_samples_split': 16}. Best is trial 5 with value: -140598.0502941825.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:22:09,875] Trial 12 finished with value: -140739.281593924 and parameters: {'max_depth': 10, 'min_samples_split': 16}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:23:46,652] Trial 13 finished with value: -142600.24342950777 and parameters: {'max_depth': 11, 'min_samples_split': 14}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:25:29,845] Trial 14 finished with value: -140739.281593924 and parameters: {'max_depth': 10, 'min_samples_split': 16}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:25:59,454] Trial 15 finished with value: -149271.01238528776 and parameters: {'max_depth': 5, 'min_samples_split': 8}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:26:55,790] Trial 16 finished with value: -142448.76761958757 and parameters: {'max_depth': 6, 'min_samples_split': 13}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:28:26,962] Trial 17 finished with value: -142328.54319335712 and parameters: {'max_depth': 10, 'min_samples_split': 14}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:29:16,626] Trial 18 finished with value: -144726.34875952144 and parameters: {'max_depth': 9, 'min_samples_split': 8}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:30:45,379] Trial 19 finished with value: -143206.9902630897 and parameters: {'max_depth': 12, 'min_samples_split': 12}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:31:59,523] Trial 20 finished with value: -141752.41923249658 and parameters: {'max_depth': 7, 'min_samples_split': 15}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:33:43,055] Trial 21 finished with value: -140739.281593924 and parameters: {'max_depth': 10, 'min_samples_split': 16}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:35:20,162] Trial 22 finished with value: -141379.915961248 and parameters: {'max_depth': 10, 'min_samples_split': 15}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:36:56,854] Trial 23 finished with value: -140598.0502941825 and parameters: {'max_depth': 9, 'min_samples_split': 16}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-03 02:38:09,275] Trial 24 finished with value: -141635.33556405365 and parameters: {'max_depth': 8, 'min_samples_split': 13}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:39:58,024] Trial 25 finished with value: -142550.8021030403 and parameters: {'max_depth': 12, 'min_samples_split': 15}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:41:11,458] Trial 26 finished with value: -142424.46637222116 and parameters: {'max_depth': 9, 'min_samples_split': 12}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:41:40,955] Trial 27 finished with value: -160682.20476678046 and parameters: {'max_depth': 11, 'min_samples_split': 4}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:42:45,835] Trial 28 finished with value: -142603.74994994607 and parameters: {'max_depth': 6, 'min_samples_split': 15}. Best is trial 5 with value: -140598.0502941825.\n",
      "C:\\Users\\Admin\\anaconda3\\lib\\site-packages\\optuna\\trial\\_trial.py:682: RuntimeWarning: Inconsistent parameter values for distribution with name \"min_samples_split\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more than once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'log': False, 'step': 1, 'low': 2, 'high': 16}\n",
      "  warnings.warn(\n",
      "[I 2024-01-03 02:43:40,873] Trial 29 finished with value: -142500.01491432707 and parameters: {'max_depth': 7, 'min_samples_split': 11}. Best is trial 5 with value: -140598.0502941825.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTUNA HPO\n",
      "Best Hyperparameters for Decission Tree: {'max_depth': 9, 'min_samples_split': 16}\n",
      "Execution time:  2096.4479999542236\n",
      "CV mean MSE Score:  136029.79964934284\n"
     ]
    }
   ],
   "source": [
    "# THIS STAGE TAKES LONG\n",
    "# SECOND STRATEGY - Optuna\n",
    "\n",
    "# Import required libraries\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "# Define the objective function\n",
    "def objective_forest(trial):\n",
    "    # Hyperparameters that are going to be tuned\n",
    "    max_depth = trial.suggest_int('max_depth',2,16)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split',2,16)\n",
    "    n_estimators = trial.suggest_int('min_samples_split',20,200)\n",
    "    \n",
    "    # Estimator with suggested hyperparameters\n",
    "    params_forest = {'max_depth': max_depth, 'min_samples_split':min_samples_split, 'n_estimators': n_estimators}\n",
    "    regr_forest = RandomForestRegressor(random_state = rs, **params_forest)\n",
    "    \n",
    "    # Define the neg means squared error as the score and the inner evaluation as corssvalidation \n",
    "    inner_score = cross_val_score(regr_forest, X_train_imputed_forest, y_train, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    return inner_score\n",
    "\n",
    "# Train the model and perform HPO\n",
    "# Measure the time\n",
    "sampler = optuna.samplers.TPESampler(seed=rs)\n",
    "study_forest = optuna.create_study(direction='maximize',sampler=sampler)\n",
    "\n",
    "iterations = 30\n",
    "t_0 = time.time()\n",
    "study_forest.optimize(objective_forest, n_trials = iterations)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params_forest_optuna = study_forest.best_params\n",
    "\n",
    "# Calculate the crossvalidation ewe3score for this model\n",
    "best_forest_optuna = RandomForestRegressor(**best_params_forest_optuna) \n",
    "cv_scores_optuna_forest = cross_val_score(best_forest_optuna, X_train_imputed_forest, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"OPTUNA HPO\")\n",
    "print(\"Best Hyperparameters for Random Forest:\", best_params_forest_optuna)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV mean MSE Score: \", np.mean(-cv_scores_optuna_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters\n",
    "best_params_forest_optuna = study_forest.best_params\n",
    "\n",
    "# Calculate the crossvalidation ewe3score for this model\n",
    "best_forest_optuna = RandomForestRegressor(**best_params_forest_optuna) \n",
    "cv_scores_optuna_forest = cross_val_score(best_forest_optuna, X_train_imputed_forest, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"OPTUNA HPO\")\n",
    "print(\"Best Hyperparameters for Random Forest:\", best_params_forest_optuna)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV mean MSE Score: \", np.mean(-cv_scores_optuna_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 9\n",
      "n_required_iterations: 9\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 14\n",
      "max_resources_: 3798\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 441\n",
      "n_resources: 14\n",
      "Fitting 5 folds for each of 441 candidates, totalling 2205 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 221\n",
      "n_resources: 28\n",
      "Fitting 5 folds for each of 221 candidates, totalling 1105 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 111\n",
      "n_resources: 56\n",
      "Fitting 5 folds for each of 111 candidates, totalling 555 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 56\n",
      "n_resources: 112\n",
      "Fitting 5 folds for each of 56 candidates, totalling 280 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 28\n",
      "n_resources: 224\n",
      "Fitting 5 folds for each of 28 candidates, totalling 140 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 14\n",
      "n_resources: 448\n",
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 7\n",
      "n_resources: 896\n",
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 4\n",
      "n_resources: 1792\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "----------\n",
      "iter: 8\n",
      "n_candidates: 2\n",
      "n_resources: 3584\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    }
   ],
   "source": [
    "# THIRD STRATEGY - Successive halving\n",
    "\n",
    "# Import required libraries\n",
    "# Create\n",
    "halving_forest = HalvingGridSearchCV(forest_default,\n",
    "                              params_forest,\n",
    "                              scoring= 'neg_mean_squared_error',\n",
    "                              cv=5,\n",
    "                              random_state=rs,\n",
    "                              factor=2,\n",
    "                              min_resources='exhaust',\n",
    "                              max_resources='auto',\n",
    "                              n_jobs=-1, verbose=1)\n",
    "\n",
    "# Train the model, optimize the hyperparameters and measure the time\n",
    "t_0 = time.time()\n",
    "halving_forest.fit(X_train_imputed_forest, y_train)\n",
    "t_1 = time.time()\n",
    "\n",
    "# Calculate the crossvalidation score for this model\n",
    "best_params_halving_forest = halving_forest.best_params_\n",
    "best_forest_halving = RandomForestRegressor(**best_params_halving_forest) \n",
    "cv_scores_halving_forest = cross_val_score(best_forest_halving, X_train_imputed_forest, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Print results\n",
    "print(\"SUCCESIVE HALVING\")\n",
    "print(\"Best Hyperparameters for Random Forest:\", best_params_halving_forest)\n",
    "print(\"Execution time: \",t_1-t_0)\n",
    "print(\"CV MSE Score: \", np.mean(-cv_scores_halving_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from this hyperparameter tuning are included in the following table:\n",
    "\n",
    "|          | 1. Random Search | 2. OPTUNA | 3. Successive Halving |\n",
    "|:-----------:|:----------------------:|:---------------------:|:----------:|\n",
    "| MSE (crossvalidation) | XXX | 136029.80 | XXX |\n",
    "| max_depth | XXX | XXX | XXX |\n",
    "| min_samples_split | XXX | XXX | XXX |\n",
    "| n_estimators | XXX | XXX | XXX |\n",
    "| Time (s) | XXX | 2096.44 | XXX |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Conclusions and future performance\n",
    "\n",
    "Out of all the trials, the Decision Tree model has demonstrated to work better with the following adjustments:\n",
    "- **Best imputation technique**: Iterative Imputer\n",
    "- **Best feature selection strategy**: no feature selection, using all available features.\n",
    "- **Best hyperparameters**:\n",
    " - Maximum depth: XXX\n",
    " - Minimum number of samples to split: XXX\n",
    " - Number of estimators: XXX\n",
    "\n",
    "Now, to estimate the future performance of this model, the test set that has been previously set aside will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimal hyperparameters\n",
    "max_depth_opt_forest = XXX\n",
    "min_samples_split_opt_forest = XXX\n",
    "n_estimators_opt_forest = XXX\n",
    "\n",
    "# Custom function for filtering\n",
    "def filter_tree(data):\n",
    "    return data.filter(regex='^(u|v).*$', axis=1)\n",
    "\n",
    "# Define the pipeline\n",
    "best_forest = Pipeline([\n",
    "    ('imputer',IterativeImputerIterativeImputer(max_iter = 10, random_state = rs)),\n",
    "    ('regression',RandomForestRegressor(max_depth = max_depth_opt_forest, min_samples_split = min_samples_split_opt_forest, n_estimators = n_estimators_opt))])\n",
    "\n",
    "# Fit training data\n",
    "best_forest.fit(X_train,y_train)\n",
    "\n",
    "# Predict test data\n",
    "y_pred_best_forest = best_forest.predict(X_test)\n",
    "MSE_best_forest = mean_squared_error(y_test,y_pred_best_forest)\n",
    "\n",
    "print('RMSE for the optimized Random Forest model: ', np.sqrt(MSE_best_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Boosting with KNN\n",
    "\n",
    "The last model to be built is also an ensemble model. In this case it consists of a Boosting ensembkle model composed by KNN models. Boosting strategy consists on...\n",
    "\n",
    "### 6.1 First model\n",
    "\n",
    "First of all, a model will be created with the default hyperparameters, no feature selection and the KNN algorithm for imputation. This model will be taken as reference to compare with other models buit under the same technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "# Create the first Boosting model\n",
    "# Define the base model for the ensemble method\n",
    "base_model = KNeighborsClassifier()\n",
    "gradientboost_model = GradientBoostingClassifier(base_model, n_estimators=100, random_state=rs)\n",
    "\n",
    "first_boosting = Pipeline([('imputer',KNNImputer()),('regression',gradientboost_model())])\n",
    "first_boosting.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_boosting = first_boosting.predict(X_inner_val)\n",
    "MSE_boosting = mean_squared_error(y_inner_val,y_predicted_boosting)\n",
    "\n",
    "print('RMSE for the first Gradient Boosting based on KNN: ', np.sqrt(MSE_boosting))\n",
    "# 392.66"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Feature selection\n",
    "Same procedure as in section 3.2. but in this case using XXX model. The variable selection will not be made again in this section, the filtered datasets are defined in section 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST OPTION - Selecting only the features that correspond to the location 13 (Sotavento)\n",
    "\n",
    "# The first Random Forest model that has been already created is trained now with the selected data\n",
    "first_boosting.fit(X_inner_train_1,y_inner_train)\n",
    "y_predicted_1_boosting = first_boosting.predict(X_inner_val_1)\n",
    "MSE_1_boosting = mean_squared_error(y_inner_val,y_predicted_1_boosting)\n",
    "\n",
    "print('RMSE for feature selection with variables from Sotavento: ', np.sqrt(MSE_1_boosting))\n",
    "# XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND OPTION - Selecting only the features related to the wind (the ones that start with u or v)\n",
    "\n",
    "# The first Random Forest model that has been already created is trained now with the selected data\n",
    "first_boosting.fit(X_inner_train_2,y_inner_train)\n",
    "y_predicted_2_boosting = first_boosting.predict(X_inner_val_2)\n",
    "MSE_2_boosting = mean_squared_error(y_inner_val,y_predicted_2_boosting)\n",
    "\n",
    "print('RMSE for feature selection with variables related to the wind: ', np.sqrt(MSE_2_boosting))\n",
    "# XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD OPTION - Selecting the features that correspond to magnitudes related to the wind in Sotavento\n",
    "\n",
    "# The first Random Forest model that has been already created is trained now with the selected data\n",
    "first_boosting.fit(X_inner_train_3,y_inner_train)\n",
    "y_predicted_3_boosting = first_boosting.predict(X_inner_val_3)\n",
    "MSE_3_boosting = mean_squared_error(y_inner_val,y_predicted_3_boosting)\n",
    "\n",
    "print('RMSE for imputation with variables from Sotavento related to the wind: ', np.sqrt(MSE_3_boosting))\n",
    "# 431.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from feature selection with Gradient Boosting based on KNN are included in the following table. \n",
    "\n",
    "|          | 0. No feature selection | 1. Sotavento features | 2. Wind features | 2. Wind features in Sotavento |\n",
    "|-----------|:----------------------:|:---------------------:|:----------:|:----------:|\n",
    "| RMSE |  XXX | XXX | XXX | XXX |\n",
    "\n",
    "XXXX\n",
    "\n",
    "\n",
    "### 6.3. Imputation techniques\n",
    "\n",
    "Same procedure as in section 3.3. but in this case using Gradient Boosting based on KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST OPTION: Simple Imputer using the mean\n",
    "simple_boosting = Pipeline([('imputer',SimpleImputer(strategy = 'mean')),('regression',gradientboost_model())])\n",
    "\n",
    "simple_boosting.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_simple_imputer_boosting = simple_boosting.predict(X_inner_val)\n",
    "MSE_simple_boosting = mean_squared_error(y_inner_val,y_predicted_simple_imputer_boosting)\n",
    "\n",
    "print('RMSE for imputation with Simple Imputer: ', np.sqrt(MSE_simple_boosting))\n",
    "# XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECOND OPTION: KNN Imputer with default hyperparameters\n",
    "knn_boosting = Pipeline([('imputer',KNNImputer()),('regression',gradientboost_model())])\n",
    "\n",
    "knn_boosting.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_knn_imputer_boosting = knn_boosting.predict(X_inner_val)\n",
    "MSE_knn_boosting = mean_squared_error(y_inner_val,y_predicted_knn_imputer_boosting)\n",
    "\n",
    "print('RMSE for imputation with KNN Imputer: ', np.sqrt(MSE_knn_boosting))\n",
    "# XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD OPTION: Iterative Imputer with 10 maximum iterations. Include random seed for reproducibility\n",
    "iterative_boosting = Pipeline([('imputer',IterativeImputer(max_iter = 10, random_state = rs)),('regression',gradientboost_model())])\n",
    "\n",
    "iterative_boosting.fit(X_inner_train,y_inner_train)\n",
    "y_predicted_iterative_imputer_boosting = iterative_boosting.predict(X_inner_val)\n",
    "MSE_iterative_boosting = mean_squared_error(y_inner_val,y_predicted_iterative_imputer_boosting)\n",
    "\n",
    "print('RMSE for imputation with Iterative Imputer: ', np.sqrt(MSE_iterative_boosting))\n",
    "# XXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from different imputation techniques are included in the following table. \n",
    "\n",
    "|          | 1. Simple Imputer | 2. KNN Imputer | 3. Iterative Imputer | \n",
    "|-----------|:----------------------:|:---------------------:|:----------:|\n",
    "| RMSE | XXX | XXX | XXX |\n",
    "\n",
    "Results from the table show that the best imputation technique for Gradient Boosting based on KNN is XXX. \n",
    "\n",
    "From this point, for the Gradient Boosting based on KNN model, XXX will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Hyperparameter tuning\n",
    "\n",
    "Once the imputation and feature selection have been performed, it is time to improve the performance of the model with HPO. Hyperparameter tuning will be carried out using crossvalidation as the inner evaluation method. In the case of  this ensemble method, the following hyperparameters will be optimized:\n",
    "\n",
    "- hp1\n",
    "- hp2\n",
    "- hp3\n",
    "\n",
    "The same three strategies used in section 3.4. will be repeated in this section but with Decission Tree as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL TO OPTIMIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FOR HPO WITH RANDOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FOR HPO WITH OPTUNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FOR HPO WITH HALVING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from this hyperparameter tuning are included in the following table:\n",
    "\n",
    "|          | 1. Random Search | 2. OPTUNA | 3. Successive Halving |\n",
    "|:-----------:|:----------------------:|:---------------------:|:----------:|\n",
    "| MSE (crossvalidation) | XXX | XXX | XXX |\n",
    "| HP1 | XXX | XXX | XXX |\n",
    "| HP2 | XXX | XXX | XXX |\n",
    "| HP3 | XXX | XXX | XXX |\n",
    "| Time (s) | XXX | XXX | XXX |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Conclusions and future performance\n",
    "\n",
    "Out of all the trials, the Gradient Boosting based on KNN model has demonstrated to work better with the following adjustments:\n",
    "- **Best imputation technique**: XXX\n",
    "- **Best feature selection strategy**: XXX\n",
    "- **Best hyperparameters**:\n",
    " - HP1: XXX\n",
    " - HP2: XXX\n",
    " - HP3: XXX\n",
    "\n",
    "Now, to estimate the future performance of this model, the test set that has been previously set aside will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FOR ESTIMATION OF FUTURE PERFORMANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Final model\n",
    "\n",
    "In this stage the four methods have already been created and trained with the complete training set. The future performance has been estimated using the Root Mean Squared Error. The results from the RMSE and the training time are stored in the following table.\n",
    "\n",
    "|          | KNN | Decision Tree | Random Forest | Final Ensemble |\n",
    "|:--------:||:--------:||:--------:||:--------:||:--------:|\n",
    "| RMSE (future performance) | 407.98 | 437.82 | XXX | XXX |\n",
    "| Training time (s) | XXX | XXX | XXX | XXX |\n",
    "\n",
    "The training time is included in the table just to demonstrate that the more complex models (Random Forest and XXX) take the longest training time. But focusing also on the error of the data, it is also true that these are the methods that provide better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Conclusions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
